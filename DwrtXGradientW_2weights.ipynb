{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtZotqTXPw4t2PID4bLIO5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zw2788/LocalMinimaConstruction/blob/main/DwrtXGradientW_2weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import Image\n",
        "from torch.autograd import grad\n"
      ],
      "metadata": {
        "id": "6IhL4Cbb1mfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, custom_W_0, custom_b, custom_V_0, custom_c):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        # Ensure that the custom weights are tensors\n",
        "        custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
        "        custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
        "        custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
        "        custom_c = torch.tensor(custom_c, dtype=torch.float64)\n",
        "\n",
        "        # Set the custom weights and biases\n",
        "        self.W_0 = nn.Parameter(custom_W_0)\n",
        "        self.b = nn.Parameter(custom_b)\n",
        "        self.V_0 = nn.Parameter(custom_V_0)\n",
        "        self.c = nn.Parameter(custom_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.W_0), self.b))\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.V_0), self.c))\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "#custom_W_0 = [[0.1, 0.2], [0.3, 0.4]]  # Replace with your own initial values\n",
        "#custom_b = [0.1, 0.2]  # Replace with your own initial values\n",
        "#custom_V_0 = [[0.1], [0.2]]  # Replace with your own initial values\n",
        "#custom_c = [0.1]  # Replace with your own initial values\n",
        "\n",
        "\n",
        "def calculate_second_order_grad(model, X_raw_torch, Y_torch):\n",
        "    # Forward pass\n",
        "    output = model(X_raw_torch)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "    # Compute gradients of the loss w.r.t. weights\n",
        "    loss.backward(create_graph=True)\n",
        "    # Combine and compute the norm of all gradients\n",
        "    all_grads = torch.cat([param.grad.flatten() for param in model.parameters()])\n",
        "    grad_norm = torch.norm(all_grads)\n",
        "    #print(all_grads)\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "    return second_order_grad\n",
        "\n",
        "def calculate_second_order_grad_trap(model, X_raw_torch, Y_torch):\n",
        "    # Forward pass\n",
        "    output = model(X_raw_torch)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "    # Compute gradients of the loss w.r.t. weights\n",
        "    loss.backward(create_graph=True)\n",
        "    # Combine and compute the norm of all gradients\n",
        "    all_grads = torch.cat([param.grad.flatten() for param in model.parameters()])\n",
        "    grad_norm = torch.norm(all_grads)\n",
        "    #print(all_grads)\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "    return second_order_grad\n",
        "\n",
        "def gradient_descent_on_input(model, X, Y, max_iterations, learning_rate=0.01):\n",
        "    X_modifiable = X.clone().detach().requires_grad_(True)\n",
        "    optimizer_x = optim.SGD([X_modifiable], lr=learning_rate)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        optimizer_x.zero_grad()\n",
        "        output = model(X_modifiable)\n",
        "        loss = -torch.mean(Y * torch.log(output) + (1 - Y) * torch.log(1 - output))\n",
        "        loss.backward()\n",
        "        optimizer_x.step()\n",
        "        print(i)\n",
        "\n",
        "    return X_modifiable\n"
      ],
      "metadata": {
        "id": "vBoW060Y1pZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb_weights_normal(model, max_deviation=0.01):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            std_dev = param.abs().mean() * max_deviation\n",
        "            noise = torch.randn(param.size()) * std_dev\n",
        "            param[:] = param + noise\n",
        "\n",
        "def perturb_weights_uniform(model, max_deviation=0.01):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            scale_factor = param.abs().mean() * max_deviation\n",
        "            # Generate uniform noise in the range [-scale_factor, scale_factor]\n",
        "            noise = (torch.rand(param.size()) * 2 - 1) * scale_factor\n",
        "            param[:] = param + noise\n",
        "def perturb_weights_uniform_fixed_range(model, scale):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            # Generate uniform noise in the range [-0.1, 0.1]\n",
        "            noise = (torch.rand(param.size()) * 2 - 1) * scale\n",
        "            param[:] = param + noise\n",
        "\n",
        "def restore_weights(model, saved_state):\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            param[:] = saved_state[name]\n",
        "\n",
        "def perturb_data(X, max_deviation=0.01):\n",
        "    \"\"\"Perturb the data tensor X.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        std_dev = X.abs().mean() * max_deviation\n",
        "        noise = torch.randn(X.size()) * std_dev\n",
        "        X.add_(noise)"
      ],
      "metadata": {
        "id": "iuOrsYOiJo91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-executing the code to define the function for computing the Hessian matrix and its eigenvalues\n",
        "\n",
        "def compute_hessian_and_eigenvalues(model, data, target):\n",
        "    \"\"\"\n",
        "    Compute the Hessian matrix and its eigenvalues for the weights of a neural network model.\n",
        "\n",
        "    :param model: The neural network model.\n",
        "    :param data: Input data (X).\n",
        "    :param target: Target data (Y).\n",
        "    :return: Hessian matrix and its eigenvalues.\n",
        "    \"\"\"\n",
        "    # Forward pass\n",
        "    output = model(data)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(target * torch.log(output) + (1 - target) * torch.log(1 - output))\n",
        "\n",
        "    # First-order gradients (w.r.t weights)\n",
        "    first_order_grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "    # Flatten the first-order gradients\n",
        "    grads_flatten = torch.cat([g.contiguous().view(-1) for g in first_order_grads])\n",
        "\n",
        "    # Hessian computation\n",
        "    hessian = []\n",
        "    for grad in grads_flatten:\n",
        "        # Compute second-order gradients (w.r.t each element in the first-order gradients)\n",
        "        second_order_grads = torch.autograd.grad(grad, model.parameters(), retain_graph=True)\n",
        "\n",
        "        # Flatten and collect the second-order gradients\n",
        "        hessian_row = torch.cat([g.contiguous().view(-1) for g in second_order_grads])\n",
        "        hessian.append(hessian_row)\n",
        "\n",
        "    # Stack to form the Hessian matrix\n",
        "    hessian_matrix = torch.stack(hessian)\n",
        "\n",
        "    # Compute eigenvalues\n",
        "    eigenvalues, _ = torch.linalg.eig(hessian_matrix)\n",
        "\n",
        "    return hessian_matrix, eigenvalues\n",
        "\n",
        "# Note: To use this function, you'll need to provide your neural network model, the input data (X), and the target data (Y).\n",
        "\n",
        "\n",
        "def check_local_minimum(eigenvalues):\n",
        "    # Check if all eigenvalues have a positive real part\n",
        "    if all(eig.real > 0 for eig in eigenvalues):\n",
        "        print(\"This is a local minimum.\")\n",
        "    else:\n",
        "        print(\"This is not a local minimum.\")\n"
      ],
      "metadata": {
        "id": "DG4bccfJA4Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot a single tensor\n",
        "def plot_tensor(tensor_data, label='Data', marker='o', color='blue'):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "\n",
        "    # Plotting the tensor\n",
        "    plt.scatter(tensor_data[:, 0].detach().numpy(), tensor_data[:, 1].detach().numpy(), label=label, marker=marker, color=color)\n",
        "\n",
        "    plt.xlabel('X Axis')\n",
        "    plt.ylabel('Y Axis')\n",
        "    plt.title(f'{label} Plot')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tq4AJolQU59J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/zw2788/LocalMinimaConstruction/main/8shape.csv\")\n",
        "\n",
        "data.head()\n",
        "\n",
        "# data , drop NaN values\n",
        "X_raw,  Y, W_0, b, V_0, c = data[['x_2dvec']].dropna().values, data['y'].dropna().values, data[['W_0']].dropna().values, data[['b']].dropna().values, data[['V_0']].dropna().values, data[['c']].dropna().values\n",
        "\n",
        "#convert string to array\n",
        "\n",
        "X_raw = np.array([eval(s[0]) for s in X_raw])\n",
        "\n",
        "W_0 = np.array([eval(s[0]) for s in W_0])\n",
        "\n",
        "b = np.array([eval(s[0]) for s in b])\n",
        "\n",
        "V_0 = np.array([eval(s[0]) for s in V_0])\n",
        "\n",
        "c = np.array([eval(s[0]) for s in c])\n",
        "\n",
        "# Standardize the input\n",
        "# Leave blank to match the example in paper\n",
        "\n",
        "# formatting\n",
        "Y = Y.reshape((-1, 1))\n",
        "print(X_raw)\n",
        "print(Y)\n",
        "print(W_0)\n",
        "#print(X_raw.shape[0])\n",
        "X_raw = torch.tensor(X_raw, requires_grad=True)\n",
        "Y = torch.tensor(Y)\n",
        "print(W_0, b, V_0, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0h9evkU59mR",
        "outputId": "b268fafc-f895-4f79-bfb9-e23cddbdad46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-4.  -4. ]\n",
            " [-4.  -1. ]\n",
            " [-1.  -1. ]\n",
            " [-1.   2. ]\n",
            " [ 2.   2. ]\n",
            " [-3.  -4. ]\n",
            " [-1.5 -4. ]\n",
            " [-1.5 -0.5]\n",
            " [ 1.  -1. ]\n",
            " [ 1.5  2.5]]\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[-7.60825117  3.76327948]\n",
            " [-4.83862577  6.00303144]]\n",
            "[[-7.60825117  3.76327948]\n",
            " [-4.83862577  6.00303144]] [[-14.73302609   7.71711836]] [[8.94303407]\n",
            " [6.73296582]] [[-6.73776928]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_initial, eigenvalues_initial = compute_hessian_and_eigenvalues(nn_model, X_raw, Y)\n",
        "\n",
        "print(eigenvalues_initial)\n",
        "check_local_minimum(eigenvalues_initial)"
      ],
      "metadata": {
        "id": "zkVm-srfFTQZ",
        "outputId": "09a795ea-3d3a-4d81-ef25-55326d665243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.3788e-01+0.j, -1.4928e-01+0.j, -6.5649e-02+0.j,  7.3909e-02+0.j,\n",
            "         4.5694e-02+0.j,  2.4847e-03+0.j, -1.4595e-03+0.j, -2.0535e-07+0.j,\n",
            "         1.2317e-03+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model_GD = SimpleNN(W_0, b, V_0, c)\n",
        "X_GD = gradient_descent_on_input(nn_model_GD , X_raw, Y, max_iterations=10000, learning_rate=0.01)\n",
        "print(\"X_raw is {}\".format(X_raw))\n",
        "print(\"X_GD is {}\".format(X_GD))\n",
        "\n",
        "hessian_matrix_GD, eigenvalues_GD = compute_hessian_and_eigenvalues(nn_model_GD, X_GD, Y)\n",
        "\n",
        "print(eigenvalues_GD)\n",
        "check_local_minimum(eigenvalues_GD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiZXsWOXbYkq",
        "outputId": "b2800d42-00c8-457e-9ad8-402df8a8e836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "5024\n",
            "5025\n",
            "5026\n",
            "5027\n",
            "5028\n",
            "5029\n",
            "5030\n",
            "5031\n",
            "5032\n",
            "5033\n",
            "5034\n",
            "5035\n",
            "5036\n",
            "5037\n",
            "5038\n",
            "5039\n",
            "5040\n",
            "5041\n",
            "5042\n",
            "5043\n",
            "5044\n",
            "5045\n",
            "5046\n",
            "5047\n",
            "5048\n",
            "5049\n",
            "5050\n",
            "5051\n",
            "5052\n",
            "5053\n",
            "5054\n",
            "5055\n",
            "5056\n",
            "5057\n",
            "5058\n",
            "5059\n",
            "5060\n",
            "5061\n",
            "5062\n",
            "5063\n",
            "5064\n",
            "5065\n",
            "5066\n",
            "5067\n",
            "5068\n",
            "5069\n",
            "5070\n",
            "5071\n",
            "5072\n",
            "5073\n",
            "5074\n",
            "5075\n",
            "5076\n",
            "5077\n",
            "5078\n",
            "5079\n",
            "5080\n",
            "5081\n",
            "5082\n",
            "5083\n",
            "5084\n",
            "5085\n",
            "5086\n",
            "5087\n",
            "5088\n",
            "5089\n",
            "5090\n",
            "5091\n",
            "5092\n",
            "5093\n",
            "5094\n",
            "5095\n",
            "5096\n",
            "5097\n",
            "5098\n",
            "5099\n",
            "5100\n",
            "5101\n",
            "5102\n",
            "5103\n",
            "5104\n",
            "5105\n",
            "5106\n",
            "5107\n",
            "5108\n",
            "5109\n",
            "5110\n",
            "5111\n",
            "5112\n",
            "5113\n",
            "5114\n",
            "5115\n",
            "5116\n",
            "5117\n",
            "5118\n",
            "5119\n",
            "5120\n",
            "5121\n",
            "5122\n",
            "5123\n",
            "5124\n",
            "5125\n",
            "5126\n",
            "5127\n",
            "5128\n",
            "5129\n",
            "5130\n",
            "5131\n",
            "5132\n",
            "5133\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "5137\n",
            "5138\n",
            "5139\n",
            "5140\n",
            "5141\n",
            "5142\n",
            "5143\n",
            "5144\n",
            "5145\n",
            "5146\n",
            "5147\n",
            "5148\n",
            "5149\n",
            "5150\n",
            "5151\n",
            "5152\n",
            "5153\n",
            "5154\n",
            "5155\n",
            "5156\n",
            "5157\n",
            "5158\n",
            "5159\n",
            "5160\n",
            "5161\n",
            "5162\n",
            "5163\n",
            "5164\n",
            "5165\n",
            "5166\n",
            "5167\n",
            "5168\n",
            "5169\n",
            "5170\n",
            "5171\n",
            "5172\n",
            "5173\n",
            "5174\n",
            "5175\n",
            "5176\n",
            "5177\n",
            "5178\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "5182\n",
            "5183\n",
            "5184\n",
            "5185\n",
            "5186\n",
            "5187\n",
            "5188\n",
            "5189\n",
            "5190\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "5195\n",
            "5196\n",
            "5197\n",
            "5198\n",
            "5199\n",
            "5200\n",
            "5201\n",
            "5202\n",
            "5203\n",
            "5204\n",
            "5205\n",
            "5206\n",
            "5207\n",
            "5208\n",
            "5209\n",
            "5210\n",
            "5211\n",
            "5212\n",
            "5213\n",
            "5214\n",
            "5215\n",
            "5216\n",
            "5217\n",
            "5218\n",
            "5219\n",
            "5220\n",
            "5221\n",
            "5222\n",
            "5223\n",
            "5224\n",
            "5225\n",
            "5226\n",
            "5227\n",
            "5228\n",
            "5229\n",
            "5230\n",
            "5231\n",
            "5232\n",
            "5233\n",
            "5234\n",
            "5235\n",
            "5236\n",
            "5237\n",
            "5238\n",
            "5239\n",
            "5240\n",
            "5241\n",
            "5242\n",
            "5243\n",
            "5244\n",
            "5245\n",
            "5246\n",
            "5247\n",
            "5248\n",
            "5249\n",
            "5250\n",
            "5251\n",
            "5252\n",
            "5253\n",
            "5254\n",
            "5255\n",
            "5256\n",
            "5257\n",
            "5258\n",
            "5259\n",
            "5260\n",
            "5261\n",
            "5262\n",
            "5263\n",
            "5264\n",
            "5265\n",
            "5266\n",
            "5267\n",
            "5268\n",
            "5269\n",
            "5270\n",
            "5271\n",
            "5272\n",
            "5273\n",
            "5274\n",
            "5275\n",
            "5276\n",
            "5277\n",
            "5278\n",
            "5279\n",
            "5280\n",
            "5281\n",
            "5282\n",
            "5283\n",
            "5284\n",
            "5285\n",
            "5286\n",
            "5287\n",
            "5288\n",
            "5289\n",
            "5290\n",
            "5291\n",
            "5292\n",
            "5293\n",
            "5294\n",
            "5295\n",
            "5296\n",
            "5297\n",
            "5298\n",
            "5299\n",
            "5300\n",
            "5301\n",
            "5302\n",
            "5303\n",
            "5304\n",
            "5305\n",
            "5306\n",
            "5307\n",
            "5308\n",
            "5309\n",
            "5310\n",
            "5311\n",
            "5312\n",
            "5313\n",
            "5314\n",
            "5315\n",
            "5316\n",
            "5317\n",
            "5318\n",
            "5319\n",
            "5320\n",
            "5321\n",
            "5322\n",
            "5323\n",
            "5324\n",
            "5325\n",
            "5326\n",
            "5327\n",
            "5328\n",
            "5329\n",
            "5330\n",
            "5331\n",
            "5332\n",
            "5333\n",
            "5334\n",
            "5335\n",
            "5336\n",
            "5337\n",
            "5338\n",
            "5339\n",
            "5340\n",
            "5341\n",
            "5342\n",
            "5343\n",
            "5344\n",
            "5345\n",
            "5346\n",
            "5347\n",
            "5348\n",
            "5349\n",
            "5350\n",
            "5351\n",
            "5352\n",
            "5353\n",
            "5354\n",
            "5355\n",
            "5356\n",
            "5357\n",
            "5358\n",
            "5359\n",
            "5360\n",
            "5361\n",
            "5362\n",
            "5363\n",
            "5364\n",
            "5365\n",
            "5366\n",
            "5367\n",
            "5368\n",
            "5369\n",
            "5370\n",
            "5371\n",
            "5372\n",
            "5373\n",
            "5374\n",
            "5375\n",
            "5376\n",
            "5377\n",
            "5378\n",
            "5379\n",
            "5380\n",
            "5381\n",
            "5382\n",
            "5383\n",
            "5384\n",
            "5385\n",
            "5386\n",
            "5387\n",
            "5388\n",
            "5389\n",
            "5390\n",
            "5391\n",
            "5392\n",
            "5393\n",
            "5394\n",
            "5395\n",
            "5396\n",
            "5397\n",
            "5398\n",
            "5399\n",
            "5400\n",
            "5401\n",
            "5402\n",
            "5403\n",
            "5404\n",
            "5405\n",
            "5406\n",
            "5407\n",
            "5408\n",
            "5409\n",
            "5410\n",
            "5411\n",
            "5412\n",
            "5413\n",
            "5414\n",
            "5415\n",
            "5416\n",
            "5417\n",
            "5418\n",
            "5419\n",
            "5420\n",
            "5421\n",
            "5422\n",
            "5423\n",
            "5424\n",
            "5425\n",
            "5426\n",
            "5427\n",
            "5428\n",
            "5429\n",
            "5430\n",
            "5431\n",
            "5432\n",
            "5433\n",
            "5434\n",
            "5435\n",
            "5436\n",
            "5437\n",
            "5438\n",
            "5439\n",
            "5440\n",
            "5441\n",
            "5442\n",
            "5443\n",
            "5444\n",
            "5445\n",
            "5446\n",
            "5447\n",
            "5448\n",
            "5449\n",
            "5450\n",
            "5451\n",
            "5452\n",
            "5453\n",
            "5454\n",
            "5455\n",
            "5456\n",
            "5457\n",
            "5458\n",
            "5459\n",
            "5460\n",
            "5461\n",
            "5462\n",
            "5463\n",
            "5464\n",
            "5465\n",
            "5466\n",
            "5467\n",
            "5468\n",
            "5469\n",
            "5470\n",
            "5471\n",
            "5472\n",
            "5473\n",
            "5474\n",
            "5475\n",
            "5476\n",
            "5477\n",
            "5478\n",
            "5479\n",
            "5480\n",
            "5481\n",
            "5482\n",
            "5483\n",
            "5484\n",
            "5485\n",
            "5486\n",
            "5487\n",
            "5488\n",
            "5489\n",
            "5490\n",
            "5491\n",
            "5492\n",
            "5493\n",
            "5494\n",
            "5495\n",
            "5496\n",
            "5497\n",
            "5498\n",
            "5499\n",
            "5500\n",
            "5501\n",
            "5502\n",
            "5503\n",
            "5504\n",
            "5505\n",
            "5506\n",
            "5507\n",
            "5508\n",
            "5509\n",
            "5510\n",
            "5511\n",
            "5512\n",
            "5513\n",
            "5514\n",
            "5515\n",
            "5516\n",
            "5517\n",
            "5518\n",
            "5519\n",
            "5520\n",
            "5521\n",
            "5522\n",
            "5523\n",
            "5524\n",
            "5525\n",
            "5526\n",
            "5527\n",
            "5528\n",
            "5529\n",
            "5530\n",
            "5531\n",
            "5532\n",
            "5533\n",
            "5534\n",
            "5535\n",
            "5536\n",
            "5537\n",
            "5538\n",
            "5539\n",
            "5540\n",
            "5541\n",
            "5542\n",
            "5543\n",
            "5544\n",
            "5545\n",
            "5546\n",
            "5547\n",
            "5548\n",
            "5549\n",
            "5550\n",
            "5551\n",
            "5552\n",
            "5553\n",
            "5554\n",
            "5555\n",
            "5556\n",
            "5557\n",
            "5558\n",
            "5559\n",
            "5560\n",
            "5561\n",
            "5562\n",
            "5563\n",
            "5564\n",
            "5565\n",
            "5566\n",
            "5567\n",
            "5568\n",
            "5569\n",
            "5570\n",
            "5571\n",
            "5572\n",
            "5573\n",
            "5574\n",
            "5575\n",
            "5576\n",
            "5577\n",
            "5578\n",
            "5579\n",
            "5580\n",
            "5581\n",
            "5582\n",
            "5583\n",
            "5584\n",
            "5585\n",
            "5586\n",
            "5587\n",
            "5588\n",
            "5589\n",
            "5590\n",
            "5591\n",
            "5592\n",
            "5593\n",
            "5594\n",
            "5595\n",
            "5596\n",
            "5597\n",
            "5598\n",
            "5599\n",
            "5600\n",
            "5601\n",
            "5602\n",
            "5603\n",
            "5604\n",
            "5605\n",
            "5606\n",
            "5607\n",
            "5608\n",
            "5609\n",
            "5610\n",
            "5611\n",
            "5612\n",
            "5613\n",
            "5614\n",
            "5615\n",
            "5616\n",
            "5617\n",
            "5618\n",
            "5619\n",
            "5620\n",
            "5621\n",
            "5622\n",
            "5623\n",
            "5624\n",
            "5625\n",
            "5626\n",
            "5627\n",
            "5628\n",
            "5629\n",
            "5630\n",
            "5631\n",
            "5632\n",
            "5633\n",
            "5634\n",
            "5635\n",
            "5636\n",
            "5637\n",
            "5638\n",
            "5639\n",
            "5640\n",
            "5641\n",
            "5642\n",
            "5643\n",
            "5644\n",
            "5645\n",
            "5646\n",
            "5647\n",
            "5648\n",
            "5649\n",
            "5650\n",
            "5651\n",
            "5652\n",
            "5653\n",
            "5654\n",
            "5655\n",
            "5656\n",
            "5657\n",
            "5658\n",
            "5659\n",
            "5660\n",
            "5661\n",
            "5662\n",
            "5663\n",
            "5664\n",
            "5665\n",
            "5666\n",
            "5667\n",
            "5668\n",
            "5669\n",
            "5670\n",
            "5671\n",
            "5672\n",
            "5673\n",
            "5674\n",
            "5675\n",
            "5676\n",
            "5677\n",
            "5678\n",
            "5679\n",
            "5680\n",
            "5681\n",
            "5682\n",
            "5683\n",
            "5684\n",
            "5685\n",
            "5686\n",
            "5687\n",
            "5688\n",
            "5689\n",
            "5690\n",
            "5691\n",
            "5692\n",
            "5693\n",
            "5694\n",
            "5695\n",
            "5696\n",
            "5697\n",
            "5698\n",
            "5699\n",
            "5700\n",
            "5701\n",
            "5702\n",
            "5703\n",
            "5704\n",
            "5705\n",
            "5706\n",
            "5707\n",
            "5708\n",
            "5709\n",
            "5710\n",
            "5711\n",
            "5712\n",
            "5713\n",
            "5714\n",
            "5715\n",
            "5716\n",
            "5717\n",
            "5718\n",
            "5719\n",
            "5720\n",
            "5721\n",
            "5722\n",
            "5723\n",
            "5724\n",
            "5725\n",
            "5726\n",
            "5727\n",
            "5728\n",
            "5729\n",
            "5730\n",
            "5731\n",
            "5732\n",
            "5733\n",
            "5734\n",
            "5735\n",
            "5736\n",
            "5737\n",
            "5738\n",
            "5739\n",
            "5740\n",
            "5741\n",
            "5742\n",
            "5743\n",
            "5744\n",
            "5745\n",
            "5746\n",
            "5747\n",
            "5748\n",
            "5749\n",
            "5750\n",
            "5751\n",
            "5752\n",
            "5753\n",
            "5754\n",
            "5755\n",
            "5756\n",
            "5757\n",
            "5758\n",
            "5759\n",
            "5760\n",
            "5761\n",
            "5762\n",
            "5763\n",
            "5764\n",
            "5765\n",
            "5766\n",
            "5767\n",
            "5768\n",
            "5769\n",
            "5770\n",
            "5771\n",
            "5772\n",
            "5773\n",
            "5774\n",
            "5775\n",
            "5776\n",
            "5777\n",
            "5778\n",
            "5779\n",
            "5780\n",
            "5781\n",
            "5782\n",
            "5783\n",
            "5784\n",
            "5785\n",
            "5786\n",
            "5787\n",
            "5788\n",
            "5789\n",
            "5790\n",
            "5791\n",
            "5792\n",
            "5793\n",
            "5794\n",
            "5795\n",
            "5796\n",
            "5797\n",
            "5798\n",
            "5799\n",
            "5800\n",
            "5801\n",
            "5802\n",
            "5803\n",
            "5804\n",
            "5805\n",
            "5806\n",
            "5807\n",
            "5808\n",
            "5809\n",
            "5810\n",
            "5811\n",
            "5812\n",
            "5813\n",
            "5814\n",
            "5815\n",
            "5816\n",
            "5817\n",
            "5818\n",
            "5819\n",
            "5820\n",
            "5821\n",
            "5822\n",
            "5823\n",
            "5824\n",
            "5825\n",
            "5826\n",
            "5827\n",
            "5828\n",
            "5829\n",
            "5830\n",
            "5831\n",
            "5832\n",
            "5833\n",
            "5834\n",
            "5835\n",
            "5836\n",
            "5837\n",
            "5838\n",
            "5839\n",
            "5840\n",
            "5841\n",
            "5842\n",
            "5843\n",
            "5844\n",
            "5845\n",
            "5846\n",
            "5847\n",
            "5848\n",
            "5849\n",
            "5850\n",
            "5851\n",
            "5852\n",
            "5853\n",
            "5854\n",
            "5855\n",
            "5856\n",
            "5857\n",
            "5858\n",
            "5859\n",
            "5860\n",
            "5861\n",
            "5862\n",
            "5863\n",
            "5864\n",
            "5865\n",
            "5866\n",
            "5867\n",
            "5868\n",
            "5869\n",
            "5870\n",
            "5871\n",
            "5872\n",
            "5873\n",
            "5874\n",
            "5875\n",
            "5876\n",
            "5877\n",
            "5878\n",
            "5879\n",
            "5880\n",
            "5881\n",
            "5882\n",
            "5883\n",
            "5884\n",
            "5885\n",
            "5886\n",
            "5887\n",
            "5888\n",
            "5889\n",
            "5890\n",
            "5891\n",
            "5892\n",
            "5893\n",
            "5894\n",
            "5895\n",
            "5896\n",
            "5897\n",
            "5898\n",
            "5899\n",
            "5900\n",
            "5901\n",
            "5902\n",
            "5903\n",
            "5904\n",
            "5905\n",
            "5906\n",
            "5907\n",
            "5908\n",
            "5909\n",
            "5910\n",
            "5911\n",
            "5912\n",
            "5913\n",
            "5914\n",
            "5915\n",
            "5916\n",
            "5917\n",
            "5918\n",
            "5919\n",
            "5920\n",
            "5921\n",
            "5922\n",
            "5923\n",
            "5924\n",
            "5925\n",
            "5926\n",
            "5927\n",
            "5928\n",
            "5929\n",
            "5930\n",
            "5931\n",
            "5932\n",
            "5933\n",
            "5934\n",
            "5935\n",
            "5936\n",
            "5937\n",
            "5938\n",
            "5939\n",
            "5940\n",
            "5941\n",
            "5942\n",
            "5943\n",
            "5944\n",
            "5945\n",
            "5946\n",
            "5947\n",
            "5948\n",
            "5949\n",
            "5950\n",
            "5951\n",
            "5952\n",
            "5953\n",
            "5954\n",
            "5955\n",
            "5956\n",
            "5957\n",
            "5958\n",
            "5959\n",
            "5960\n",
            "5961\n",
            "5962\n",
            "5963\n",
            "5964\n",
            "5965\n",
            "5966\n",
            "5967\n",
            "5968\n",
            "5969\n",
            "5970\n",
            "5971\n",
            "5972\n",
            "5973\n",
            "5974\n",
            "5975\n",
            "5976\n",
            "5977\n",
            "5978\n",
            "5979\n",
            "5980\n",
            "5981\n",
            "5982\n",
            "5983\n",
            "5984\n",
            "5985\n",
            "5986\n",
            "5987\n",
            "5988\n",
            "5989\n",
            "5990\n",
            "5991\n",
            "5992\n",
            "5993\n",
            "5994\n",
            "5995\n",
            "5996\n",
            "5997\n",
            "5998\n",
            "5999\n",
            "6000\n",
            "6001\n",
            "6002\n",
            "6003\n",
            "6004\n",
            "6005\n",
            "6006\n",
            "6007\n",
            "6008\n",
            "6009\n",
            "6010\n",
            "6011\n",
            "6012\n",
            "6013\n",
            "6014\n",
            "6015\n",
            "6016\n",
            "6017\n",
            "6018\n",
            "6019\n",
            "6020\n",
            "6021\n",
            "6022\n",
            "6023\n",
            "6024\n",
            "6025\n",
            "6026\n",
            "6027\n",
            "6028\n",
            "6029\n",
            "6030\n",
            "6031\n",
            "6032\n",
            "6033\n",
            "6034\n",
            "6035\n",
            "6036\n",
            "6037\n",
            "6038\n",
            "6039\n",
            "6040\n",
            "6041\n",
            "6042\n",
            "6043\n",
            "6044\n",
            "6045\n",
            "6046\n",
            "6047\n",
            "6048\n",
            "6049\n",
            "6050\n",
            "6051\n",
            "6052\n",
            "6053\n",
            "6054\n",
            "6055\n",
            "6056\n",
            "6057\n",
            "6058\n",
            "6059\n",
            "6060\n",
            "6061\n",
            "6062\n",
            "6063\n",
            "6064\n",
            "6065\n",
            "6066\n",
            "6067\n",
            "6068\n",
            "6069\n",
            "6070\n",
            "6071\n",
            "6072\n",
            "6073\n",
            "6074\n",
            "6075\n",
            "6076\n",
            "6077\n",
            "6078\n",
            "6079\n",
            "6080\n",
            "6081\n",
            "6082\n",
            "6083\n",
            "6084\n",
            "6085\n",
            "6086\n",
            "6087\n",
            "6088\n",
            "6089\n",
            "6090\n",
            "6091\n",
            "6092\n",
            "6093\n",
            "6094\n",
            "6095\n",
            "6096\n",
            "6097\n",
            "6098\n",
            "6099\n",
            "6100\n",
            "6101\n",
            "6102\n",
            "6103\n",
            "6104\n",
            "6105\n",
            "6106\n",
            "6107\n",
            "6108\n",
            "6109\n",
            "6110\n",
            "6111\n",
            "6112\n",
            "6113\n",
            "6114\n",
            "6115\n",
            "6116\n",
            "6117\n",
            "6118\n",
            "6119\n",
            "6120\n",
            "6121\n",
            "6122\n",
            "6123\n",
            "6124\n",
            "6125\n",
            "6126\n",
            "6127\n",
            "6128\n",
            "6129\n",
            "6130\n",
            "6131\n",
            "6132\n",
            "6133\n",
            "6134\n",
            "6135\n",
            "6136\n",
            "6137\n",
            "6138\n",
            "6139\n",
            "6140\n",
            "6141\n",
            "6142\n",
            "6143\n",
            "6144\n",
            "6145\n",
            "6146\n",
            "6147\n",
            "6148\n",
            "6149\n",
            "6150\n",
            "6151\n",
            "6152\n",
            "6153\n",
            "6154\n",
            "6155\n",
            "6156\n",
            "6157\n",
            "6158\n",
            "6159\n",
            "6160\n",
            "6161\n",
            "6162\n",
            "6163\n",
            "6164\n",
            "6165\n",
            "6166\n",
            "6167\n",
            "6168\n",
            "6169\n",
            "6170\n",
            "6171\n",
            "6172\n",
            "6173\n",
            "6174\n",
            "6175\n",
            "6176\n",
            "6177\n",
            "6178\n",
            "6179\n",
            "6180\n",
            "6181\n",
            "6182\n",
            "6183\n",
            "6184\n",
            "6185\n",
            "6186\n",
            "6187\n",
            "6188\n",
            "6189\n",
            "6190\n",
            "6191\n",
            "6192\n",
            "6193\n",
            "6194\n",
            "6195\n",
            "6196\n",
            "6197\n",
            "6198\n",
            "6199\n",
            "6200\n",
            "6201\n",
            "6202\n",
            "6203\n",
            "6204\n",
            "6205\n",
            "6206\n",
            "6207\n",
            "6208\n",
            "6209\n",
            "6210\n",
            "6211\n",
            "6212\n",
            "6213\n",
            "6214\n",
            "6215\n",
            "6216\n",
            "6217\n",
            "6218\n",
            "6219\n",
            "6220\n",
            "6221\n",
            "6222\n",
            "6223\n",
            "6224\n",
            "6225\n",
            "6226\n",
            "6227\n",
            "6228\n",
            "6229\n",
            "6230\n",
            "6231\n",
            "6232\n",
            "6233\n",
            "6234\n",
            "6235\n",
            "6236\n",
            "6237\n",
            "6238\n",
            "6239\n",
            "6240\n",
            "6241\n",
            "6242\n",
            "6243\n",
            "6244\n",
            "6245\n",
            "6246\n",
            "6247\n",
            "6248\n",
            "6249\n",
            "6250\n",
            "6251\n",
            "6252\n",
            "6253\n",
            "6254\n",
            "6255\n",
            "6256\n",
            "6257\n",
            "6258\n",
            "6259\n",
            "6260\n",
            "6261\n",
            "6262\n",
            "6263\n",
            "6264\n",
            "6265\n",
            "6266\n",
            "6267\n",
            "6268\n",
            "6269\n",
            "6270\n",
            "6271\n",
            "6272\n",
            "6273\n",
            "6274\n",
            "6275\n",
            "6276\n",
            "6277\n",
            "6278\n",
            "6279\n",
            "6280\n",
            "6281\n",
            "6282\n",
            "6283\n",
            "6284\n",
            "6285\n",
            "6286\n",
            "6287\n",
            "6288\n",
            "6289\n",
            "6290\n",
            "6291\n",
            "6292\n",
            "6293\n",
            "6294\n",
            "6295\n",
            "6296\n",
            "6297\n",
            "6298\n",
            "6299\n",
            "6300\n",
            "6301\n",
            "6302\n",
            "6303\n",
            "6304\n",
            "6305\n",
            "6306\n",
            "6307\n",
            "6308\n",
            "6309\n",
            "6310\n",
            "6311\n",
            "6312\n",
            "6313\n",
            "6314\n",
            "6315\n",
            "6316\n",
            "6317\n",
            "6318\n",
            "6319\n",
            "6320\n",
            "6321\n",
            "6322\n",
            "6323\n",
            "6324\n",
            "6325\n",
            "6326\n",
            "6327\n",
            "6328\n",
            "6329\n",
            "6330\n",
            "6331\n",
            "6332\n",
            "6333\n",
            "6334\n",
            "6335\n",
            "6336\n",
            "6337\n",
            "6338\n",
            "6339\n",
            "6340\n",
            "6341\n",
            "6342\n",
            "6343\n",
            "6344\n",
            "6345\n",
            "6346\n",
            "6347\n",
            "6348\n",
            "6349\n",
            "6350\n",
            "6351\n",
            "6352\n",
            "6353\n",
            "6354\n",
            "6355\n",
            "6356\n",
            "6357\n",
            "6358\n",
            "6359\n",
            "6360\n",
            "6361\n",
            "6362\n",
            "6363\n",
            "6364\n",
            "6365\n",
            "6366\n",
            "6367\n",
            "6368\n",
            "6369\n",
            "6370\n",
            "6371\n",
            "6372\n",
            "6373\n",
            "6374\n",
            "6375\n",
            "6376\n",
            "6377\n",
            "6378\n",
            "6379\n",
            "6380\n",
            "6381\n",
            "6382\n",
            "6383\n",
            "6384\n",
            "6385\n",
            "6386\n",
            "6387\n",
            "6388\n",
            "6389\n",
            "6390\n",
            "6391\n",
            "6392\n",
            "6393\n",
            "6394\n",
            "6395\n",
            "6396\n",
            "6397\n",
            "6398\n",
            "6399\n",
            "6400\n",
            "6401\n",
            "6402\n",
            "6403\n",
            "6404\n",
            "6405\n",
            "6406\n",
            "6407\n",
            "6408\n",
            "6409\n",
            "6410\n",
            "6411\n",
            "6412\n",
            "6413\n",
            "6414\n",
            "6415\n",
            "6416\n",
            "6417\n",
            "6418\n",
            "6419\n",
            "6420\n",
            "6421\n",
            "6422\n",
            "6423\n",
            "6424\n",
            "6425\n",
            "6426\n",
            "6427\n",
            "6428\n",
            "6429\n",
            "6430\n",
            "6431\n",
            "6432\n",
            "6433\n",
            "6434\n",
            "6435\n",
            "6436\n",
            "6437\n",
            "6438\n",
            "6439\n",
            "6440\n",
            "6441\n",
            "6442\n",
            "6443\n",
            "6444\n",
            "6445\n",
            "6446\n",
            "6447\n",
            "6448\n",
            "6449\n",
            "6450\n",
            "6451\n",
            "6452\n",
            "6453\n",
            "6454\n",
            "6455\n",
            "6456\n",
            "6457\n",
            "6458\n",
            "6459\n",
            "6460\n",
            "6461\n",
            "6462\n",
            "6463\n",
            "6464\n",
            "6465\n",
            "6466\n",
            "6467\n",
            "6468\n",
            "6469\n",
            "6470\n",
            "6471\n",
            "6472\n",
            "6473\n",
            "6474\n",
            "6475\n",
            "6476\n",
            "6477\n",
            "6478\n",
            "6479\n",
            "6480\n",
            "6481\n",
            "6482\n",
            "6483\n",
            "6484\n",
            "6485\n",
            "6486\n",
            "6487\n",
            "6488\n",
            "6489\n",
            "6490\n",
            "6491\n",
            "6492\n",
            "6493\n",
            "6494\n",
            "6495\n",
            "6496\n",
            "6497\n",
            "6498\n",
            "6499\n",
            "6500\n",
            "6501\n",
            "6502\n",
            "6503\n",
            "6504\n",
            "6505\n",
            "6506\n",
            "6507\n",
            "6508\n",
            "6509\n",
            "6510\n",
            "6511\n",
            "6512\n",
            "6513\n",
            "6514\n",
            "6515\n",
            "6516\n",
            "6517\n",
            "6518\n",
            "6519\n",
            "6520\n",
            "6521\n",
            "6522\n",
            "6523\n",
            "6524\n",
            "6525\n",
            "6526\n",
            "6527\n",
            "6528\n",
            "6529\n",
            "6530\n",
            "6531\n",
            "6532\n",
            "6533\n",
            "6534\n",
            "6535\n",
            "6536\n",
            "6537\n",
            "6538\n",
            "6539\n",
            "6540\n",
            "6541\n",
            "6542\n",
            "6543\n",
            "6544\n",
            "6545\n",
            "6546\n",
            "6547\n",
            "6548\n",
            "6549\n",
            "6550\n",
            "6551\n",
            "6552\n",
            "6553\n",
            "6554\n",
            "6555\n",
            "6556\n",
            "6557\n",
            "6558\n",
            "6559\n",
            "6560\n",
            "6561\n",
            "6562\n",
            "6563\n",
            "6564\n",
            "6565\n",
            "6566\n",
            "6567\n",
            "6568\n",
            "6569\n",
            "6570\n",
            "6571\n",
            "6572\n",
            "6573\n",
            "6574\n",
            "6575\n",
            "6576\n",
            "6577\n",
            "6578\n",
            "6579\n",
            "6580\n",
            "6581\n",
            "6582\n",
            "6583\n",
            "6584\n",
            "6585\n",
            "6586\n",
            "6587\n",
            "6588\n",
            "6589\n",
            "6590\n",
            "6591\n",
            "6592\n",
            "6593\n",
            "6594\n",
            "6595\n",
            "6596\n",
            "6597\n",
            "6598\n",
            "6599\n",
            "6600\n",
            "6601\n",
            "6602\n",
            "6603\n",
            "6604\n",
            "6605\n",
            "6606\n",
            "6607\n",
            "6608\n",
            "6609\n",
            "6610\n",
            "6611\n",
            "6612\n",
            "6613\n",
            "6614\n",
            "6615\n",
            "6616\n",
            "6617\n",
            "6618\n",
            "6619\n",
            "6620\n",
            "6621\n",
            "6622\n",
            "6623\n",
            "6624\n",
            "6625\n",
            "6626\n",
            "6627\n",
            "6628\n",
            "6629\n",
            "6630\n",
            "6631\n",
            "6632\n",
            "6633\n",
            "6634\n",
            "6635\n",
            "6636\n",
            "6637\n",
            "6638\n",
            "6639\n",
            "6640\n",
            "6641\n",
            "6642\n",
            "6643\n",
            "6644\n",
            "6645\n",
            "6646\n",
            "6647\n",
            "6648\n",
            "6649\n",
            "6650\n",
            "6651\n",
            "6652\n",
            "6653\n",
            "6654\n",
            "6655\n",
            "6656\n",
            "6657\n",
            "6658\n",
            "6659\n",
            "6660\n",
            "6661\n",
            "6662\n",
            "6663\n",
            "6664\n",
            "6665\n",
            "6666\n",
            "6667\n",
            "6668\n",
            "6669\n",
            "6670\n",
            "6671\n",
            "6672\n",
            "6673\n",
            "6674\n",
            "6675\n",
            "6676\n",
            "6677\n",
            "6678\n",
            "6679\n",
            "6680\n",
            "6681\n",
            "6682\n",
            "6683\n",
            "6684\n",
            "6685\n",
            "6686\n",
            "6687\n",
            "6688\n",
            "6689\n",
            "6690\n",
            "6691\n",
            "6692\n",
            "6693\n",
            "6694\n",
            "6695\n",
            "6696\n",
            "6697\n",
            "6698\n",
            "6699\n",
            "6700\n",
            "6701\n",
            "6702\n",
            "6703\n",
            "6704\n",
            "6705\n",
            "6706\n",
            "6707\n",
            "6708\n",
            "6709\n",
            "6710\n",
            "6711\n",
            "6712\n",
            "6713\n",
            "6714\n",
            "6715\n",
            "6716\n",
            "6717\n",
            "6718\n",
            "6719\n",
            "6720\n",
            "6721\n",
            "6722\n",
            "6723\n",
            "6724\n",
            "6725\n",
            "6726\n",
            "6727\n",
            "6728\n",
            "6729\n",
            "6730\n",
            "6731\n",
            "6732\n",
            "6733\n",
            "6734\n",
            "6735\n",
            "6736\n",
            "6737\n",
            "6738\n",
            "6739\n",
            "6740\n",
            "6741\n",
            "6742\n",
            "6743\n",
            "6744\n",
            "6745\n",
            "6746\n",
            "6747\n",
            "6748\n",
            "6749\n",
            "6750\n",
            "6751\n",
            "6752\n",
            "6753\n",
            "6754\n",
            "6755\n",
            "6756\n",
            "6757\n",
            "6758\n",
            "6759\n",
            "6760\n",
            "6761\n",
            "6762\n",
            "6763\n",
            "6764\n",
            "6765\n",
            "6766\n",
            "6767\n",
            "6768\n",
            "6769\n",
            "6770\n",
            "6771\n",
            "6772\n",
            "6773\n",
            "6774\n",
            "6775\n",
            "6776\n",
            "6777\n",
            "6778\n",
            "6779\n",
            "6780\n",
            "6781\n",
            "6782\n",
            "6783\n",
            "6784\n",
            "6785\n",
            "6786\n",
            "6787\n",
            "6788\n",
            "6789\n",
            "6790\n",
            "6791\n",
            "6792\n",
            "6793\n",
            "6794\n",
            "6795\n",
            "6796\n",
            "6797\n",
            "6798\n",
            "6799\n",
            "6800\n",
            "6801\n",
            "6802\n",
            "6803\n",
            "6804\n",
            "6805\n",
            "6806\n",
            "6807\n",
            "6808\n",
            "6809\n",
            "6810\n",
            "6811\n",
            "6812\n",
            "6813\n",
            "6814\n",
            "6815\n",
            "6816\n",
            "6817\n",
            "6818\n",
            "6819\n",
            "6820\n",
            "6821\n",
            "6822\n",
            "6823\n",
            "6824\n",
            "6825\n",
            "6826\n",
            "6827\n",
            "6828\n",
            "6829\n",
            "6830\n",
            "6831\n",
            "6832\n",
            "6833\n",
            "6834\n",
            "6835\n",
            "6836\n",
            "6837\n",
            "6838\n",
            "6839\n",
            "6840\n",
            "6841\n",
            "6842\n",
            "6843\n",
            "6844\n",
            "6845\n",
            "6846\n",
            "6847\n",
            "6848\n",
            "6849\n",
            "6850\n",
            "6851\n",
            "6852\n",
            "6853\n",
            "6854\n",
            "6855\n",
            "6856\n",
            "6857\n",
            "6858\n",
            "6859\n",
            "6860\n",
            "6861\n",
            "6862\n",
            "6863\n",
            "6864\n",
            "6865\n",
            "6866\n",
            "6867\n",
            "6868\n",
            "6869\n",
            "6870\n",
            "6871\n",
            "6872\n",
            "6873\n",
            "6874\n",
            "6875\n",
            "6876\n",
            "6877\n",
            "6878\n",
            "6879\n",
            "6880\n",
            "6881\n",
            "6882\n",
            "6883\n",
            "6884\n",
            "6885\n",
            "6886\n",
            "6887\n",
            "6888\n",
            "6889\n",
            "6890\n",
            "6891\n",
            "6892\n",
            "6893\n",
            "6894\n",
            "6895\n",
            "6896\n",
            "6897\n",
            "6898\n",
            "6899\n",
            "6900\n",
            "6901\n",
            "6902\n",
            "6903\n",
            "6904\n",
            "6905\n",
            "6906\n",
            "6907\n",
            "6908\n",
            "6909\n",
            "6910\n",
            "6911\n",
            "6912\n",
            "6913\n",
            "6914\n",
            "6915\n",
            "6916\n",
            "6917\n",
            "6918\n",
            "6919\n",
            "6920\n",
            "6921\n",
            "6922\n",
            "6923\n",
            "6924\n",
            "6925\n",
            "6926\n",
            "6927\n",
            "6928\n",
            "6929\n",
            "6930\n",
            "6931\n",
            "6932\n",
            "6933\n",
            "6934\n",
            "6935\n",
            "6936\n",
            "6937\n",
            "6938\n",
            "6939\n",
            "6940\n",
            "6941\n",
            "6942\n",
            "6943\n",
            "6944\n",
            "6945\n",
            "6946\n",
            "6947\n",
            "6948\n",
            "6949\n",
            "6950\n",
            "6951\n",
            "6952\n",
            "6953\n",
            "6954\n",
            "6955\n",
            "6956\n",
            "6957\n",
            "6958\n",
            "6959\n",
            "6960\n",
            "6961\n",
            "6962\n",
            "6963\n",
            "6964\n",
            "6965\n",
            "6966\n",
            "6967\n",
            "6968\n",
            "6969\n",
            "6970\n",
            "6971\n",
            "6972\n",
            "6973\n",
            "6974\n",
            "6975\n",
            "6976\n",
            "6977\n",
            "6978\n",
            "6979\n",
            "6980\n",
            "6981\n",
            "6982\n",
            "6983\n",
            "6984\n",
            "6985\n",
            "6986\n",
            "6987\n",
            "6988\n",
            "6989\n",
            "6990\n",
            "6991\n",
            "6992\n",
            "6993\n",
            "6994\n",
            "6995\n",
            "6996\n",
            "6997\n",
            "6998\n",
            "6999\n",
            "7000\n",
            "7001\n",
            "7002\n",
            "7003\n",
            "7004\n",
            "7005\n",
            "7006\n",
            "7007\n",
            "7008\n",
            "7009\n",
            "7010\n",
            "7011\n",
            "7012\n",
            "7013\n",
            "7014\n",
            "7015\n",
            "7016\n",
            "7017\n",
            "7018\n",
            "7019\n",
            "7020\n",
            "7021\n",
            "7022\n",
            "7023\n",
            "7024\n",
            "7025\n",
            "7026\n",
            "7027\n",
            "7028\n",
            "7029\n",
            "7030\n",
            "7031\n",
            "7032\n",
            "7033\n",
            "7034\n",
            "7035\n",
            "7036\n",
            "7037\n",
            "7038\n",
            "7039\n",
            "7040\n",
            "7041\n",
            "7042\n",
            "7043\n",
            "7044\n",
            "7045\n",
            "7046\n",
            "7047\n",
            "7048\n",
            "7049\n",
            "7050\n",
            "7051\n",
            "7052\n",
            "7053\n",
            "7054\n",
            "7055\n",
            "7056\n",
            "7057\n",
            "7058\n",
            "7059\n",
            "7060\n",
            "7061\n",
            "7062\n",
            "7063\n",
            "7064\n",
            "7065\n",
            "7066\n",
            "7067\n",
            "7068\n",
            "7069\n",
            "7070\n",
            "7071\n",
            "7072\n",
            "7073\n",
            "7074\n",
            "7075\n",
            "7076\n",
            "7077\n",
            "7078\n",
            "7079\n",
            "7080\n",
            "7081\n",
            "7082\n",
            "7083\n",
            "7084\n",
            "7085\n",
            "7086\n",
            "7087\n",
            "7088\n",
            "7089\n",
            "7090\n",
            "7091\n",
            "7092\n",
            "7093\n",
            "7094\n",
            "7095\n",
            "7096\n",
            "7097\n",
            "7098\n",
            "7099\n",
            "7100\n",
            "7101\n",
            "7102\n",
            "7103\n",
            "7104\n",
            "7105\n",
            "7106\n",
            "7107\n",
            "7108\n",
            "7109\n",
            "7110\n",
            "7111\n",
            "7112\n",
            "7113\n",
            "7114\n",
            "7115\n",
            "7116\n",
            "7117\n",
            "7118\n",
            "7119\n",
            "7120\n",
            "7121\n",
            "7122\n",
            "7123\n",
            "7124\n",
            "7125\n",
            "7126\n",
            "7127\n",
            "7128\n",
            "7129\n",
            "7130\n",
            "7131\n",
            "7132\n",
            "7133\n",
            "7134\n",
            "7135\n",
            "7136\n",
            "7137\n",
            "7138\n",
            "7139\n",
            "7140\n",
            "7141\n",
            "7142\n",
            "7143\n",
            "7144\n",
            "7145\n",
            "7146\n",
            "7147\n",
            "7148\n",
            "7149\n",
            "7150\n",
            "7151\n",
            "7152\n",
            "7153\n",
            "7154\n",
            "7155\n",
            "7156\n",
            "7157\n",
            "7158\n",
            "7159\n",
            "7160\n",
            "7161\n",
            "7162\n",
            "7163\n",
            "7164\n",
            "7165\n",
            "7166\n",
            "7167\n",
            "7168\n",
            "7169\n",
            "7170\n",
            "7171\n",
            "7172\n",
            "7173\n",
            "7174\n",
            "7175\n",
            "7176\n",
            "7177\n",
            "7178\n",
            "7179\n",
            "7180\n",
            "7181\n",
            "7182\n",
            "7183\n",
            "7184\n",
            "7185\n",
            "7186\n",
            "7187\n",
            "7188\n",
            "7189\n",
            "7190\n",
            "7191\n",
            "7192\n",
            "7193\n",
            "7194\n",
            "7195\n",
            "7196\n",
            "7197\n",
            "7198\n",
            "7199\n",
            "7200\n",
            "7201\n",
            "7202\n",
            "7203\n",
            "7204\n",
            "7205\n",
            "7206\n",
            "7207\n",
            "7208\n",
            "7209\n",
            "7210\n",
            "7211\n",
            "7212\n",
            "7213\n",
            "7214\n",
            "7215\n",
            "7216\n",
            "7217\n",
            "7218\n",
            "7219\n",
            "7220\n",
            "7221\n",
            "7222\n",
            "7223\n",
            "7224\n",
            "7225\n",
            "7226\n",
            "7227\n",
            "7228\n",
            "7229\n",
            "7230\n",
            "7231\n",
            "7232\n",
            "7233\n",
            "7234\n",
            "7235\n",
            "7236\n",
            "7237\n",
            "7238\n",
            "7239\n",
            "7240\n",
            "7241\n",
            "7242\n",
            "7243\n",
            "7244\n",
            "7245\n",
            "7246\n",
            "7247\n",
            "7248\n",
            "7249\n",
            "7250\n",
            "7251\n",
            "7252\n",
            "7253\n",
            "7254\n",
            "7255\n",
            "7256\n",
            "7257\n",
            "7258\n",
            "7259\n",
            "7260\n",
            "7261\n",
            "7262\n",
            "7263\n",
            "7264\n",
            "7265\n",
            "7266\n",
            "7267\n",
            "7268\n",
            "7269\n",
            "7270\n",
            "7271\n",
            "7272\n",
            "7273\n",
            "7274\n",
            "7275\n",
            "7276\n",
            "7277\n",
            "7278\n",
            "7279\n",
            "7280\n",
            "7281\n",
            "7282\n",
            "7283\n",
            "7284\n",
            "7285\n",
            "7286\n",
            "7287\n",
            "7288\n",
            "7289\n",
            "7290\n",
            "7291\n",
            "7292\n",
            "7293\n",
            "7294\n",
            "7295\n",
            "7296\n",
            "7297\n",
            "7298\n",
            "7299\n",
            "7300\n",
            "7301\n",
            "7302\n",
            "7303\n",
            "7304\n",
            "7305\n",
            "7306\n",
            "7307\n",
            "7308\n",
            "7309\n",
            "7310\n",
            "7311\n",
            "7312\n",
            "7313\n",
            "7314\n",
            "7315\n",
            "7316\n",
            "7317\n",
            "7318\n",
            "7319\n",
            "7320\n",
            "7321\n",
            "7322\n",
            "7323\n",
            "7324\n",
            "7325\n",
            "7326\n",
            "7327\n",
            "7328\n",
            "7329\n",
            "7330\n",
            "7331\n",
            "7332\n",
            "7333\n",
            "7334\n",
            "7335\n",
            "7336\n",
            "7337\n",
            "7338\n",
            "7339\n",
            "7340\n",
            "7341\n",
            "7342\n",
            "7343\n",
            "7344\n",
            "7345\n",
            "7346\n",
            "7347\n",
            "7348\n",
            "7349\n",
            "7350\n",
            "7351\n",
            "7352\n",
            "7353\n",
            "7354\n",
            "7355\n",
            "7356\n",
            "7357\n",
            "7358\n",
            "7359\n",
            "7360\n",
            "7361\n",
            "7362\n",
            "7363\n",
            "7364\n",
            "7365\n",
            "7366\n",
            "7367\n",
            "7368\n",
            "7369\n",
            "7370\n",
            "7371\n",
            "7372\n",
            "7373\n",
            "7374\n",
            "7375\n",
            "7376\n",
            "7377\n",
            "7378\n",
            "7379\n",
            "7380\n",
            "7381\n",
            "7382\n",
            "7383\n",
            "7384\n",
            "7385\n",
            "7386\n",
            "7387\n",
            "7388\n",
            "7389\n",
            "7390\n",
            "7391\n",
            "7392\n",
            "7393\n",
            "7394\n",
            "7395\n",
            "7396\n",
            "7397\n",
            "7398\n",
            "7399\n",
            "7400\n",
            "7401\n",
            "7402\n",
            "7403\n",
            "7404\n",
            "7405\n",
            "7406\n",
            "7407\n",
            "7408\n",
            "7409\n",
            "7410\n",
            "7411\n",
            "7412\n",
            "7413\n",
            "7414\n",
            "7415\n",
            "7416\n",
            "7417\n",
            "7418\n",
            "7419\n",
            "7420\n",
            "7421\n",
            "7422\n",
            "7423\n",
            "7424\n",
            "7425\n",
            "7426\n",
            "7427\n",
            "7428\n",
            "7429\n",
            "7430\n",
            "7431\n",
            "7432\n",
            "7433\n",
            "7434\n",
            "7435\n",
            "7436\n",
            "7437\n",
            "7438\n",
            "7439\n",
            "7440\n",
            "7441\n",
            "7442\n",
            "7443\n",
            "7444\n",
            "7445\n",
            "7446\n",
            "7447\n",
            "7448\n",
            "7449\n",
            "7450\n",
            "7451\n",
            "7452\n",
            "7453\n",
            "7454\n",
            "7455\n",
            "7456\n",
            "7457\n",
            "7458\n",
            "7459\n",
            "7460\n",
            "7461\n",
            "7462\n",
            "7463\n",
            "7464\n",
            "7465\n",
            "7466\n",
            "7467\n",
            "7468\n",
            "7469\n",
            "7470\n",
            "7471\n",
            "7472\n",
            "7473\n",
            "7474\n",
            "7475\n",
            "7476\n",
            "7477\n",
            "7478\n",
            "7479\n",
            "7480\n",
            "7481\n",
            "7482\n",
            "7483\n",
            "7484\n",
            "7485\n",
            "7486\n",
            "7487\n",
            "7488\n",
            "7489\n",
            "7490\n",
            "7491\n",
            "7492\n",
            "7493\n",
            "7494\n",
            "7495\n",
            "7496\n",
            "7497\n",
            "7498\n",
            "7499\n",
            "7500\n",
            "7501\n",
            "7502\n",
            "7503\n",
            "7504\n",
            "7505\n",
            "7506\n",
            "7507\n",
            "7508\n",
            "7509\n",
            "7510\n",
            "7511\n",
            "7512\n",
            "7513\n",
            "7514\n",
            "7515\n",
            "7516\n",
            "7517\n",
            "7518\n",
            "7519\n",
            "7520\n",
            "7521\n",
            "7522\n",
            "7523\n",
            "7524\n",
            "7525\n",
            "7526\n",
            "7527\n",
            "7528\n",
            "7529\n",
            "7530\n",
            "7531\n",
            "7532\n",
            "7533\n",
            "7534\n",
            "7535\n",
            "7536\n",
            "7537\n",
            "7538\n",
            "7539\n",
            "7540\n",
            "7541\n",
            "7542\n",
            "7543\n",
            "7544\n",
            "7545\n",
            "7546\n",
            "7547\n",
            "7548\n",
            "7549\n",
            "7550\n",
            "7551\n",
            "7552\n",
            "7553\n",
            "7554\n",
            "7555\n",
            "7556\n",
            "7557\n",
            "7558\n",
            "7559\n",
            "7560\n",
            "7561\n",
            "7562\n",
            "7563\n",
            "7564\n",
            "7565\n",
            "7566\n",
            "7567\n",
            "7568\n",
            "7569\n",
            "7570\n",
            "7571\n",
            "7572\n",
            "7573\n",
            "7574\n",
            "7575\n",
            "7576\n",
            "7577\n",
            "7578\n",
            "7579\n",
            "7580\n",
            "7581\n",
            "7582\n",
            "7583\n",
            "7584\n",
            "7585\n",
            "7586\n",
            "7587\n",
            "7588\n",
            "7589\n",
            "7590\n",
            "7591\n",
            "7592\n",
            "7593\n",
            "7594\n",
            "7595\n",
            "7596\n",
            "7597\n",
            "7598\n",
            "7599\n",
            "7600\n",
            "7601\n",
            "7602\n",
            "7603\n",
            "7604\n",
            "7605\n",
            "7606\n",
            "7607\n",
            "7608\n",
            "7609\n",
            "7610\n",
            "7611\n",
            "7612\n",
            "7613\n",
            "7614\n",
            "7615\n",
            "7616\n",
            "7617\n",
            "7618\n",
            "7619\n",
            "7620\n",
            "7621\n",
            "7622\n",
            "7623\n",
            "7624\n",
            "7625\n",
            "7626\n",
            "7627\n",
            "7628\n",
            "7629\n",
            "7630\n",
            "7631\n",
            "7632\n",
            "7633\n",
            "7634\n",
            "7635\n",
            "7636\n",
            "7637\n",
            "7638\n",
            "7639\n",
            "7640\n",
            "7641\n",
            "7642\n",
            "7643\n",
            "7644\n",
            "7645\n",
            "7646\n",
            "7647\n",
            "7648\n",
            "7649\n",
            "7650\n",
            "7651\n",
            "7652\n",
            "7653\n",
            "7654\n",
            "7655\n",
            "7656\n",
            "7657\n",
            "7658\n",
            "7659\n",
            "7660\n",
            "7661\n",
            "7662\n",
            "7663\n",
            "7664\n",
            "7665\n",
            "7666\n",
            "7667\n",
            "7668\n",
            "7669\n",
            "7670\n",
            "7671\n",
            "7672\n",
            "7673\n",
            "7674\n",
            "7675\n",
            "7676\n",
            "7677\n",
            "7678\n",
            "7679\n",
            "7680\n",
            "7681\n",
            "7682\n",
            "7683\n",
            "7684\n",
            "7685\n",
            "7686\n",
            "7687\n",
            "7688\n",
            "7689\n",
            "7690\n",
            "7691\n",
            "7692\n",
            "7693\n",
            "7694\n",
            "7695\n",
            "7696\n",
            "7697\n",
            "7698\n",
            "7699\n",
            "7700\n",
            "7701\n",
            "7702\n",
            "7703\n",
            "7704\n",
            "7705\n",
            "7706\n",
            "7707\n",
            "7708\n",
            "7709\n",
            "7710\n",
            "7711\n",
            "7712\n",
            "7713\n",
            "7714\n",
            "7715\n",
            "7716\n",
            "7717\n",
            "7718\n",
            "7719\n",
            "7720\n",
            "7721\n",
            "7722\n",
            "7723\n",
            "7724\n",
            "7725\n",
            "7726\n",
            "7727\n",
            "7728\n",
            "7729\n",
            "7730\n",
            "7731\n",
            "7732\n",
            "7733\n",
            "7734\n",
            "7735\n",
            "7736\n",
            "7737\n",
            "7738\n",
            "7739\n",
            "7740\n",
            "7741\n",
            "7742\n",
            "7743\n",
            "7744\n",
            "7745\n",
            "7746\n",
            "7747\n",
            "7748\n",
            "7749\n",
            "7750\n",
            "7751\n",
            "7752\n",
            "7753\n",
            "7754\n",
            "7755\n",
            "7756\n",
            "7757\n",
            "7758\n",
            "7759\n",
            "7760\n",
            "7761\n",
            "7762\n",
            "7763\n",
            "7764\n",
            "7765\n",
            "7766\n",
            "7767\n",
            "7768\n",
            "7769\n",
            "7770\n",
            "7771\n",
            "7772\n",
            "7773\n",
            "7774\n",
            "7775\n",
            "7776\n",
            "7777\n",
            "7778\n",
            "7779\n",
            "7780\n",
            "7781\n",
            "7782\n",
            "7783\n",
            "7784\n",
            "7785\n",
            "7786\n",
            "7787\n",
            "7788\n",
            "7789\n",
            "7790\n",
            "7791\n",
            "7792\n",
            "7793\n",
            "7794\n",
            "7795\n",
            "7796\n",
            "7797\n",
            "7798\n",
            "7799\n",
            "7800\n",
            "7801\n",
            "7802\n",
            "7803\n",
            "7804\n",
            "7805\n",
            "7806\n",
            "7807\n",
            "7808\n",
            "7809\n",
            "7810\n",
            "7811\n",
            "7812\n",
            "7813\n",
            "7814\n",
            "7815\n",
            "7816\n",
            "7817\n",
            "7818\n",
            "7819\n",
            "7820\n",
            "7821\n",
            "7822\n",
            "7823\n",
            "7824\n",
            "7825\n",
            "7826\n",
            "7827\n",
            "7828\n",
            "7829\n",
            "7830\n",
            "7831\n",
            "7832\n",
            "7833\n",
            "7834\n",
            "7835\n",
            "7836\n",
            "7837\n",
            "7838\n",
            "7839\n",
            "7840\n",
            "7841\n",
            "7842\n",
            "7843\n",
            "7844\n",
            "7845\n",
            "7846\n",
            "7847\n",
            "7848\n",
            "7849\n",
            "7850\n",
            "7851\n",
            "7852\n",
            "7853\n",
            "7854\n",
            "7855\n",
            "7856\n",
            "7857\n",
            "7858\n",
            "7859\n",
            "7860\n",
            "7861\n",
            "7862\n",
            "7863\n",
            "7864\n",
            "7865\n",
            "7866\n",
            "7867\n",
            "7868\n",
            "7869\n",
            "7870\n",
            "7871\n",
            "7872\n",
            "7873\n",
            "7874\n",
            "7875\n",
            "7876\n",
            "7877\n",
            "7878\n",
            "7879\n",
            "7880\n",
            "7881\n",
            "7882\n",
            "7883\n",
            "7884\n",
            "7885\n",
            "7886\n",
            "7887\n",
            "7888\n",
            "7889\n",
            "7890\n",
            "7891\n",
            "7892\n",
            "7893\n",
            "7894\n",
            "7895\n",
            "7896\n",
            "7897\n",
            "7898\n",
            "7899\n",
            "7900\n",
            "7901\n",
            "7902\n",
            "7903\n",
            "7904\n",
            "7905\n",
            "7906\n",
            "7907\n",
            "7908\n",
            "7909\n",
            "7910\n",
            "7911\n",
            "7912\n",
            "7913\n",
            "7914\n",
            "7915\n",
            "7916\n",
            "7917\n",
            "7918\n",
            "7919\n",
            "7920\n",
            "7921\n",
            "7922\n",
            "7923\n",
            "7924\n",
            "7925\n",
            "7926\n",
            "7927\n",
            "7928\n",
            "7929\n",
            "7930\n",
            "7931\n",
            "7932\n",
            "7933\n",
            "7934\n",
            "7935\n",
            "7936\n",
            "7937\n",
            "7938\n",
            "7939\n",
            "7940\n",
            "7941\n",
            "7942\n",
            "7943\n",
            "7944\n",
            "7945\n",
            "7946\n",
            "7947\n",
            "7948\n",
            "7949\n",
            "7950\n",
            "7951\n",
            "7952\n",
            "7953\n",
            "7954\n",
            "7955\n",
            "7956\n",
            "7957\n",
            "7958\n",
            "7959\n",
            "7960\n",
            "7961\n",
            "7962\n",
            "7963\n",
            "7964\n",
            "7965\n",
            "7966\n",
            "7967\n",
            "7968\n",
            "7969\n",
            "7970\n",
            "7971\n",
            "7972\n",
            "7973\n",
            "7974\n",
            "7975\n",
            "7976\n",
            "7977\n",
            "7978\n",
            "7979\n",
            "7980\n",
            "7981\n",
            "7982\n",
            "7983\n",
            "7984\n",
            "7985\n",
            "7986\n",
            "7987\n",
            "7988\n",
            "7989\n",
            "7990\n",
            "7991\n",
            "7992\n",
            "7993\n",
            "7994\n",
            "7995\n",
            "7996\n",
            "7997\n",
            "7998\n",
            "7999\n",
            "8000\n",
            "8001\n",
            "8002\n",
            "8003\n",
            "8004\n",
            "8005\n",
            "8006\n",
            "8007\n",
            "8008\n",
            "8009\n",
            "8010\n",
            "8011\n",
            "8012\n",
            "8013\n",
            "8014\n",
            "8015\n",
            "8016\n",
            "8017\n",
            "8018\n",
            "8019\n",
            "8020\n",
            "8021\n",
            "8022\n",
            "8023\n",
            "8024\n",
            "8025\n",
            "8026\n",
            "8027\n",
            "8028\n",
            "8029\n",
            "8030\n",
            "8031\n",
            "8032\n",
            "8033\n",
            "8034\n",
            "8035\n",
            "8036\n",
            "8037\n",
            "8038\n",
            "8039\n",
            "8040\n",
            "8041\n",
            "8042\n",
            "8043\n",
            "8044\n",
            "8045\n",
            "8046\n",
            "8047\n",
            "8048\n",
            "8049\n",
            "8050\n",
            "8051\n",
            "8052\n",
            "8053\n",
            "8054\n",
            "8055\n",
            "8056\n",
            "8057\n",
            "8058\n",
            "8059\n",
            "8060\n",
            "8061\n",
            "8062\n",
            "8063\n",
            "8064\n",
            "8065\n",
            "8066\n",
            "8067\n",
            "8068\n",
            "8069\n",
            "8070\n",
            "8071\n",
            "8072\n",
            "8073\n",
            "8074\n",
            "8075\n",
            "8076\n",
            "8077\n",
            "8078\n",
            "8079\n",
            "8080\n",
            "8081\n",
            "8082\n",
            "8083\n",
            "8084\n",
            "8085\n",
            "8086\n",
            "8087\n",
            "8088\n",
            "8089\n",
            "8090\n",
            "8091\n",
            "8092\n",
            "8093\n",
            "8094\n",
            "8095\n",
            "8096\n",
            "8097\n",
            "8098\n",
            "8099\n",
            "8100\n",
            "8101\n",
            "8102\n",
            "8103\n",
            "8104\n",
            "8105\n",
            "8106\n",
            "8107\n",
            "8108\n",
            "8109\n",
            "8110\n",
            "8111\n",
            "8112\n",
            "8113\n",
            "8114\n",
            "8115\n",
            "8116\n",
            "8117\n",
            "8118\n",
            "8119\n",
            "8120\n",
            "8121\n",
            "8122\n",
            "8123\n",
            "8124\n",
            "8125\n",
            "8126\n",
            "8127\n",
            "8128\n",
            "8129\n",
            "8130\n",
            "8131\n",
            "8132\n",
            "8133\n",
            "8134\n",
            "8135\n",
            "8136\n",
            "8137\n",
            "8138\n",
            "8139\n",
            "8140\n",
            "8141\n",
            "8142\n",
            "8143\n",
            "8144\n",
            "8145\n",
            "8146\n",
            "8147\n",
            "8148\n",
            "8149\n",
            "8150\n",
            "8151\n",
            "8152\n",
            "8153\n",
            "8154\n",
            "8155\n",
            "8156\n",
            "8157\n",
            "8158\n",
            "8159\n",
            "8160\n",
            "8161\n",
            "8162\n",
            "8163\n",
            "8164\n",
            "8165\n",
            "8166\n",
            "8167\n",
            "8168\n",
            "8169\n",
            "8170\n",
            "8171\n",
            "8172\n",
            "8173\n",
            "8174\n",
            "8175\n",
            "8176\n",
            "8177\n",
            "8178\n",
            "8179\n",
            "8180\n",
            "8181\n",
            "8182\n",
            "8183\n",
            "8184\n",
            "8185\n",
            "8186\n",
            "8187\n",
            "8188\n",
            "8189\n",
            "8190\n",
            "8191\n",
            "8192\n",
            "8193\n",
            "8194\n",
            "8195\n",
            "8196\n",
            "8197\n",
            "8198\n",
            "8199\n",
            "8200\n",
            "8201\n",
            "8202\n",
            "8203\n",
            "8204\n",
            "8205\n",
            "8206\n",
            "8207\n",
            "8208\n",
            "8209\n",
            "8210\n",
            "8211\n",
            "8212\n",
            "8213\n",
            "8214\n",
            "8215\n",
            "8216\n",
            "8217\n",
            "8218\n",
            "8219\n",
            "8220\n",
            "8221\n",
            "8222\n",
            "8223\n",
            "8224\n",
            "8225\n",
            "8226\n",
            "8227\n",
            "8228\n",
            "8229\n",
            "8230\n",
            "8231\n",
            "8232\n",
            "8233\n",
            "8234\n",
            "8235\n",
            "8236\n",
            "8237\n",
            "8238\n",
            "8239\n",
            "8240\n",
            "8241\n",
            "8242\n",
            "8243\n",
            "8244\n",
            "8245\n",
            "8246\n",
            "8247\n",
            "8248\n",
            "8249\n",
            "8250\n",
            "8251\n",
            "8252\n",
            "8253\n",
            "8254\n",
            "8255\n",
            "8256\n",
            "8257\n",
            "8258\n",
            "8259\n",
            "8260\n",
            "8261\n",
            "8262\n",
            "8263\n",
            "8264\n",
            "8265\n",
            "8266\n",
            "8267\n",
            "8268\n",
            "8269\n",
            "8270\n",
            "8271\n",
            "8272\n",
            "8273\n",
            "8274\n",
            "8275\n",
            "8276\n",
            "8277\n",
            "8278\n",
            "8279\n",
            "8280\n",
            "8281\n",
            "8282\n",
            "8283\n",
            "8284\n",
            "8285\n",
            "8286\n",
            "8287\n",
            "8288\n",
            "8289\n",
            "8290\n",
            "8291\n",
            "8292\n",
            "8293\n",
            "8294\n",
            "8295\n",
            "8296\n",
            "8297\n",
            "8298\n",
            "8299\n",
            "8300\n",
            "8301\n",
            "8302\n",
            "8303\n",
            "8304\n",
            "8305\n",
            "8306\n",
            "8307\n",
            "8308\n",
            "8309\n",
            "8310\n",
            "8311\n",
            "8312\n",
            "8313\n",
            "8314\n",
            "8315\n",
            "8316\n",
            "8317\n",
            "8318\n",
            "8319\n",
            "8320\n",
            "8321\n",
            "8322\n",
            "8323\n",
            "8324\n",
            "8325\n",
            "8326\n",
            "8327\n",
            "8328\n",
            "8329\n",
            "8330\n",
            "8331\n",
            "8332\n",
            "8333\n",
            "8334\n",
            "8335\n",
            "8336\n",
            "8337\n",
            "8338\n",
            "8339\n",
            "8340\n",
            "8341\n",
            "8342\n",
            "8343\n",
            "8344\n",
            "8345\n",
            "8346\n",
            "8347\n",
            "8348\n",
            "8349\n",
            "8350\n",
            "8351\n",
            "8352\n",
            "8353\n",
            "8354\n",
            "8355\n",
            "8356\n",
            "8357\n",
            "8358\n",
            "8359\n",
            "8360\n",
            "8361\n",
            "8362\n",
            "8363\n",
            "8364\n",
            "8365\n",
            "8366\n",
            "8367\n",
            "8368\n",
            "8369\n",
            "8370\n",
            "8371\n",
            "8372\n",
            "8373\n",
            "8374\n",
            "8375\n",
            "8376\n",
            "8377\n",
            "8378\n",
            "8379\n",
            "8380\n",
            "8381\n",
            "8382\n",
            "8383\n",
            "8384\n",
            "8385\n",
            "8386\n",
            "8387\n",
            "8388\n",
            "8389\n",
            "8390\n",
            "8391\n",
            "8392\n",
            "8393\n",
            "8394\n",
            "8395\n",
            "8396\n",
            "8397\n",
            "8398\n",
            "8399\n",
            "8400\n",
            "8401\n",
            "8402\n",
            "8403\n",
            "8404\n",
            "8405\n",
            "8406\n",
            "8407\n",
            "8408\n",
            "8409\n",
            "8410\n",
            "8411\n",
            "8412\n",
            "8413\n",
            "8414\n",
            "8415\n",
            "8416\n",
            "8417\n",
            "8418\n",
            "8419\n",
            "8420\n",
            "8421\n",
            "8422\n",
            "8423\n",
            "8424\n",
            "8425\n",
            "8426\n",
            "8427\n",
            "8428\n",
            "8429\n",
            "8430\n",
            "8431\n",
            "8432\n",
            "8433\n",
            "8434\n",
            "8435\n",
            "8436\n",
            "8437\n",
            "8438\n",
            "8439\n",
            "8440\n",
            "8441\n",
            "8442\n",
            "8443\n",
            "8444\n",
            "8445\n",
            "8446\n",
            "8447\n",
            "8448\n",
            "8449\n",
            "8450\n",
            "8451\n",
            "8452\n",
            "8453\n",
            "8454\n",
            "8455\n",
            "8456\n",
            "8457\n",
            "8458\n",
            "8459\n",
            "8460\n",
            "8461\n",
            "8462\n",
            "8463\n",
            "8464\n",
            "8465\n",
            "8466\n",
            "8467\n",
            "8468\n",
            "8469\n",
            "8470\n",
            "8471\n",
            "8472\n",
            "8473\n",
            "8474\n",
            "8475\n",
            "8476\n",
            "8477\n",
            "8478\n",
            "8479\n",
            "8480\n",
            "8481\n",
            "8482\n",
            "8483\n",
            "8484\n",
            "8485\n",
            "8486\n",
            "8487\n",
            "8488\n",
            "8489\n",
            "8490\n",
            "8491\n",
            "8492\n",
            "8493\n",
            "8494\n",
            "8495\n",
            "8496\n",
            "8497\n",
            "8498\n",
            "8499\n",
            "8500\n",
            "8501\n",
            "8502\n",
            "8503\n",
            "8504\n",
            "8505\n",
            "8506\n",
            "8507\n",
            "8508\n",
            "8509\n",
            "8510\n",
            "8511\n",
            "8512\n",
            "8513\n",
            "8514\n",
            "8515\n",
            "8516\n",
            "8517\n",
            "8518\n",
            "8519\n",
            "8520\n",
            "8521\n",
            "8522\n",
            "8523\n",
            "8524\n",
            "8525\n",
            "8526\n",
            "8527\n",
            "8528\n",
            "8529\n",
            "8530\n",
            "8531\n",
            "8532\n",
            "8533\n",
            "8534\n",
            "8535\n",
            "8536\n",
            "8537\n",
            "8538\n",
            "8539\n",
            "8540\n",
            "8541\n",
            "8542\n",
            "8543\n",
            "8544\n",
            "8545\n",
            "8546\n",
            "8547\n",
            "8548\n",
            "8549\n",
            "8550\n",
            "8551\n",
            "8552\n",
            "8553\n",
            "8554\n",
            "8555\n",
            "8556\n",
            "8557\n",
            "8558\n",
            "8559\n",
            "8560\n",
            "8561\n",
            "8562\n",
            "8563\n",
            "8564\n",
            "8565\n",
            "8566\n",
            "8567\n",
            "8568\n",
            "8569\n",
            "8570\n",
            "8571\n",
            "8572\n",
            "8573\n",
            "8574\n",
            "8575\n",
            "8576\n",
            "8577\n",
            "8578\n",
            "8579\n",
            "8580\n",
            "8581\n",
            "8582\n",
            "8583\n",
            "8584\n",
            "8585\n",
            "8586\n",
            "8587\n",
            "8588\n",
            "8589\n",
            "8590\n",
            "8591\n",
            "8592\n",
            "8593\n",
            "8594\n",
            "8595\n",
            "8596\n",
            "8597\n",
            "8598\n",
            "8599\n",
            "8600\n",
            "8601\n",
            "8602\n",
            "8603\n",
            "8604\n",
            "8605\n",
            "8606\n",
            "8607\n",
            "8608\n",
            "8609\n",
            "8610\n",
            "8611\n",
            "8612\n",
            "8613\n",
            "8614\n",
            "8615\n",
            "8616\n",
            "8617\n",
            "8618\n",
            "8619\n",
            "8620\n",
            "8621\n",
            "8622\n",
            "8623\n",
            "8624\n",
            "8625\n",
            "8626\n",
            "8627\n",
            "8628\n",
            "8629\n",
            "8630\n",
            "8631\n",
            "8632\n",
            "8633\n",
            "8634\n",
            "8635\n",
            "8636\n",
            "8637\n",
            "8638\n",
            "8639\n",
            "8640\n",
            "8641\n",
            "8642\n",
            "8643\n",
            "8644\n",
            "8645\n",
            "8646\n",
            "8647\n",
            "8648\n",
            "8649\n",
            "8650\n",
            "8651\n",
            "8652\n",
            "8653\n",
            "8654\n",
            "8655\n",
            "8656\n",
            "8657\n",
            "8658\n",
            "8659\n",
            "8660\n",
            "8661\n",
            "8662\n",
            "8663\n",
            "8664\n",
            "8665\n",
            "8666\n",
            "8667\n",
            "8668\n",
            "8669\n",
            "8670\n",
            "8671\n",
            "8672\n",
            "8673\n",
            "8674\n",
            "8675\n",
            "8676\n",
            "8677\n",
            "8678\n",
            "8679\n",
            "8680\n",
            "8681\n",
            "8682\n",
            "8683\n",
            "8684\n",
            "8685\n",
            "8686\n",
            "8687\n",
            "8688\n",
            "8689\n",
            "8690\n",
            "8691\n",
            "8692\n",
            "8693\n",
            "8694\n",
            "8695\n",
            "8696\n",
            "8697\n",
            "8698\n",
            "8699\n",
            "8700\n",
            "8701\n",
            "8702\n",
            "8703\n",
            "8704\n",
            "8705\n",
            "8706\n",
            "8707\n",
            "8708\n",
            "8709\n",
            "8710\n",
            "8711\n",
            "8712\n",
            "8713\n",
            "8714\n",
            "8715\n",
            "8716\n",
            "8717\n",
            "8718\n",
            "8719\n",
            "8720\n",
            "8721\n",
            "8722\n",
            "8723\n",
            "8724\n",
            "8725\n",
            "8726\n",
            "8727\n",
            "8728\n",
            "8729\n",
            "8730\n",
            "8731\n",
            "8732\n",
            "8733\n",
            "8734\n",
            "8735\n",
            "8736\n",
            "8737\n",
            "8738\n",
            "8739\n",
            "8740\n",
            "8741\n",
            "8742\n",
            "8743\n",
            "8744\n",
            "8745\n",
            "8746\n",
            "8747\n",
            "8748\n",
            "8749\n",
            "8750\n",
            "8751\n",
            "8752\n",
            "8753\n",
            "8754\n",
            "8755\n",
            "8756\n",
            "8757\n",
            "8758\n",
            "8759\n",
            "8760\n",
            "8761\n",
            "8762\n",
            "8763\n",
            "8764\n",
            "8765\n",
            "8766\n",
            "8767\n",
            "8768\n",
            "8769\n",
            "8770\n",
            "8771\n",
            "8772\n",
            "8773\n",
            "8774\n",
            "8775\n",
            "8776\n",
            "8777\n",
            "8778\n",
            "8779\n",
            "8780\n",
            "8781\n",
            "8782\n",
            "8783\n",
            "8784\n",
            "8785\n",
            "8786\n",
            "8787\n",
            "8788\n",
            "8789\n",
            "8790\n",
            "8791\n",
            "8792\n",
            "8793\n",
            "8794\n",
            "8795\n",
            "8796\n",
            "8797\n",
            "8798\n",
            "8799\n",
            "8800\n",
            "8801\n",
            "8802\n",
            "8803\n",
            "8804\n",
            "8805\n",
            "8806\n",
            "8807\n",
            "8808\n",
            "8809\n",
            "8810\n",
            "8811\n",
            "8812\n",
            "8813\n",
            "8814\n",
            "8815\n",
            "8816\n",
            "8817\n",
            "8818\n",
            "8819\n",
            "8820\n",
            "8821\n",
            "8822\n",
            "8823\n",
            "8824\n",
            "8825\n",
            "8826\n",
            "8827\n",
            "8828\n",
            "8829\n",
            "8830\n",
            "8831\n",
            "8832\n",
            "8833\n",
            "8834\n",
            "8835\n",
            "8836\n",
            "8837\n",
            "8838\n",
            "8839\n",
            "8840\n",
            "8841\n",
            "8842\n",
            "8843\n",
            "8844\n",
            "8845\n",
            "8846\n",
            "8847\n",
            "8848\n",
            "8849\n",
            "8850\n",
            "8851\n",
            "8852\n",
            "8853\n",
            "8854\n",
            "8855\n",
            "8856\n",
            "8857\n",
            "8858\n",
            "8859\n",
            "8860\n",
            "8861\n",
            "8862\n",
            "8863\n",
            "8864\n",
            "8865\n",
            "8866\n",
            "8867\n",
            "8868\n",
            "8869\n",
            "8870\n",
            "8871\n",
            "8872\n",
            "8873\n",
            "8874\n",
            "8875\n",
            "8876\n",
            "8877\n",
            "8878\n",
            "8879\n",
            "8880\n",
            "8881\n",
            "8882\n",
            "8883\n",
            "8884\n",
            "8885\n",
            "8886\n",
            "8887\n",
            "8888\n",
            "8889\n",
            "8890\n",
            "8891\n",
            "8892\n",
            "8893\n",
            "8894\n",
            "8895\n",
            "8896\n",
            "8897\n",
            "8898\n",
            "8899\n",
            "8900\n",
            "8901\n",
            "8902\n",
            "8903\n",
            "8904\n",
            "8905\n",
            "8906\n",
            "8907\n",
            "8908\n",
            "8909\n",
            "8910\n",
            "8911\n",
            "8912\n",
            "8913\n",
            "8914\n",
            "8915\n",
            "8916\n",
            "8917\n",
            "8918\n",
            "8919\n",
            "8920\n",
            "8921\n",
            "8922\n",
            "8923\n",
            "8924\n",
            "8925\n",
            "8926\n",
            "8927\n",
            "8928\n",
            "8929\n",
            "8930\n",
            "8931\n",
            "8932\n",
            "8933\n",
            "8934\n",
            "8935\n",
            "8936\n",
            "8937\n",
            "8938\n",
            "8939\n",
            "8940\n",
            "8941\n",
            "8942\n",
            "8943\n",
            "8944\n",
            "8945\n",
            "8946\n",
            "8947\n",
            "8948\n",
            "8949\n",
            "8950\n",
            "8951\n",
            "8952\n",
            "8953\n",
            "8954\n",
            "8955\n",
            "8956\n",
            "8957\n",
            "8958\n",
            "8959\n",
            "8960\n",
            "8961\n",
            "8962\n",
            "8963\n",
            "8964\n",
            "8965\n",
            "8966\n",
            "8967\n",
            "8968\n",
            "8969\n",
            "8970\n",
            "8971\n",
            "8972\n",
            "8973\n",
            "8974\n",
            "8975\n",
            "8976\n",
            "8977\n",
            "8978\n",
            "8979\n",
            "8980\n",
            "8981\n",
            "8982\n",
            "8983\n",
            "8984\n",
            "8985\n",
            "8986\n",
            "8987\n",
            "8988\n",
            "8989\n",
            "8990\n",
            "8991\n",
            "8992\n",
            "8993\n",
            "8994\n",
            "8995\n",
            "8996\n",
            "8997\n",
            "8998\n",
            "8999\n",
            "9000\n",
            "9001\n",
            "9002\n",
            "9003\n",
            "9004\n",
            "9005\n",
            "9006\n",
            "9007\n",
            "9008\n",
            "9009\n",
            "9010\n",
            "9011\n",
            "9012\n",
            "9013\n",
            "9014\n",
            "9015\n",
            "9016\n",
            "9017\n",
            "9018\n",
            "9019\n",
            "9020\n",
            "9021\n",
            "9022\n",
            "9023\n",
            "9024\n",
            "9025\n",
            "9026\n",
            "9027\n",
            "9028\n",
            "9029\n",
            "9030\n",
            "9031\n",
            "9032\n",
            "9033\n",
            "9034\n",
            "9035\n",
            "9036\n",
            "9037\n",
            "9038\n",
            "9039\n",
            "9040\n",
            "9041\n",
            "9042\n",
            "9043\n",
            "9044\n",
            "9045\n",
            "9046\n",
            "9047\n",
            "9048\n",
            "9049\n",
            "9050\n",
            "9051\n",
            "9052\n",
            "9053\n",
            "9054\n",
            "9055\n",
            "9056\n",
            "9057\n",
            "9058\n",
            "9059\n",
            "9060\n",
            "9061\n",
            "9062\n",
            "9063\n",
            "9064\n",
            "9065\n",
            "9066\n",
            "9067\n",
            "9068\n",
            "9069\n",
            "9070\n",
            "9071\n",
            "9072\n",
            "9073\n",
            "9074\n",
            "9075\n",
            "9076\n",
            "9077\n",
            "9078\n",
            "9079\n",
            "9080\n",
            "9081\n",
            "9082\n",
            "9083\n",
            "9084\n",
            "9085\n",
            "9086\n",
            "9087\n",
            "9088\n",
            "9089\n",
            "9090\n",
            "9091\n",
            "9092\n",
            "9093\n",
            "9094\n",
            "9095\n",
            "9096\n",
            "9097\n",
            "9098\n",
            "9099\n",
            "9100\n",
            "9101\n",
            "9102\n",
            "9103\n",
            "9104\n",
            "9105\n",
            "9106\n",
            "9107\n",
            "9108\n",
            "9109\n",
            "9110\n",
            "9111\n",
            "9112\n",
            "9113\n",
            "9114\n",
            "9115\n",
            "9116\n",
            "9117\n",
            "9118\n",
            "9119\n",
            "9120\n",
            "9121\n",
            "9122\n",
            "9123\n",
            "9124\n",
            "9125\n",
            "9126\n",
            "9127\n",
            "9128\n",
            "9129\n",
            "9130\n",
            "9131\n",
            "9132\n",
            "9133\n",
            "9134\n",
            "9135\n",
            "9136\n",
            "9137\n",
            "9138\n",
            "9139\n",
            "9140\n",
            "9141\n",
            "9142\n",
            "9143\n",
            "9144\n",
            "9145\n",
            "9146\n",
            "9147\n",
            "9148\n",
            "9149\n",
            "9150\n",
            "9151\n",
            "9152\n",
            "9153\n",
            "9154\n",
            "9155\n",
            "9156\n",
            "9157\n",
            "9158\n",
            "9159\n",
            "9160\n",
            "9161\n",
            "9162\n",
            "9163\n",
            "9164\n",
            "9165\n",
            "9166\n",
            "9167\n",
            "9168\n",
            "9169\n",
            "9170\n",
            "9171\n",
            "9172\n",
            "9173\n",
            "9174\n",
            "9175\n",
            "9176\n",
            "9177\n",
            "9178\n",
            "9179\n",
            "9180\n",
            "9181\n",
            "9182\n",
            "9183\n",
            "9184\n",
            "9185\n",
            "9186\n",
            "9187\n",
            "9188\n",
            "9189\n",
            "9190\n",
            "9191\n",
            "9192\n",
            "9193\n",
            "9194\n",
            "9195\n",
            "9196\n",
            "9197\n",
            "9198\n",
            "9199\n",
            "9200\n",
            "9201\n",
            "9202\n",
            "9203\n",
            "9204\n",
            "9205\n",
            "9206\n",
            "9207\n",
            "9208\n",
            "9209\n",
            "9210\n",
            "9211\n",
            "9212\n",
            "9213\n",
            "9214\n",
            "9215\n",
            "9216\n",
            "9217\n",
            "9218\n",
            "9219\n",
            "9220\n",
            "9221\n",
            "9222\n",
            "9223\n",
            "9224\n",
            "9225\n",
            "9226\n",
            "9227\n",
            "9228\n",
            "9229\n",
            "9230\n",
            "9231\n",
            "9232\n",
            "9233\n",
            "9234\n",
            "9235\n",
            "9236\n",
            "9237\n",
            "9238\n",
            "9239\n",
            "9240\n",
            "9241\n",
            "9242\n",
            "9243\n",
            "9244\n",
            "9245\n",
            "9246\n",
            "9247\n",
            "9248\n",
            "9249\n",
            "9250\n",
            "9251\n",
            "9252\n",
            "9253\n",
            "9254\n",
            "9255\n",
            "9256\n",
            "9257\n",
            "9258\n",
            "9259\n",
            "9260\n",
            "9261\n",
            "9262\n",
            "9263\n",
            "9264\n",
            "9265\n",
            "9266\n",
            "9267\n",
            "9268\n",
            "9269\n",
            "9270\n",
            "9271\n",
            "9272\n",
            "9273\n",
            "9274\n",
            "9275\n",
            "9276\n",
            "9277\n",
            "9278\n",
            "9279\n",
            "9280\n",
            "9281\n",
            "9282\n",
            "9283\n",
            "9284\n",
            "9285\n",
            "9286\n",
            "9287\n",
            "9288\n",
            "9289\n",
            "9290\n",
            "9291\n",
            "9292\n",
            "9293\n",
            "9294\n",
            "9295\n",
            "9296\n",
            "9297\n",
            "9298\n",
            "9299\n",
            "9300\n",
            "9301\n",
            "9302\n",
            "9303\n",
            "9304\n",
            "9305\n",
            "9306\n",
            "9307\n",
            "9308\n",
            "9309\n",
            "9310\n",
            "9311\n",
            "9312\n",
            "9313\n",
            "9314\n",
            "9315\n",
            "9316\n",
            "9317\n",
            "9318\n",
            "9319\n",
            "9320\n",
            "9321\n",
            "9322\n",
            "9323\n",
            "9324\n",
            "9325\n",
            "9326\n",
            "9327\n",
            "9328\n",
            "9329\n",
            "9330\n",
            "9331\n",
            "9332\n",
            "9333\n",
            "9334\n",
            "9335\n",
            "9336\n",
            "9337\n",
            "9338\n",
            "9339\n",
            "9340\n",
            "9341\n",
            "9342\n",
            "9343\n",
            "9344\n",
            "9345\n",
            "9346\n",
            "9347\n",
            "9348\n",
            "9349\n",
            "9350\n",
            "9351\n",
            "9352\n",
            "9353\n",
            "9354\n",
            "9355\n",
            "9356\n",
            "9357\n",
            "9358\n",
            "9359\n",
            "9360\n",
            "9361\n",
            "9362\n",
            "9363\n",
            "9364\n",
            "9365\n",
            "9366\n",
            "9367\n",
            "9368\n",
            "9369\n",
            "9370\n",
            "9371\n",
            "9372\n",
            "9373\n",
            "9374\n",
            "9375\n",
            "9376\n",
            "9377\n",
            "9378\n",
            "9379\n",
            "9380\n",
            "9381\n",
            "9382\n",
            "9383\n",
            "9384\n",
            "9385\n",
            "9386\n",
            "9387\n",
            "9388\n",
            "9389\n",
            "9390\n",
            "9391\n",
            "9392\n",
            "9393\n",
            "9394\n",
            "9395\n",
            "9396\n",
            "9397\n",
            "9398\n",
            "9399\n",
            "9400\n",
            "9401\n",
            "9402\n",
            "9403\n",
            "9404\n",
            "9405\n",
            "9406\n",
            "9407\n",
            "9408\n",
            "9409\n",
            "9410\n",
            "9411\n",
            "9412\n",
            "9413\n",
            "9414\n",
            "9415\n",
            "9416\n",
            "9417\n",
            "9418\n",
            "9419\n",
            "9420\n",
            "9421\n",
            "9422\n",
            "9423\n",
            "9424\n",
            "9425\n",
            "9426\n",
            "9427\n",
            "9428\n",
            "9429\n",
            "9430\n",
            "9431\n",
            "9432\n",
            "9433\n",
            "9434\n",
            "9435\n",
            "9436\n",
            "9437\n",
            "9438\n",
            "9439\n",
            "9440\n",
            "9441\n",
            "9442\n",
            "9443\n",
            "9444\n",
            "9445\n",
            "9446\n",
            "9447\n",
            "9448\n",
            "9449\n",
            "9450\n",
            "9451\n",
            "9452\n",
            "9453\n",
            "9454\n",
            "9455\n",
            "9456\n",
            "9457\n",
            "9458\n",
            "9459\n",
            "9460\n",
            "9461\n",
            "9462\n",
            "9463\n",
            "9464\n",
            "9465\n",
            "9466\n",
            "9467\n",
            "9468\n",
            "9469\n",
            "9470\n",
            "9471\n",
            "9472\n",
            "9473\n",
            "9474\n",
            "9475\n",
            "9476\n",
            "9477\n",
            "9478\n",
            "9479\n",
            "9480\n",
            "9481\n",
            "9482\n",
            "9483\n",
            "9484\n",
            "9485\n",
            "9486\n",
            "9487\n",
            "9488\n",
            "9489\n",
            "9490\n",
            "9491\n",
            "9492\n",
            "9493\n",
            "9494\n",
            "9495\n",
            "9496\n",
            "9497\n",
            "9498\n",
            "9499\n",
            "9500\n",
            "9501\n",
            "9502\n",
            "9503\n",
            "9504\n",
            "9505\n",
            "9506\n",
            "9507\n",
            "9508\n",
            "9509\n",
            "9510\n",
            "9511\n",
            "9512\n",
            "9513\n",
            "9514\n",
            "9515\n",
            "9516\n",
            "9517\n",
            "9518\n",
            "9519\n",
            "9520\n",
            "9521\n",
            "9522\n",
            "9523\n",
            "9524\n",
            "9525\n",
            "9526\n",
            "9527\n",
            "9528\n",
            "9529\n",
            "9530\n",
            "9531\n",
            "9532\n",
            "9533\n",
            "9534\n",
            "9535\n",
            "9536\n",
            "9537\n",
            "9538\n",
            "9539\n",
            "9540\n",
            "9541\n",
            "9542\n",
            "9543\n",
            "9544\n",
            "9545\n",
            "9546\n",
            "9547\n",
            "9548\n",
            "9549\n",
            "9550\n",
            "9551\n",
            "9552\n",
            "9553\n",
            "9554\n",
            "9555\n",
            "9556\n",
            "9557\n",
            "9558\n",
            "9559\n",
            "9560\n",
            "9561\n",
            "9562\n",
            "9563\n",
            "9564\n",
            "9565\n",
            "9566\n",
            "9567\n",
            "9568\n",
            "9569\n",
            "9570\n",
            "9571\n",
            "9572\n",
            "9573\n",
            "9574\n",
            "9575\n",
            "9576\n",
            "9577\n",
            "9578\n",
            "9579\n",
            "9580\n",
            "9581\n",
            "9582\n",
            "9583\n",
            "9584\n",
            "9585\n",
            "9586\n",
            "9587\n",
            "9588\n",
            "9589\n",
            "9590\n",
            "9591\n",
            "9592\n",
            "9593\n",
            "9594\n",
            "9595\n",
            "9596\n",
            "9597\n",
            "9598\n",
            "9599\n",
            "9600\n",
            "9601\n",
            "9602\n",
            "9603\n",
            "9604\n",
            "9605\n",
            "9606\n",
            "9607\n",
            "9608\n",
            "9609\n",
            "9610\n",
            "9611\n",
            "9612\n",
            "9613\n",
            "9614\n",
            "9615\n",
            "9616\n",
            "9617\n",
            "9618\n",
            "9619\n",
            "9620\n",
            "9621\n",
            "9622\n",
            "9623\n",
            "9624\n",
            "9625\n",
            "9626\n",
            "9627\n",
            "9628\n",
            "9629\n",
            "9630\n",
            "9631\n",
            "9632\n",
            "9633\n",
            "9634\n",
            "9635\n",
            "9636\n",
            "9637\n",
            "9638\n",
            "9639\n",
            "9640\n",
            "9641\n",
            "9642\n",
            "9643\n",
            "9644\n",
            "9645\n",
            "9646\n",
            "9647\n",
            "9648\n",
            "9649\n",
            "9650\n",
            "9651\n",
            "9652\n",
            "9653\n",
            "9654\n",
            "9655\n",
            "9656\n",
            "9657\n",
            "9658\n",
            "9659\n",
            "9660\n",
            "9661\n",
            "9662\n",
            "9663\n",
            "9664\n",
            "9665\n",
            "9666\n",
            "9667\n",
            "9668\n",
            "9669\n",
            "9670\n",
            "9671\n",
            "9672\n",
            "9673\n",
            "9674\n",
            "9675\n",
            "9676\n",
            "9677\n",
            "9678\n",
            "9679\n",
            "9680\n",
            "9681\n",
            "9682\n",
            "9683\n",
            "9684\n",
            "9685\n",
            "9686\n",
            "9687\n",
            "9688\n",
            "9689\n",
            "9690\n",
            "9691\n",
            "9692\n",
            "9693\n",
            "9694\n",
            "9695\n",
            "9696\n",
            "9697\n",
            "9698\n",
            "9699\n",
            "9700\n",
            "9701\n",
            "9702\n",
            "9703\n",
            "9704\n",
            "9705\n",
            "9706\n",
            "9707\n",
            "9708\n",
            "9709\n",
            "9710\n",
            "9711\n",
            "9712\n",
            "9713\n",
            "9714\n",
            "9715\n",
            "9716\n",
            "9717\n",
            "9718\n",
            "9719\n",
            "9720\n",
            "9721\n",
            "9722\n",
            "9723\n",
            "9724\n",
            "9725\n",
            "9726\n",
            "9727\n",
            "9728\n",
            "9729\n",
            "9730\n",
            "9731\n",
            "9732\n",
            "9733\n",
            "9734\n",
            "9735\n",
            "9736\n",
            "9737\n",
            "9738\n",
            "9739\n",
            "9740\n",
            "9741\n",
            "9742\n",
            "9743\n",
            "9744\n",
            "9745\n",
            "9746\n",
            "9747\n",
            "9748\n",
            "9749\n",
            "9750\n",
            "9751\n",
            "9752\n",
            "9753\n",
            "9754\n",
            "9755\n",
            "9756\n",
            "9757\n",
            "9758\n",
            "9759\n",
            "9760\n",
            "9761\n",
            "9762\n",
            "9763\n",
            "9764\n",
            "9765\n",
            "9766\n",
            "9767\n",
            "9768\n",
            "9769\n",
            "9770\n",
            "9771\n",
            "9772\n",
            "9773\n",
            "9774\n",
            "9775\n",
            "9776\n",
            "9777\n",
            "9778\n",
            "9779\n",
            "9780\n",
            "9781\n",
            "9782\n",
            "9783\n",
            "9784\n",
            "9785\n",
            "9786\n",
            "9787\n",
            "9788\n",
            "9789\n",
            "9790\n",
            "9791\n",
            "9792\n",
            "9793\n",
            "9794\n",
            "9795\n",
            "9796\n",
            "9797\n",
            "9798\n",
            "9799\n",
            "9800\n",
            "9801\n",
            "9802\n",
            "9803\n",
            "9804\n",
            "9805\n",
            "9806\n",
            "9807\n",
            "9808\n",
            "9809\n",
            "9810\n",
            "9811\n",
            "9812\n",
            "9813\n",
            "9814\n",
            "9815\n",
            "9816\n",
            "9817\n",
            "9818\n",
            "9819\n",
            "9820\n",
            "9821\n",
            "9822\n",
            "9823\n",
            "9824\n",
            "9825\n",
            "9826\n",
            "9827\n",
            "9828\n",
            "9829\n",
            "9830\n",
            "9831\n",
            "9832\n",
            "9833\n",
            "9834\n",
            "9835\n",
            "9836\n",
            "9837\n",
            "9838\n",
            "9839\n",
            "9840\n",
            "9841\n",
            "9842\n",
            "9843\n",
            "9844\n",
            "9845\n",
            "9846\n",
            "9847\n",
            "9848\n",
            "9849\n",
            "9850\n",
            "9851\n",
            "9852\n",
            "9853\n",
            "9854\n",
            "9855\n",
            "9856\n",
            "9857\n",
            "9858\n",
            "9859\n",
            "9860\n",
            "9861\n",
            "9862\n",
            "9863\n",
            "9864\n",
            "9865\n",
            "9866\n",
            "9867\n",
            "9868\n",
            "9869\n",
            "9870\n",
            "9871\n",
            "9872\n",
            "9873\n",
            "9874\n",
            "9875\n",
            "9876\n",
            "9877\n",
            "9878\n",
            "9879\n",
            "9880\n",
            "9881\n",
            "9882\n",
            "9883\n",
            "9884\n",
            "9885\n",
            "9886\n",
            "9887\n",
            "9888\n",
            "9889\n",
            "9890\n",
            "9891\n",
            "9892\n",
            "9893\n",
            "9894\n",
            "9895\n",
            "9896\n",
            "9897\n",
            "9898\n",
            "9899\n",
            "9900\n",
            "9901\n",
            "9902\n",
            "9903\n",
            "9904\n",
            "9905\n",
            "9906\n",
            "9907\n",
            "9908\n",
            "9909\n",
            "9910\n",
            "9911\n",
            "9912\n",
            "9913\n",
            "9914\n",
            "9915\n",
            "9916\n",
            "9917\n",
            "9918\n",
            "9919\n",
            "9920\n",
            "9921\n",
            "9922\n",
            "9923\n",
            "9924\n",
            "9925\n",
            "9926\n",
            "9927\n",
            "9928\n",
            "9929\n",
            "9930\n",
            "9931\n",
            "9932\n",
            "9933\n",
            "9934\n",
            "9935\n",
            "9936\n",
            "9937\n",
            "9938\n",
            "9939\n",
            "9940\n",
            "9941\n",
            "9942\n",
            "9943\n",
            "9944\n",
            "9945\n",
            "9946\n",
            "9947\n",
            "9948\n",
            "9949\n",
            "9950\n",
            "9951\n",
            "9952\n",
            "9953\n",
            "9954\n",
            "9955\n",
            "9956\n",
            "9957\n",
            "9958\n",
            "9959\n",
            "9960\n",
            "9961\n",
            "9962\n",
            "9963\n",
            "9964\n",
            "9965\n",
            "9966\n",
            "9967\n",
            "9968\n",
            "9969\n",
            "9970\n",
            "9971\n",
            "9972\n",
            "9973\n",
            "9974\n",
            "9975\n",
            "9976\n",
            "9977\n",
            "9978\n",
            "9979\n",
            "9980\n",
            "9981\n",
            "9982\n",
            "9983\n",
            "9984\n",
            "9985\n",
            "9986\n",
            "9987\n",
            "9988\n",
            "9989\n",
            "9990\n",
            "9991\n",
            "9992\n",
            "9993\n",
            "9994\n",
            "9995\n",
            "9996\n",
            "9997\n",
            "9998\n",
            "9999\n",
            "X_raw is tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [-1.0000, -1.0000],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-1.5000, -0.5000],\n",
            "        [ 1.0000, -1.0000],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "X_GD is tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -0.9999],\n",
            "        [-1.8611, -1.4674],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.4999, -4.0000],\n",
            "        [-1.0499, -0.9031],\n",
            "        [ 0.4280, -1.9126],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([ 1.8595e-01+0.j,  5.4428e-02+0.j,  4.6905e-03+0.j,  1.9682e-03+0.j,\n",
            "         5.3192e-04+0.j,  1.1670e-04+0.j,  2.9323e-05+0.j,  3.5878e-07+0.j,\n",
            "        -6.8535e-06+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tensor(X_raw, label='X_raw', marker='o', color='blue')\n",
        "plot_tensor(X_GD, label='X_GD', marker='o', color='blue')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "yCHAe32ZenAY",
        "outputId": "d3124ab8-1610-4c3c-d44c-727e93f54964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGJCAYAAADWn3rYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzf0lEQVR4nO3de1xU5b7H8e84IkKKNzBR8IKalpV2NM0rUGmmu/JwzMoy2ZWZecnUUqvtJWtr5S4r3WblbWekpZhl6ZGTtzxatksr25pHwxugpiYoGIwz6/wxmwlkBgZcwzDweb9evGCeeWbNj98M8GWtZ9ZYDMMwBAAAYIJq/i4AAABUHgQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAldq0adNksVj8XQZQZRAsgCrkgQceUM2aNbV///4i182aNUsWi0Vr164t9Xa//PJLDRo0SE2aNFGNGjVUp04ddenSRc8//7xOnDhRaG5cXJwsFossFouqVaumsLAwtWnTRkOGDFFKSorX95mYmOjajsViUVhYmNq3b6+//e1vys3NLfX34M7f//53LVmyxJRtAVWFhfcKAaqOkydPqm3bturQoYM2btzoGk9NTVW7du3Ur18/rVy5slTbnDJlimbMmKGYmBjde++9iomJ0e+//65vv/1Wq1atUnh4uA4ePOiaHxcXp4MHD2rmzJmSpOzsbB04cEDJycn65ZdfNGjQIC1btkxBQUHF3m9iYqKWL1+ud999V5J09uxZrVq1Sps3b9Y999yj5cuXS3LusZg+fbrK8qvu2muvVXh4uDZv3lzq2wJVlgGgSnn77bcNScaSJUtcY3379jXCwsKMY8eOlWpby5cvNyQZgwYNMnJzc4tcf/bsWWPq1KmFxmJjY4127doVmXvx4kXj8ccfNyQZTz/9dIn3PXToUOOKK64oNGa3241OnToZkoy0tDTDMAxj6tSpRll/1bVr186IjY0t022BqopDIUAV88gjj6h79+6aMGGCTp8+reXLl2v9+vV64YUX1KRJk1Jta8qUKQoPD9fChQtVo0aNItfXqVNH06ZN82pbVqtVb7zxhq655hrNnTtXmZmZpapFkqpVq6a4uDhJ0qFDhzzOu3jxombMmKGWLVsqODhYzZs31zPPPFPoEErz5s31008/acuWLa7DLfnbBuAZwQKoYiwWixYsWKDMzEyNGDFCTz75pDp16qSRI0eWajv79+/X/v37NWDAANWqVcuU2qxWq+677z7l5ORo27ZtZdpG/mGXBg0aeJzzyCOPaMqUKfqP//gPvfbaa4qNjdXMmTN17733uubMmTNHUVFRatu2rd577z299957evbZZ8tUE1CVVPd3AQDKX7t27TRhwgTNnDlTVqtVn332mapVK93/Gfv27ZPkXIdQkGEYOn36dKGxunXrqnp1737d5G+v4LqM4pw6dUqSlJmZqQ8//FAff/yxrr/+erVp08bt/O+//15Lly7VI488onfeeUeS9Pjjj6thw4aaPXu2Nm3apPj4eA0YMEDPPfecwsPD9cADD3hVCwD2WABVVnh4uCSpcePGRcKBN7KysiSpyN6KzMxMRUREFPrYvXu319vN3965c+dKnJudne26j1atWumZZ55R165dtXr1ao+3+fzzzyVJ48aNKzQ+fvx4SdJnn33mda0AimKPBVAFHT16VFOnTtW1116rPXv26OWXX9Zzzz1Xqm3Url1bknT+/PlC47Vq1XK9bHTDhg165ZVXSrXd/O3lb784NWvW1KeffipJCg4OVosWLRQVFVXsbQ4fPqxq1aqpVatWhcYbNWqkunXr6vDhw6WqF0BhBAugCho1apQkad26dRo3bpxefPFFDR48WDExMV5vo23btpKkPXv2FBqvXr26br31VknSsWPHSl1b/vYu/cPvjtVqdd1XaXHSLMA3OBQCVDGrV6/WJ598ohkzZigqKkpz5sxRjRo1Sr14s02bNmrdurU+/vhjZWdnm1Kb3W5XUlKSQkND1aNHD1O2ealmzZrJ4XDo//7v/wqNnzhxQmfPnlWzZs1cY4QPoPQIFkAVcu7cOY0ZM0Y33HCDRo8eLcm5xmLGjBlav369Pvroo1Jtb9q0aTp16pSGDRsmm81W5HqjFCelstvtGjNmjPbu3asxY8YoLCysVLV4q1+/fpKcr/oo6NVXX5Uk9e/f3zV2xRVX6OzZsz6pA6isOBQCVCHPPfec0tPTlZycLKvV6hofOXKkli5dqrFjx6pv375erW+QpMGDB2vPnj2aOXOmdu7cqXvvvVctWrRQdna29uzZow8++EC1a9dWvXr1Ct0uMzNTy5YtkyTl5OS4zrx58OBB3XvvvZoxY4Z53/Ql2rdvr6FDh+rtt9/W2bNnFRsbq507d2rp0qUaMGCA4uPjXXM7duyo+fPn64UXXlCrVq3UsGFD3XzzzT6rDagU/H2GLgDl45///KdhtVqNUaNGub1+586dRrVq1YwxY8aUetubN282Bg4caERGRhpBQUFGWFiY0alTJ2Pq1KlGRkZGobmxsbGGJNdHrVq1jNatWxsPPPCAsWHDBq/v092ZN91xd+ZNm81mTJ8+3WjRooURFBRkREdHG5MnTzZ+//33QvOOHz9u9O/f36hdu7YhibNwAl7gvUIAAIBpWGMBAABMwxoLAEWcOXNGeXl5Hq+3Wq2KiIgox4oABAoOhQAoIi4uTlu2bPF4fbNmzYp9ky8AVRfBAkAR3377rX777TeP14eEhKh79+7lWBGAQEGwAAAApmHxJgAAME2VWrzpcDiUnp6u2rVrc6peAABKwTAMnTt3To0bN1a1ap73S1SpYJGenq7o6Gh/lwEAQMA6evRose8iXKWCRf5pio8ePWra+xDYbDZt2LBBffr0UVBQkCnbrAzoi2f0xj364hm9cY++eOaL3mRlZSk6OrrEU/5XqWCRf/gjLCzM1GARGhqqsLAwntgF0BfP6I179MUzeuMeffHMl70paSkBizcBAIBpCBYAAMA0BAsAAGCaKrXGwhuGYejixYuy2+1ezbfZbKpevbp+//13r29TFeT3JTc3V5JUvXp1XuILAFUAwaKAvLw8ZWRkKCcnx+vbGIahRo0a6ejRo/zhLCC/L0eOHJHFYlFoaKgiIyNVo0YNf5cGAPAhgsW/ORwOpaamymq1qnHjxqpRo4ZXQcHhcOj8+fOqVatWsScMqWry+3LFFVfo4sWL+vXXX5WamqrWrVvTJwCoxAgW/5aXlyeHw6Ho6GiFhoZ6fTuHw6G8vDzVrFmTP5gF5PclJCRE1apVU1BQkA4fPuzqFQBUFna79OWXUkaGFBkp9ewpWa3+rsp/CBaXIBz4Bn0FUBklJ0tPPCEdO/bHWFSU9PrrUkKC/+ryJ37bAwBQBsnJ0sCBhUOFJKWlOceTk/1Tl78RLAAAKCW73bmnwjCKXpc/Nnasc15VQ7AAAKCUvvyy6J6KggxDOnrUOa+qIVgEOLvdrm7duinhkoN5mZmZio6O1rPPPuvVdlatWqWbb75Z9erVU0hIiNq0aaOHHnpIu3btcs1ZsmSJLBaLLBaLrFar6tWrpy5duuj5559XZmamqd8XAFRkGRnmzqtMCBYms9ulzZulDz5wfvb1bjCr1aolS5Zo/fr1ev/9913jo0ePVv369TV16tQStzFx4kTdc8896tChgz755BP9/PPPSkpKUkxMjCZPnlxoblhYmDIyMnTs2DFt375djz76qP7xj3+oQ4cOSk9PN/37A4CKKDLS3HmVCa8KMZG/VgdfddVVmjVrlkaPHq2bb75ZO3fu1PLly/XNN9+UeEKqr776Si+//LJef/11jRkzxjXetGlTdezYUcYlBxAtFosaNWokSYqMjNTVV1+tO+64Q+3atdPTTz+tZcuWmf8NAkAF07On8/d7Wpr7dRYWi/P6nj3LvzZ/Y4+FSfy9Onj06NFq3769hgwZokcffVRTpkxR+/btS7zdBx98oFq1aunxxx93e703Jwlr2LCh7r//fn3yySec1hxAlWC1Ov9plJwhoqD8y3PmVM3zWRAsTGC3S08+afHr6mCLxaL58+friy++0JVXXqlJkyZ5dbv9+/crJiZG1av/sfPq1VdfVa1atVwf3qyfaNu2rc6dO6fTp0+X+XsAgECSkCCtXCk1aVJ4PCrKOc55LFBmO3ZU17Fjnv+zL6/VwYsWLVJoaKhSU1N1rLjlyiV46KGHtHv3bi1YsEDZ2dlFDoe4kz+H90sBUJUkJEiHDkmbNklJSc7PqalVN1RIBAtTHD/u3R9TX64O3r59u1577TWtXbtWnTt31sMPP+xVIGjdurV++eUX2Ww211jdunXVqlUrNbk0hhdj7969CgsLU4MGDcpUPwAEKqtViouT7rvP+bkqHv4oiGBhgkaNSv4DLvludXBOTo4SExM1YsQIxcfHa+HChdq5c6feeuutEm9733336fz58/r73/9e5vs/efKkkpKSNGDAAE7dDQBVHK8KMUHXrhcVFWUoLc39Ogtfrw6ePHmyDMPQrFmzJEnNmzfX7NmzNWHCBN1+++1q3rx5MbV31fjx4zV+/HgdPnxYCQkJio6OVkZGhhYuXCiLxVIoLBiGoePHj8swDJ09e1Y7duzQX//6V9WpU8d1/wCAqot/L01gtUqvvZa/xqDwdb5eHbxlyxbNmzdPixcvLvSurMOHD1e3bt28OiQye/ZsJSUladeuXfrTn/6k1q1b6+6775bD4dCOHTsUFhbmmpuVlaXIyEg1adJEXbt21YIFCzR06FDt2rVLkVXxBdsAgELYY2GS/NXB7s5jMWeO7xbyxMbG6uLFi26v++///m+vtzNo0CANGjSo2DmJiYlKTEwsTXkAgCqGYGGihATprrucr/7IyHCuqejZk4U8AICqg0MhJqtoq4Mfe+yxQuekKPjx2GOP+bc4AEClwx6LSu7555/XhAkT3F5XcO0EAABmIFhUcg0bNlTDhg39XQYAoIrgUMglvDmpFEqPvgJA1UCw+LegoCBJzpNNwXz5fc3vM1CQ3S5t2+b8ets2376vDlDZ+fvnKWAOhcycOVPJycnat2+fQkJC1K1bN7300ktq06aNKdu3Wq2qW7euTp48KUkKDQ316n0vHA6H8vLy9Pvvv3PWyQLy+3LhwgX9/vvvOnnypOrWrSurv1ezosJJTna+TPv0aemDD6T+/aUGDZzvHFmV328BKIuK8PMUMMFiy5YtGjlypG688UZdvHhRzzzzjPr06aN//etfuuKKK0y5j0aNGkmSK1x4wzAMXbhwQSEhIbwBVwGX9qVu3bqu/gL5kpOlgQOdb9QXEvLHeFqac7wqv0MkUFoV5ecpYILF+vXrC11esmSJGjZsqG+//Va9evUy5T4sFosiIyPVsGHDQm/KVRybzaatW7eqV69e7OYvIL8vsbGxCgkJYU8FirDbnf9ZuVt+YxjOs9aOHes8NwxPH6B4FennKWCCxaUyMzMlSfXr1/c4Jzc3V7m5ua7LWVlZkpx/9EoKDt7+IXQ4HLp48aKsVit/PAvI70u1atXkcDjkcDj8XVKFkf/c8za8Vlbbtjl31+b/ZxUSYiv0WZJOnZK2bpV69PBHhRUHzxn36MsfyuPnyds+W4wAXK7vcDh055136uzZs9qWv0LFjWnTpmn69OlFxpOSkgq9rwYAACheTk6OBg8erMzMzGLPgxSQwWLEiBFat26dtm3bpqioKI/z3O2xiI6O1qlTp0w7OZTNZlNKSop69+7NoZAC6Itn9MZp2zbnwrJ8ISE2LVqUooce6q0LF/7oy2efsceC54x79OUP5fHzlJWVpfDw8BKDRcAdChk1apTWrl2rrVu3FhsqJCk4OFjBwcFFxoOCgkx/Evpim5UBffGsqvemVy/navW0tMLHhS9cCNKFC0GyWJxv4terF2ss8lX154wn9KV8fp687XHAvD7SMAyNGjVKq1ev1saNG9WiRQt/lwTgMlitzpfASc6FZQXlX54zh1ABeKMi/TwFTLAYOXKkli1bpqSkJNWuXVvHjx/X8ePHdeHCBX+XBqCMEhKcL4Fr0qTweFQULzUFSqui/DwFTLCYP3++MjMzFRcXp8jISNfHihUr/F0agMuQkCAdOuQ89is5P6emEiqAsqgIP08Bs8YiANeYAvCS1epcUPb5587PHP4Ays7fP08Bs8cCAABUfAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADANwQIAAJiGYAEAAExDsAAAAKYhWAAAANMQLAAAgGkIFgAAwDQECwAAYBqCBQAAMA3BAgAAmIZgAQAATEOwAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpAipYbN26VXfccYcaN24si8Wijz/+2N8lAQCAAgIqWGRnZ6t9+/aaN2+ev0sBAABuVPd3AaVx++236/bbb/d3GQAAwIOAChallZubq9zcXNflrKwsSZLNZpPNZjPlPvK3Y9b2Kgv64hm9cY++eEZv3KMvnvmiN95uy2IYhmHavZYji8Wi1atXa8CAAR7nTJs2TdOnTy8ynpSUpNDQUB9WBwBA5ZKTk6PBgwcrMzNTYWFhHudV6mDhbo9FdHS0Tp06VWxTSsNmsyklJUW9e/dWUFCQKdusDOiLZ/TGPfriGb1xj7545oveZGVlKTw8vMRgUakPhQQHBys4OLjIeFBQkOlPQl9sszKgL57RG/foi2f0xj364pmZvfF2OwH1qhAAAFCxBdQei/Pnz+vAgQOuy6mpqdq9e7fq16+vpk2b+rEyAAAgBViw+Oc//6n4+HjX5XHjxkmShg4dqiVLlvipKgAAkC+ggkVcXJwCdK0pAABVAmssAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADANwQIAAJiGYAEAAExDsAAAAKYhWAAAANMQLAAAgGkIFgAAwDQECwAAYBqCBQAAMA3BAgAAmIZgAQAATEOwAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpAi5YzJs3T82bN1fNmjXVpUsX7dy5098lAQCAfwuoYLFixQqNGzdOU6dO1Xfffaf27dvrtttu08mTJ/1dGgAAkFTd3wWUxquvvqphw4bpz3/+syTprbfe0meffaZFixZp0qRJRebn5uYqNzfXdTkrK0uSZLPZZLPZTKkpfztmba+yoC+e0Rv36Itn9MY9+uKZL3rj7bYshmEYpt2rD+Xl5Sk0NFQrV67UgAEDXONDhw7V2bNntWbNmiK3mTZtmqZPn15kPCkpSaGhob4sFwCASiUnJ0eDBw9WZmamwsLCPM4LmD0Wp06dkt1u15VXXllo/Morr9S+ffvc3mby5MkaN26c63JWVpaio6PVp0+fYptSGjabTSkpKerdu7eCgoJM2WZlQF88ozfu0RfP6I179MUzX/Qmf69/SQImWJRFcHCwgoODi4wHBQWZ/iT0xTYrA/riGb1xj754Rm/coy+emdkbb7cTMIs3w8PDZbVadeLEiULjJ06cUKNGjfxUFQAAKChggkWNGjXUsWNHffHFF64xh8OhL774Ql27dvVjZQAAIF9AHQoZN26chg4dqk6dOqlz586aM2eOsrOzXa8SAQAA/hVQweKee+7Rr7/+qilTpuj48ePq0KGD1q9fX2RBJwAA8I+AChaSNGrUKI0aNcrfZQAAADcCZo0FAACo+AgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMU+pgsX79em3bts11ed68eerQoYMGDx6s3377zdTiAABAYCl1sHjqqadc73D2448/avz48erXr59SU1MLvZMoAACoekp9gqzU1FRdc801kqRVq1bpT3/6k/7617/qu+++U79+/UwvEAAABI5S77GoUaOGcnJyJEn/8z//oz59+kiS6tev7/V7tQMAgMqp1HssevTooXHjxql79+7auXOnVqxYIUnav3+/oqKiTC8QAAAEjlLvsZg7d66qV6+ulStXav78+WrSpIkkad26derbt6/pBQIAgMBR6j0WTZs21dq1a4uMv/baa6YUBAAAApdXwSIrK0thYWGur4uTPw8AAFQ9XgWLevXqKSMjQw0bNlTdunVlsViKzDEMQxaLRXa73fQiAQBAYPAqWGzcuFH169d3fe0uWAAAAHgVLGJjY11fx8XF+aoWAAAQ4Er9qpBp06bJ4XAUGc/MzNR9991nSlEAACAwlTpYLFy4UD169NAvv/ziGtu8ebOuu+46HTx40NTiAABAYCl1sPjhhx8UFRWlDh066J133tFTTz2lPn36aMiQIdq+fbsvagQAAAGi1OexqFevnj788EM988wzGj58uKpXr65169bplltu8UV9AAAggJR6j4Ukvfnmm3r99dd13333KSYmRmPGjNH3339vdm0AACDAlDpY9O3bV9OnT9fSpUv1/vvva9euXerVq5duuukmvfzyy76oEQAABIhSBwu73a4ffvhBAwcOlCSFhIRo/vz5WrlyJaf1BgCgiiv1GouUlBS34/3799ePP/542QUBAIDAVaY1Fpfav3+/Jk6cqOuuu86MzQEAgABV6j0W+XJycrRixQotWrRIO3bsUKdOnTRu3DgzawNQidnt0pdfShkZUmSkdNNN/q4IgBlKHSy++uorvfvuu/roo4/UtGlT7d27V5s2bVLPnj19UR+ASig5WXriCenYsT/GWrWSZs/2X00AzOH1oZC//e1vateunQYOHKh69epp69at+vHHH2WxWNSgQQNf1gigEklOlgYOLBwqJCk93fn500/LvyYA5vE6WEycOFEDBgzQ4cOH9corr6h9+/a+rAtAJWS3O/dUGEbR6/LHJk1yzgMQmLwOFjNmzNBHH32kFi1aaOLEidqzZ48v6woIdru0bZvz623b+GUIlOTLL4vuqbjUsWPOeQACk9fBYvLkydq/f7/ee+89HT9+XF26dFH79u1lGIZ+++03X9ZYISUnS82bS/37Oy/37++8nJzsz6qAii0jw9x5ACqeUr/cNDY2VkuXLtXx48f1+OOPq2PHjoqNjVW3bt306quv+qJGSdKLL76obt26KTQ0VHXr1vXZ/XjD0zHitDTnOOECcC8y0tx5ACqeMp/Honbt2ho+fLi+/vpr7dq1S507d9asWbPMrK2QvLw83X333RoxYoTP7sMb3hwjHjuWwyKAOz17SlFRksXieU5UlHMegMBU5vNYFHTddddpzpw5euWVV8zYnFvTp0+XJC1ZssTr2+Tm5io3N9d1OSsrS5Jks9lks9nKVMe2bdLp01JIiPNySIit0GdJOnVK2rpV6tGjTHdRKeT3t6x9rsyqem9ef10aMsT5dcGAHhrq7MesWTY5HJLD4YfiKqiq/pzxhL545oveeLsti2G4+9+74lqyZInGjh2rs2fPljh32rRprkBSUFJSkkJDQ31QHQAAlVNOTo4GDx6szMxMhYWFeZxnyh6Limry5MmFzgaalZWl6Oho9enTp9imFGfbtj8WbErOPRWLFqXooYd668KFINf4Z5+xxyIlJUW9e/dWUFBQyTeoQuiNk90u7dghHT8uNWok3XijTV98QV/c4TnjHn3xzBe9yd/rXxKvg0V6eroaN25c5oLcmTRpkl566aVi5+zdu1dt27Yt0/aDg4MVHBxcZDwoKKjMje7VS2rQwLlQs+C+ngsXgnThQpAsFucx4l69JKu1THdRqVxOryu7qt6boCApPv6Py/l7Wat6X4pDb9yjL56Z2Rtvt+N1sGjXrp3mzZunwYMHl7moS40fP16JiYnFzomJiTHt/sxgtTqPEQ8cWHQBWv7lOXMIFQCAqsnrYPHiiy9q+PDhWr16tRYsWKD69etf9p1HREQoIiLisrdT3hISpJUrna8OOX36j/GoKGeoSEjwW2kAAPiV1y83ffzxx/XDDz/o9OnTuuaaa/RpOZ/Q/8iRI9q9e7eOHDkiu92u3bt3a/fu3Tp//ny51pEvIUE6dMi5lkJyfk5NJVQAAKq2Ui3ebNGihTZu3Ki5c+cqISFBV199tapXL7yJ7777ztQC802ZMkVLly51Xb7hhhskSZs2bVJcXJxP7rMkVqtzgebnnzs/c/gDAFDVlfpVIYcPH1ZycrLq1aunu+66q0iw8JUlS5aU6hwWAACg/JUqFbzzzjsaP368br31Vv30008BuT4CAAD4jtfBom/fvtq5c6fmzp2rBx980Jc1AQCAAOV1sLDb7frhhx8UFRXly3oAAEAA8zpYpKSk+LIOAABQCZT53U0BAAAuRbAAAACmIVgAAADTECwAAIBpKvXbpgOBym6XvvxSysiQIiOlnj05syuAwECwACqY5GTnG9wdO/bHWFSU8111eS8aABUdh0KACiQ5WRo4sHCokKS0NOd4crJ/6gIAbxEsgArCbnfuqTCMotflj40d65wHABUVwQKoIL78suieioIMQzp61DkPACoqggVQQWRkmDsPAPyBYAFUEJGR5s4DAH8gWAAVRM+ezld/WCzur7dYpOho5zwAqKgIFkAFYbU6X1IqFQ0X+ZfnzOF8FgAqNoIFUIEkJEgrV0pNmhQej4pyjnMeCwAVHSfIAiqYhATprrs48yaAwESwACogq1WKi/N3FQBQehwKAQAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhvNYAACKsNs5SRvKhmABACgkOVl64gnp2LE/xqKinO9lw2nlURIOhQAAXJKTpYEDC4cKSUpLc44nJ/unLgQOggUAQJLz8McTT0iGUfS6/LGxY53zAE8IFgAASc41FZfuqSjIMKSjR53zAE8CIlgcOnRIDz/8sFq0aKGQkBC1bNlSU6dOVV5enr9LA4BKIyPD3HmomgJi8ea+ffvkcDi0YMECtWrVSnv27NGwYcOUnZ2t2bNn+7s8AKgUIiPNnYeqKSCCRd++fdW3b1/X5ZiYGP3888+aP38+wQIATNKzp/PVH2lp7tdZWCzO63v2LP/aEDgCIli4k5mZqfr16xc7Jzc3V7m5ua7LWVlZkiSbzSabzWZKHfnbMWt7lQV98YzeuEdfPCvP3rz+ujRkiPPrguHCYnF+njNHcjicH/7Gc8YzX/TG221ZDMNdLq3YDhw4oI4dO2r27NkaNmyYx3nTpk3T9OnTi4wnJSUpNDTUlyUCAFCp5OTkaPDgwcrMzFRYWJjHeX4NFpMmTdJLL71U7Jy9e/eqbdu2rstpaWmKjY1VXFyc3n333WJv626PRXR0tE6dOlVsU0rDZrMpJSVFvXv3VlBQkCnbrAzoi2f0xj364pk/emO3Szt2SMePS40aSV27Vrwzb/Kc8cwXvcnKylJ4eHiJwcKvh0LGjx+vxMTEYufExMS4vk5PT1d8fLy6deumt99+u8TtBwcHKzg4uMh4UFCQ6U9CX2yzMqAvntEb9+iLZ+XZm6AgKT6+XO7qsvGc8czM3ni7Hb8Gi4iICEVERHg1Ny0tTfHx8erYsaMWL16satUC4pWyAABUKQGxeDMtLU1xcXFq1qyZZs+erV9//dV1XaNGjfxYGQAAKCgggkVKSooOHDigAwcOKCoqqtB1Abj2FACASisgjickJibKMAy3HwAAoOIIiGABAAACA8ECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADANwQIAAJiGYAEAAExDsAAAAKYhWAAAANMQLAAAgGkIFgAAwDQECwAAYBqCBQAAMA3BAgAAmIZgAQAATEOwAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGCagAkWd955p5o2baqaNWsqMjJSQ4YMUXp6ur/LAgAABQRMsIiPj9eHH36on3/+WatWrdLBgwc1cOBAf5cFAAAKqO7vArz15JNPur5u1qyZJk2apAEDBshmsykoKMiPlQEAgHwBEywKOnPmjN5//31169at2FCRm5ur3Nxc1+WsrCxJks1mk81mM6WW/O2Ytb3Kgr54Rm/coy+e0Rv36ItnvuiNt9uyGIZhmHavPjZx4kTNnTtXOTk5uummm7R27Vo1aNDA4/xp06Zp+vTpRcaTkpIUGhrqy1IBAKhUcnJyNHjwYGVmZiosLMzjPL8Gi0mTJumll14qds7evXvVtm1bSdKpU6d05swZHT58WNOnT1edOnW0du1aWSwWt7d1t8ciOjpap06dKrYppWGz2ZSSkqLevXtzSKYA+uIZvXGPvnhGb9yjL575ojdZWVkKDw8vMVj49VDI+PHjlZiYWOycmJgY19fh4eEKDw/XVVddpauvvlrR0dH66quv1LVrV7e3DQ4OVnBwcJHxoKAg05+EvthmZUBfPKM37tEXz+iNe/TFMzN74+12/BosIiIiFBERUabbOhwOSSq0RwIAAPhXQCze/Prrr/XNN9+oR48eqlevng4ePKi//OUvatmypce9FQAAoPwFxHksQkNDlZycrFtuuUVt2rTRww8/rOuvv15btmxxe6gDAAD4R0Dssbjuuuu0ceNGf5cBAABKEBB7LAAAQGAgWAAAANMQLAAAgGkIFgAAwDQECwAAYBqCBQAAMA3BAgAAmIZgAQAATEOwAAAApiFYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0BAsAAGAaggUAADANwQIAAJiGYAEAAExDsAAAAKYhWAAAANMQLAAAgGkIFgAAwDQECwAAYBqCBQAAMA3BAgAAmIZgcRnsdmnbNufX27Y5L4O+BAq7Xdq8WfrgA+dnHicAZgi4YJGbm6sOHTrIYrFo9+7dfqsjOVlq3lzq3995uX9/5+XkZL+VVCHQl8CQ/zjFx0uDBzs/8zgBMEPABYunn35ajRs39msNycnSwIHSsWOFx9PSnONV9ZczfQkMPE4AfCmggsW6deu0YcMGzZ4922812O3SE09IhlH0uvyxsWOr3m5l+hIYeJwA+Fp1fxfgrRMnTmjYsGH6+OOPFRoa6tVtcnNzlZub67qclZUlSbLZbLLZbGWqY9s26fRpKSTEeTkkxFbosySdOiVt3Sr16FGmuwhI9MV7+c+9sj4HL8elj5M7/nqc/NmXio7euEdfPPNFb7zdlsUw3P3vUrEYhqF+/fqpe/fueu6553To0CG1aNFCu3btUocOHTzebtq0aZo+fXqR8aSkJK/DCQAAkHJycjR48GBlZmYqLCzM4zy/BotJkybppZdeKnbO3r17tWHDBn344YfasmWLrFar18HC3R6L6OhonTp1qtimFGfbtj8WJkrO/8gXLUrRQw/11oULQa7xzz6rWv+Z0xfv2Ww2paSkqHfv3goKCir5Bia69HHyxB+Pkz/7UtHRG/foi2e+6E1WVpbCw8NLDBZ+PRQyfvx4JSYmFjsnJiZGGzdu1I4dOxQcHFzouk6dOun+++/X0qVL3d42ODi4yG0kKSgoqMyN7tVLatDAudCtYCS7cCFIFy4EyWKRoqKc86zWMt1FQKIvpXc5z8Oy8vQ45asIj5M/+hIo6I179MUzM3vj7Xb8GiwiIiIUERFR4rw33nhDL7zwgutyenq6brvtNq1YsUJdunTxZYlFWK3S6687V89bLIWvy788Z07V++NJXwLDpY9TwXDB4wTADAHxqpCmTZvq2muvdX1cddVVkqSWLVsqKiqq3OtJSJBWrpSaNCk8HhXlHE9IKPeSKgT6Ehh4nAD4UsC8KqSiSUiQ7rrLuXo+K8t5TJrd/PQlUOQ/Tl9+KWVkSJGRUs+ePE4ALl9ABovmzZurIryYxWp1LnD7/HPnZ34pO9GXwGC1SnFx/q4CQGUTEIdCAABAYCBYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwTUC+3LSs8l+imv8up2aw2WzKyclRVlYWp5QtgL54Rm/coy+e0Rv36ItnvuhN/t/Okk73UKWCxblz5yRJ0dHRfq4EAIDAdO7cOdWpU8fj9QHxtulmcTgcSk9PV+3atWW59A0tyij/HVOPHj1a5ndMrYzoi2f0xj364hm9cY++eOaL3hiGoXPnzqlx48aqVs3zSooqtceiWrVqPntvkbCwMJ7YbtAXz+iNe/TFM3rjHn3xzOzeFLenIh+LNwEAgGkIFgAAwDQEi8sUHBysqVOnKjg42N+lVCj0xTN64x598YzeuEdfPPNnb6rU4k0AAOBb7LEAAACmIVgAAADTECwAAIBpCBYAAMA0BAsfyM3NVYcOHWSxWLR7925/l1Mh3HnnnWratKlq1qypyMhIDRkyROnp6f4uy68OHTqkhx9+WC1atFBISIhatmypqVOnKi8vz9+l+d2LL76obt26KTQ0VHXr1vV3OX41b948NW/eXDVr1lSXLl20c+dOf5fkd1u3btUdd9yhxo0by2Kx6OOPP/Z3SRXCzJkzdeONN6p27dpq2LChBgwYoJ9//rnc6yBY+MDTTz+txo0b+7uMCiU+Pl4ffvihfv75Z61atUoHDx7UwIED/V2WX+3bt08Oh0MLFizQTz/9pNdee01vvfWWnnnmGX+X5nd5eXm6++67NWLECH+X4lcrVqzQuHHjNHXqVH333Xdq3769brvtNp08edLfpflVdna22rdvr3nz5vm7lAply5YtGjlypL766iulpKTIZrOpT58+ys7OLt9CDJjq888/N9q2bWv89NNPhiRj165d/i6pQlqzZo1hsViMvLw8f5dSobz88stGixYt/F1GhbF48WKjTp06/i7Dbzp37myMHDnSddlutxuNGzc2Zs6c6ceqKhZJxurVq/1dRoV08uRJQ5KxZcuWcr1f9liY6MSJExo2bJjee+89hYaG+rucCuvMmTN6//331a1bN97q+BKZmZmqX7++v8tABZCXl6dvv/1Wt956q2usWrVquvXWW7Vjxw4/VoZAkZmZKUnl/juFYGESwzCUmJioxx57TJ06dfJ3ORXSxIkTdcUVV6hBgwY6cuSI1qxZ4++SKpQDBw7ozTff1PDhw/1dCiqAU6dOyW6368orryw0fuWVV+r48eN+qgqBwuFwaOzYserevbuuvfbacr1vgkUJJk2aJIvFUuzHvn379Oabb+rcuXOaPHmyv0suN972Jt9TTz2lXbt2acOGDbJarXrwwQdlVMITv5a2L5KUlpamvn376u6779awYcP8VLlvlaUvAMpm5MiR2rNnj5YvX17u980pvUvw66+/6vTp08XOiYmJ0aBBg/Tpp5/KYrG4xu12u6xWq+6//34tXbrU16WWO297U6NGjSLjx44dU3R0tLZv366uXbv6qkS/KG1f0tPTFRcXp5tuuklLlixRtWqVM++X5fmyZMkSjR07VmfPnvVxdRVPXl6eQkNDtXLlSg0YMMA1PnToUJ09e5Y9fv9msVi0evXqQj2q6kaNGqU1a9Zo69atatGiRbnff/Vyv8cAExERoYiIiBLnvfHGG3rhhRdcl9PT03XbbbdpxYoV6tKliy9L9Btve+OOw+GQ5HxpbmVTmr6kpaUpPj5eHTt21OLFiyttqJAu7/lSFdWoUUMdO3bUF1984fqj6XA49MUXX2jUqFH+LQ4VkmEYGj16tFavXq3Nmzf7JVRIBAvTNG3atNDlWrVqSZJatmypqKgof5RUYXz99df65ptv1KNHD9WrV08HDx7UX/7yF7Vs2bLS7a0ojbS0NMXFxalZs2aaPXu2fv31V9d1jRo18mNl/nfkyBGdOXNGR44ckd1ud50PplWrVq6frapg3LhxGjp0qDp16qTOnTtrzpw5ys7O1p///Gd/l+ZX58+f14EDB1yXU1NTtXv3btWvX7/I7+KqZOTIkUpKStKaNWtUu3Zt11qcOnXqKCQkpPwKKdfXoFQhqampvNz033744QcjPj7eqF+/vhEcHGw0b97ceOyxx4xjx475uzS/Wrx4sSHJ7UdVN3ToULd92bRpk79LK3dvvvmm0bRpU6NGjRpG586dja+++srfJfndpk2b3D4/hg4d6u/S/MrT75PFixeXax2ssQAAAKapvAd0AQBAuSNYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAEJAOHToki8XiOt03gIqBYAGgTOx2u7p166aEhIRC45mZmYqOjtazzz5b4jY++OADWa1WjRw5stT3Hx0drYyMDF177bWlvi0A3+GU3gDKbP/+/erQoYPeeecd3X///ZKkBx98UN9//72++eabQm+B7s6tt96qG2+8UQsWLFB6erpq1qxZHmUD8CH2WAAos6uuukqzZs3S6NGjlZGRoTVr1mj58uX6xz/+UWKoSE1N1fbt2zVp0iRdddVVSk5OLnT9Qw89pOuvv165ubmSpLy8PN1www168MEHJRU9FPLbb7/p/vvvV0REhEJCQtS6dWstXrzY/G8aQLEIFgAuy+jRo9W+fXsNGTJEjz76qKZMmaL27duXeLvFixerf//+qlOnjh544AEtXLiw0PVvvPGGsrOzNWnSJEnSs88+q7Nnz2ru3Llut/eXv/xF//rXv7Ru3Trt3btX8+fPV3h4+OV/gwBKpbq/CwAQ2CwWi+bPn6+rr75a1113nSsIFMfhcGjJkiV68803JUn33nuvxo8fr9TUVLVo0UKSVKtWLS1btkyxsbGqXbu25syZo02bNiksLMztNo8cOaIbbrhBnTp1kiQ1b97cnG8QQKmwxwLAZVu0aJFCQ0OVmpqqY8eOlTg/JSVF2dnZ6tevnyQpPDxcvXv31qJFiwrN69q1qyZMmKAZM2Zo/Pjx6tGjh8dtjhgxQsuXL1eHDh309NNPa/v27Zf3TQEoE4IFgMuyfft2vfbaa1q7dq06d+6shx9+WCWtCV+4cKHOnDmjkJAQVa9eXdWrV9fnn3+upUuXyuFwuOY5HA797//+r6xWqw4cOFDsNm+//XYdPnxYTz75pNLT03XLLbdowoQJpnyPALxHsABQZjk5OUpMTNSIESMUHx+vhQsXaufOnXrrrbc83ub06dOuRZ67d+92fezatUu//fabNmzY4Jr7yiuvaN++fdqyZYvWr19f4mLMiIgIDR06VMuWLdOcOXP09ttvm/a9AvAOaywAlNnkyZNlGIZmzZolybmuYfbs2ZowYYJuv/12t+sc3nvvPTVo0ECDBg2SxWIpdF2/fv20cOFC9e3bV7t27dKUKVO0cuVKde/eXa+++qqeeOIJxcbGKiYmpsh2p0yZoo4dO6pdu3bKzc3V2rVrdfXVV/vk+wbgGXssAJTJli1bNG/ePC1evFihoaGu8eHDh6tbt24eD4ksWrRI//mf/1kkVEjSf/3Xf+mTTz7RsWPH9MADDygxMVF33HGHJOnRRx9VfHy8hgwZIrvdXuS2NWrU0OTJk3X99derV69eslqtWr58uYnfMQBvcIIsAABgGvZYAAAA0xAsAACAaQgWAADANAQLAABgGoIFAAAwDcECAACYhmABAABMQ7AAAACmIVgAAADTECwAAIBpCBYAAMA0/w9S2HU4Jh0p7AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_data_with_centralgradients(X_raw, Y, W_0, b, V_0, c, max_iterations=20, learning_rate=0.1,  threshold=0.001):\n",
        "    \"\"\"\n",
        "    Optimize data using gradient calculations and Monte Carlo method.\n",
        "\n",
        "    :param nn_model: Neural Network Model class.\n",
        "    :param X_raw: Input data.\n",
        "    :param Y: Target data.\n",
        "    :param W_0, b, V_0, c: Initial weights and biases for the neural network.\n",
        "    :param max_iterations: Maximum number of iterations.\n",
        "    :param learning_rate: Learning rate for optimization.\n",
        "    :param MC_num_samples: Number of samples for Monte Carlo method.\n",
        "    :param surrounding_proportion: Proportion of surrounding points' gradients.\n",
        "    :param max_deviation_for_weight: Maximum deviation for weight perturbation.\n",
        "    :param threshold: Threshold for the norm of the second-order gradient.\n",
        "    :return: Optimized X_raw tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_raw_tensor = X_raw.clone().detach().requires_grad_(True) if X_raw.requires_grad else torch.tensor(X_raw, dtype=torch.float64, requires_grad=True)\n",
        "    Y_tensor = Y.clone().detach().requires_grad_(True) if Y.requires_grad else torch.tensor(Y, dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "\n",
        "    # Initialize the neural network with provided weights\n",
        "    nn_model_instance = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "    # Store original weights\n",
        "    original_weights = {\n",
        "        'W_0': nn_model_instance.W_0.data.clone(),\n",
        "        'b': nn_model_instance.b.data.clone(),\n",
        "        'V_0': nn_model_instance.V_0.data.clone(),\n",
        "        'c': nn_model_instance.c.data.clone()\n",
        "    }\n",
        "    print(\"Original weight is {}\".format(original_weights))\n",
        "    print(\"Initial X_GD {}\".format(X_raw_tensor))\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # [Insert the existing logic of your loop here, using nn_model_instance, X_raw_tensor, Y_tensor, and other parameters]\n",
        "        # Calculate the gradient at the central point\n",
        "        central_grad = calculate_second_order_grad(nn_model_instance, X_raw_tensor, Y_tensor)\n",
        "        # Check if grad_X is None before proceeding\n",
        "        central_grad_norm = torch.norm(central_grad)\n",
        "        central_grad = central_grad / central_grad_norm\n",
        "        #print(central_grad)\n",
        "        # Surrouning points' grads\n",
        "        surrounding_grads_pre = []\n",
        "        norms_pre = []\n",
        "        negative_eigenvalues = []\n",
        "\n",
        "\n",
        "        # Combine gradients\n",
        "        combined_grad = central_grad\n",
        "        #combined_grad =  average_surrounding_grad\n",
        "        #print(combined_grad)\n",
        "        # Calculate the norm of the combined gradient\n",
        "        combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "        # Check for a non-zero norm to avoid division by zero\n",
        "        if combined_grad_norm == 0:\n",
        "        # Normalize the gradient\n",
        "          print(\"Gradient is zero; no update required.\")\n",
        "\n",
        "###############\n",
        "\n",
        "        # Check if the norm of the second-order gradient is below the threshold\n",
        "        if torch.norm(combined_grad) < threshold:\n",
        "            print(f\"Convergence reached at iteration {i}\")\n",
        "            break\n",
        "        # Update X_raw using the normalized gradient and learning rate\n",
        "        X_raw_tensor.data -= learning_rate * combined_grad_norm\n",
        "\n",
        "        # Zero out gradients for the next iteration\n",
        "        nn_model_instance.zero_grad()\n",
        "        X_raw_tensor.grad = None\n",
        "        # Update and checks as per your original code\n",
        "\n",
        "\n",
        "\n",
        "    # Print final modified data\n",
        "\n",
        "    print(\"Output X is: {}\".format(X_raw_tensor))\n",
        "\n",
        "\n",
        "    print(central_grad_norm)\n",
        "    # Return the optimized X_raw tensor\n",
        "    return X_raw_tensor\n",
        "\n",
        "# Example usage\n",
        "optimized_X_1 = optimize_data_with_centralgradients(X_GD, Y, W_0, b, V_0, c, max_iterations=250, learning_rate=0.001,  threshold=0.0001)\n"
      ],
      "metadata": {
        "id": "QR7lCrl3d2un",
        "outputId": "169db78c-3218-4d4e-f3fa-f1f575ba690a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-cd124b452df5>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_tensor = Y.clone().detach().requires_grad_(True) if Y.requires_grad else torch.tensor(Y, dtype=torch.float64, requires_grad=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight is {'W_0': tensor([[-7.6083,  3.7633],\n",
            "        [-4.8386,  6.0030]], dtype=torch.float64), 'b': tensor([[-14.7330,   7.7171]], dtype=torch.float64), 'V_0': tensor([[8.9430],\n",
            "        [6.7330]], dtype=torch.float64), 'c': tensor([[-6.7378]], dtype=torch.float64)}\n",
            "Initial X_GD tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -0.9999],\n",
            "        [-1.8611, -1.4674],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.4999, -4.0000],\n",
            "        [-1.0499, -0.9031],\n",
            "        [ 0.4280, -1.9126],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "Output X is: tensor([[-4.2500, -4.2500],\n",
            "        [-4.2500, -1.2499],\n",
            "        [-2.1111, -1.7174],\n",
            "        [-1.2500,  1.7500],\n",
            "        [ 1.7500,  1.7500],\n",
            "        [-3.2500, -4.2500],\n",
            "        [-1.7499, -4.2500],\n",
            "        [-1.2999, -1.1531],\n",
            "        [ 0.1780, -2.1626],\n",
            "        [ 1.2500,  2.2500]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(1.0237, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_central, eigenvalues_central = compute_hessian_and_eigenvalues(nn_model, optimized_X_1, Y)\n",
        "\n",
        "print(eigenvalues_central)\n",
        "check_local_minimum(eigenvalues_central)"
      ],
      "metadata": {
        "id": "LOlqYSsAhqW5",
        "outputId": "ea693ab8-f6a4-4e4c-f9ea-c41851875338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.4494e-01+0.j, 1.4612e-01+0.j, 3.8637e-02+0.j, 1.4144e-02+0.j, 5.4204e-04+0.j,\n",
            "        2.9575e-05+0.j, 1.1508e-05+0.j, 2.2023e-07+0.j, 2.3854e-07+0.j],\n",
            "       dtype=torch.complex128)\n",
            "This is a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable anomaly detection\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "def make_model_copy(original_model):\n",
        "    # Create a new instance of the model\n",
        "    model_copy = SimpleNN(W_0, b, V_0, c)  # Use the same parameters as used to create the original model\n",
        "    # Copy the initial weights from the original model\n",
        "    model_copy.load_state_dict(original_model.state_dict())\n",
        "    return model_copy\n",
        "\n",
        "def trap_model(original_model, X, Y, max_iterations, number_of_x_iterations, weightlr=0.01, xlr=0.01):\n",
        "    X_modifiable = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    for current_iteration in range(max_iterations):\n",
        "        # Use a fresh copy of the model\n",
        "        model = make_model_copy(original_model)\n",
        "\n",
        "        optimizer_weights = optim.SGD(model.parameters(), lr=weightlr)\n",
        "\n",
        "    # Optimizer for the weights\n",
        "    #optimizer_weights = optim.SGD(model.parameters(), lr=weightlr)\n",
        "\n",
        "        # Save initial state of the model\n",
        "        initial_state_dict = model.state_dict()\n",
        "\n",
        "        # Load initial weights using the deep copied state\n",
        "        model.load_state_dict(copy.deepcopy(initial_state_dict))\n",
        "\n",
        "        # Reset the optimizer state\n",
        "        optimizer_weights = optim.SGD(model.parameters(), lr=weightlr)\n",
        "\n",
        "        # Forward pass with current X_modifiable and initial weights\n",
        "        model_output = model(X_modifiable)\n",
        "\n",
        "        old_loss = -torch.mean(Y * torch.log(model_output) + (1 - Y) * torch.log(1 - model_output))\n",
        "\n",
        "        # Gradient descent step for weights\n",
        "        optimizer_weights.zero_grad()\n",
        "        old_loss.backward()\n",
        "        optimizer_weights.step()\n",
        "        # Gather all gradients into a list after flattening\n",
        "        gradients = [param.grad.view(-1) for param in model.parameters() if param.grad is not None]\n",
        "\n",
        "        # Concatenate all gradients to form a single vector and calculate its norm\n",
        "        total_gradient = torch.cat(gradients)\n",
        "        total_gradient_norm = total_gradient.norm()\n",
        "\n",
        "        # Check if the combined gradient norm is below the threshold\n",
        "        if total_gradient_norm < 1e-8:\n",
        "            print(\"Combined gradient norm below threshold, stopping optimization.\")\n",
        "            break\n",
        "\n",
        "        # Save new weights and create a copy of X_modifiable for the inner loop\n",
        "        new_state_dict = model.state_dict()\n",
        "        X_inner = X_modifiable.clone().detach().requires_grad_(True)\n",
        "        optimizer_x = optim.SGD([X_inner], lr=xlr)\n",
        "\n",
        "        for _ in range(number_of_x_iterations):\n",
        "            # Load new weights\n",
        "            model.load_state_dict(new_state_dict)\n",
        "            print(\"before {}\".format(X_inner))\n",
        "            output_new = model(X_inner)\n",
        "            new_loss = -torch.mean(Y * torch.log(output_new) + (1 - Y) * torch.log(1 - output_new))\n",
        "\n",
        "            # Calculate combined loss\n",
        "            combined_loss = old_loss.detach() - new_loss\n",
        "            print(combined_loss)\n",
        "\n",
        "\n",
        "            # Check condition\n",
        "            if combined_loss <= 0:\n",
        "                print(\"acheive! at:{}\".format(current_iteration))\n",
        "                print(X_inner)\n",
        "                break\n",
        "            else:\n",
        "                optimizer_x.zero_grad()\n",
        "                combined_loss.backward()\n",
        "                optimizer_x.step()\n",
        "        #print(X_inner.data)\n",
        "        # Update X_modifiable with changes from the inner loop\n",
        "        # Calculate the difference between X_modifiable and X_inner\n",
        "        difference = (X_modifiable - X_inner).norm()\n",
        "\n",
        "        # Check if the difference is below a certain threshold\n",
        "        if difference < 1e-8:  # You can adjust this threshold as needed\n",
        "          print(\"X_modifiable did not change significantly, stopping early.\")\n",
        "          break\n",
        "        else:\n",
        "\n",
        "          # Update X_modifiable with changes from the inner loop\n",
        "          X_modifiable.data = X_inner.data\n",
        "\n",
        "\n",
        "    # After completing all iterations, load initial weights\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "\n",
        "    # Forward pass with the final X_modifiable and initial weights\n",
        "    final_output = model(X_modifiable)\n",
        "    final_loss = -torch.mean(Y * torch.log(final_output) + (1 - Y) * torch.log(1 - final_output))\n",
        "\n",
        "    # Calculate gradients with respect to the initial weights\n",
        "    optimizer_weights.zero_grad()\n",
        "    final_loss.backward()\n",
        "\n",
        "    # Calculate and print the norm of the gradients for each parameter\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(f\"Gradient norm for {name}: {param.grad.norm().item()}\")\n",
        "\n",
        "    # Return the modified X\n",
        "    return X_modifiable\n",
        "\n",
        "# Initialize your model, X, Y, etc. outside the function\n",
        "# ... [Your initialization code here] ...\n",
        "\n",
        "# Call the function\n",
        "# ... [Call to train_model function here] ...\n",
        "#Example\n",
        "# Initialize your model, X, Y, etc. outside the function\n",
        "#custom_W_0 = [...]  # Replace [...] with your initial weights\n",
        "#custom_b = [...]   # Replace [...] with your initial bias\n",
        "#custom_V_0 = [...]  # Replace [...] with your next layer weights\n",
        "#custom_c = [...]   # Replace [...] with your next layer bias\n",
        "#model = SimpleNN(custom_W_0, custom_b, custom_V_0, custom_c)\n",
        "#X = torch.tensor([...], dtype=torch.float64, requires_grad=True)  # Replace [...] with your initial data\n",
        "#Y = torch.tensor([...], dtype=torch.float64)  # Replace [...] with target data\n",
        "\n",
        "# Call the function\n",
        "nn_model_trap = SimpleNN(W_0, b, V_0, c)\n",
        "trap_X = trap_model(nn_model_trap, X_raw, Y, max_iterations=1000, number_of_x_iterations=2, weightlr= 0.01, xlr= 0.05)\n",
        "plot_tensor(X_raw, label='X_raw', marker='o', color='blue')\n",
        "plot_tensor(trap_X, label='trap_X', marker='o', color='blue')"
      ],
      "metadata": {
        "id": "szMjSQUcynkl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a8ca69b-c547-4849-e5d7-7119ef69b9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4444, -2.4870],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0682,  2.1464],\n",
            "        [ 1.1326, -0.7886],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4446, -2.4873],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0686,  2.1468],\n",
            "        [ 1.1326, -0.7885],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4449, -2.4876],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0689,  2.1471],\n",
            "        [ 1.1326, -0.7885],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4452, -2.4879],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0693,  2.1475],\n",
            "        [ 1.1327, -0.7884],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4455, -2.4881],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0697,  2.1479],\n",
            "        [ 1.1327, -0.7883],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4458, -2.4884],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0700,  2.1483],\n",
            "        [ 1.1328, -0.7882],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4460, -2.4887],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0704,  2.1486],\n",
            "        [ 1.1328, -0.7882],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4463, -2.4890],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0707,  2.1490],\n",
            "        [ 1.1329, -0.7881],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4466, -2.4893],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0711,  2.1494],\n",
            "        [ 1.1329, -0.7880],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4469, -2.4896],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0714,  2.1498],\n",
            "        [ 1.1330, -0.7880],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4472, -2.4899],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0718,  2.1501],\n",
            "        [ 1.1330, -0.7879],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4474, -2.4902],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0721,  2.1505],\n",
            "        [ 1.1330, -0.7878],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4477, -2.4905],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0725,  2.1509],\n",
            "        [ 1.1331, -0.7877],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4480, -2.4908],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0729,  2.1513],\n",
            "        [ 1.1331, -0.7877],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4483, -2.4911],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0732,  2.1516],\n",
            "        [ 1.1332, -0.7876],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4486, -2.4914],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0736,  2.1520],\n",
            "        [ 1.1332, -0.7875],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4488, -2.4917],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0739,  2.1524],\n",
            "        [ 1.1333, -0.7875],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4491, -2.4919],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0743,  2.1528],\n",
            "        [ 1.1333, -0.7874],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4494, -2.4922],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0746,  2.1531],\n",
            "        [ 1.1334, -0.7873],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4497, -2.4925],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0750,  2.1535],\n",
            "        [ 1.1334, -0.7872],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4499, -2.4928],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0753,  2.1539],\n",
            "        [ 1.1334, -0.7872],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4502, -2.4931],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0757,  2.1542],\n",
            "        [ 1.1335, -0.7871],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4505, -2.4934],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0760,  2.1546],\n",
            "        [ 1.1335, -0.7870],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4508, -2.4937],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0764,  2.1550],\n",
            "        [ 1.1336, -0.7870],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4510, -2.4940],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0767,  2.1554],\n",
            "        [ 1.1336, -0.7869],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4513, -2.4943],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0771,  2.1557],\n",
            "        [ 1.1337, -0.7868],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4516, -2.4946],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0775,  2.1561],\n",
            "        [ 1.1337, -0.7867],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4519, -2.4948],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0778,  2.1565],\n",
            "        [ 1.1338, -0.7867],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4521, -2.4951],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0782,  2.1568],\n",
            "        [ 1.1338, -0.7866],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4524, -2.4954],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0785,  2.1572],\n",
            "        [ 1.1338, -0.7865],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4527, -2.4957],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0789,  2.1576],\n",
            "        [ 1.1339, -0.7865],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4530, -2.4960],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0792,  2.1579],\n",
            "        [ 1.1339, -0.7864],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4532, -2.4963],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0796,  2.1583],\n",
            "        [ 1.1340, -0.7863],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4535, -2.4966],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0799,  2.1587],\n",
            "        [ 1.1340, -0.7863],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4538, -2.4969],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0802,  2.1590],\n",
            "        [ 1.1341, -0.7862],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4541, -2.4972],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0806,  2.1594],\n",
            "        [ 1.1341, -0.7861],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4543, -2.4974],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0809,  2.1598],\n",
            "        [ 1.1342, -0.7860],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4546, -2.4977],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0813,  2.1601],\n",
            "        [ 1.1342, -0.7860],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4549, -2.4980],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0816,  2.1605],\n",
            "        [ 1.1342, -0.7859],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4552, -2.4983],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0820,  2.1609],\n",
            "        [ 1.1343, -0.7858],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4554, -2.4986],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0823,  2.1612],\n",
            "        [ 1.1343, -0.7858],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4557, -2.4989],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0827,  2.1616],\n",
            "        [ 1.1344, -0.7857],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4560, -2.4992],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0830,  2.1619],\n",
            "        [ 1.1344, -0.7856],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4563, -2.4994],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0834,  2.1623],\n",
            "        [ 1.1345, -0.7856],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4565, -2.4997],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0837,  2.1627],\n",
            "        [ 1.1345, -0.7855],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4568, -2.5000],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0841,  2.1630],\n",
            "        [ 1.1345, -0.7854],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4571, -2.5003],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0844,  2.1634],\n",
            "        [ 1.1346, -0.7853],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4573, -2.5006],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0848,  2.1638],\n",
            "        [ 1.1346, -0.7853],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4576, -2.5009],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0851,  2.1641],\n",
            "        [ 1.1347, -0.7852],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4579, -2.5012],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0854,  2.1645],\n",
            "        [ 1.1347, -0.7851],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4582, -2.5014],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0858,  2.1648],\n",
            "        [ 1.1348, -0.7851],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4584, -2.5017],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0861,  2.1652],\n",
            "        [ 1.1348, -0.7850],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4587, -2.5020],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0865,  2.1656],\n",
            "        [ 1.1348, -0.7849],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4590, -2.5023],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0868,  2.1659],\n",
            "        [ 1.1349, -0.7849],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4592, -2.5026],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0872,  2.1663],\n",
            "        [ 1.1349, -0.7848],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4595, -2.5029],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0875,  2.1666],\n",
            "        [ 1.1350, -0.7847],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4598, -2.5031],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0878,  2.1670],\n",
            "        [ 1.1350, -0.7847],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4600, -2.5034],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0882,  2.1674],\n",
            "        [ 1.1351, -0.7846],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4603, -2.5037],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0885,  2.1677],\n",
            "        [ 1.1351, -0.7845],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4606, -2.5040],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0889,  2.1681],\n",
            "        [ 1.1352, -0.7844],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4609, -2.5043],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0892,  2.1684],\n",
            "        [ 1.1352, -0.7844],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4611, -2.5046],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0896,  2.1688],\n",
            "        [ 1.1352, -0.7843],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4614, -2.5048],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0899,  2.1692],\n",
            "        [ 1.1353, -0.7842],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4617, -2.5051],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0902,  2.1695],\n",
            "        [ 1.1353, -0.7842],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4619, -2.5054],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0906,  2.1699],\n",
            "        [ 1.1354, -0.7841],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4622, -2.5057],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0909,  2.1702],\n",
            "        [ 1.1354, -0.7840],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4625, -2.5060],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0913,  2.1706],\n",
            "        [ 1.1355, -0.7840],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4627, -2.5063],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0916,  2.1709],\n",
            "        [ 1.1355, -0.7839],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4630, -2.5065],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0919,  2.1713],\n",
            "        [ 1.1355, -0.7838],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4633, -2.5068],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0923,  2.1716],\n",
            "        [ 1.1356, -0.7838],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4635, -2.5071],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0926,  2.1720],\n",
            "        [ 1.1356, -0.7837],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4638, -2.5074],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0929,  2.1724],\n",
            "        [ 1.1357, -0.7836],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4641, -2.5077],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0933,  2.1727],\n",
            "        [ 1.1357, -0.7836],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4643, -2.5079],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0936,  2.1731],\n",
            "        [ 1.1358, -0.7835],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4646, -2.5082],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0940,  2.1734],\n",
            "        [ 1.1358, -0.7834],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4649, -2.5085],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0943,  2.1738],\n",
            "        [ 1.1358, -0.7833],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4651, -2.5088],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0946,  2.1741],\n",
            "        [ 1.1359, -0.7833],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4654, -2.5091],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0950,  2.1745],\n",
            "        [ 1.1359, -0.7832],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4657, -2.5093],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0953,  2.1748],\n",
            "        [ 1.1360, -0.7831],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4659, -2.5096],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0956,  2.1752],\n",
            "        [ 1.1360, -0.7831],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4662, -2.5099],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0960,  2.1755],\n",
            "        [ 1.1361, -0.7830],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4665, -2.5102],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0963,  2.1759],\n",
            "        [ 1.1361, -0.7829],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4667, -2.5105],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0966,  2.1762],\n",
            "        [ 1.1361, -0.7829],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4670, -2.5107],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0970,  2.1766],\n",
            "        [ 1.1362, -0.7828],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4673, -2.5110],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0973,  2.1769],\n",
            "        [ 1.1362, -0.7827],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4675, -2.5113],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0976,  2.1773],\n",
            "        [ 1.1363, -0.7827],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4678, -2.5116],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0980,  2.1776],\n",
            "        [ 1.1363, -0.7826],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4681, -2.5118],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0983,  2.1780],\n",
            "        [ 1.1364, -0.7825],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4683, -2.5121],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0986,  2.1783],\n",
            "        [ 1.1364, -0.7825],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4686, -2.5124],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0990,  2.1787],\n",
            "        [ 1.1364, -0.7824],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4689, -2.5127],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0993,  2.1790],\n",
            "        [ 1.1365, -0.7823],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4691, -2.5130],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0996,  2.1794],\n",
            "        [ 1.1365, -0.7823],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4694, -2.5132],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1000,  2.1797],\n",
            "        [ 1.1366, -0.7822],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4696, -2.5135],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1003,  2.1801],\n",
            "        [ 1.1366, -0.7821],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4699, -2.5138],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1006,  2.1804],\n",
            "        [ 1.1367, -0.7821],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4702, -2.5141],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1010,  2.1808],\n",
            "        [ 1.1367, -0.7820],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4704, -2.5143],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1013,  2.1811],\n",
            "        [ 1.1367, -0.7819],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4707, -2.5146],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1016,  2.1815],\n",
            "        [ 1.1368, -0.7818],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4710, -2.5149],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1020,  2.1818],\n",
            "        [ 1.1368, -0.7818],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4712, -2.5152],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1023,  2.1822],\n",
            "        [ 1.1369, -0.7817],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4715, -2.5154],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1026,  2.1825],\n",
            "        [ 1.1369, -0.7816],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4717, -2.5157],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1029,  2.1829],\n",
            "        [ 1.1370, -0.7816],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4720, -2.5160],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1033,  2.1832],\n",
            "        [ 1.1370, -0.7815],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4723, -2.5163],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1036,  2.1835],\n",
            "        [ 1.1370, -0.7814],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4725, -2.5165],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1039,  2.1839],\n",
            "        [ 1.1371, -0.7814],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4728, -2.5168],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1043,  2.1842],\n",
            "        [ 1.1371, -0.7813],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4731, -2.5171],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1046,  2.1846],\n",
            "        [ 1.1372, -0.7812],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4733, -2.5174],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1049,  2.1849],\n",
            "        [ 1.1372, -0.7812],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4736, -2.5176],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1052,  2.1853],\n",
            "        [ 1.1372, -0.7811],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4738, -2.5179],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1056,  2.1856],\n",
            "        [ 1.1373, -0.7810],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4741, -2.5182],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1059,  2.1860],\n",
            "        [ 1.1373, -0.7810],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4744, -2.5185],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1062,  2.1863],\n",
            "        [ 1.1374, -0.7809],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4746, -2.5187],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1066,  2.1866],\n",
            "        [ 1.1374, -0.7808],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4749, -2.5190],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1069,  2.1870],\n",
            "        [ 1.1375, -0.7808],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4751, -2.5193],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1072,  2.1873],\n",
            "        [ 1.1375, -0.7807],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4754, -2.5195],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1075,  2.1877],\n",
            "        [ 1.1375, -0.7806],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4757, -2.5198],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1079,  2.1880],\n",
            "        [ 1.1376, -0.7806],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4759, -2.5201],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1082,  2.1884],\n",
            "        [ 1.1376, -0.7805],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4762, -2.5204],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1085,  2.1887],\n",
            "        [ 1.1377, -0.7804],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4764, -2.5206],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1088,  2.1890],\n",
            "        [ 1.1377, -0.7804],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4767, -2.5209],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1092,  2.1894],\n",
            "        [ 1.1378, -0.7803],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4770, -2.5212],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1095,  2.1897],\n",
            "        [ 1.1378, -0.7802],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4772, -2.5214],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1098,  2.1901],\n",
            "        [ 1.1378, -0.7802],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4775, -2.5217],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1101,  2.1904],\n",
            "        [ 1.1379, -0.7801],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4777, -2.5220],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1105,  2.1907],\n",
            "        [ 1.1379, -0.7800],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4780, -2.5223],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1108,  2.1911],\n",
            "        [ 1.1380, -0.7800],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4782, -2.5225],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1111,  2.1914],\n",
            "        [ 1.1380, -0.7799],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4785, -2.5228],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1114,  2.1917],\n",
            "        [ 1.1380, -0.7798],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4788, -2.5231],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1117,  2.1921],\n",
            "        [ 1.1381, -0.7798],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4790, -2.5233],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1121,  2.1924],\n",
            "        [ 1.1381, -0.7797],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4793, -2.5236],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1124,  2.1928],\n",
            "        [ 1.1382, -0.7796],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4795, -2.5239],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1127,  2.1931],\n",
            "        [ 1.1382, -0.7796],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4798, -2.5241],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1130,  2.1934],\n",
            "        [ 1.1383, -0.7795],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4800, -2.5244],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1133,  2.1938],\n",
            "        [ 1.1383, -0.7794],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4803, -2.5247],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1137,  2.1941],\n",
            "        [ 1.1383, -0.7794],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4806, -2.5250],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1140,  2.1944],\n",
            "        [ 1.1384, -0.7793],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4808, -2.5252],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1143,  2.1948],\n",
            "        [ 1.1384, -0.7792],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4811, -2.5255],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1146,  2.1951],\n",
            "        [ 1.1385, -0.7792],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4813, -2.5258],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1149,  2.1955],\n",
            "        [ 1.1385, -0.7791],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4816, -2.5260],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1153,  2.1958],\n",
            "        [ 1.1385, -0.7790],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4818, -2.5263],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1156,  2.1961],\n",
            "        [ 1.1386, -0.7790],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4821, -2.5266],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1159,  2.1965],\n",
            "        [ 1.1386, -0.7789],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4823, -2.5268],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1162,  2.1968],\n",
            "        [ 1.1387, -0.7788],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4826, -2.5271],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1165,  2.1971],\n",
            "        [ 1.1387, -0.7788],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4829, -2.5274],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1169,  2.1975],\n",
            "        [ 1.1387, -0.7787],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4831, -2.5276],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1172,  2.1978],\n",
            "        [ 1.1388, -0.7786],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4834, -2.5279],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1175,  2.1981],\n",
            "        [ 1.1388, -0.7786],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4836, -2.5282],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1178,  2.1985],\n",
            "        [ 1.1389, -0.7785],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4839, -2.5284],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1181,  2.1988],\n",
            "        [ 1.1389, -0.7784],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4841, -2.5287],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1184,  2.1991],\n",
            "        [ 1.1390, -0.7784],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4844, -2.5290],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1188,  2.1995],\n",
            "        [ 1.1390, -0.7783],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4846, -2.5292],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1191,  2.1998],\n",
            "        [ 1.1390, -0.7783],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4849, -2.5295],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1194,  2.2001],\n",
            "        [ 1.1391, -0.7782],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4851, -2.5298],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1197,  2.2005],\n",
            "        [ 1.1391, -0.7781],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4854, -2.5300],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1200,  2.2008],\n",
            "        [ 1.1392, -0.7781],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4856, -2.5303],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1203,  2.2011],\n",
            "        [ 1.1392, -0.7780],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4859, -2.5306],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1207,  2.2014],\n",
            "        [ 1.1392, -0.7779],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4862, -2.5308],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1210,  2.2018],\n",
            "        [ 1.1393, -0.7779],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4864, -2.5311],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1213,  2.2021],\n",
            "        [ 1.1393, -0.7778],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4867, -2.5314],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1216,  2.2024],\n",
            "        [ 1.1394, -0.7777],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4869, -2.5316],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1219,  2.2028],\n",
            "        [ 1.1394, -0.7777],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4872, -2.5319],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1222,  2.2031],\n",
            "        [ 1.1394, -0.7776],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4874, -2.5322],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1225,  2.2034],\n",
            "        [ 1.1395, -0.7775],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4877, -2.5324],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1229,  2.2038],\n",
            "        [ 1.1395, -0.7775],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4879, -2.5327],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1232,  2.2041],\n",
            "        [ 1.1396, -0.7774],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4882, -2.5329],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1235,  2.2044],\n",
            "        [ 1.1396, -0.7773],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4884, -2.5332],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1238,  2.2047],\n",
            "        [ 1.1397, -0.7773],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4887, -2.5335],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1241,  2.2051],\n",
            "        [ 1.1397, -0.7772],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4889, -2.5337],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1244,  2.2054],\n",
            "        [ 1.1397, -0.7771],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4892, -2.5340],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1247,  2.2057],\n",
            "        [ 1.1398, -0.7771],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4894, -2.5343],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1250,  2.2061],\n",
            "        [ 1.1398, -0.7770],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4897, -2.5345],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1254,  2.2064],\n",
            "        [ 1.1399, -0.7769],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4899, -2.5348],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1257,  2.2067],\n",
            "        [ 1.1399, -0.7769],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4902, -2.5351],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1260,  2.2070],\n",
            "        [ 1.1399, -0.7768],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4904, -2.5353],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1263,  2.2074],\n",
            "        [ 1.1400, -0.7767],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4907, -2.5356],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1266,  2.2077],\n",
            "        [ 1.1400, -0.7767],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4909, -2.5358],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1269,  2.2080],\n",
            "        [ 1.1401, -0.7766],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4912, -2.5361],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1272,  2.2083],\n",
            "        [ 1.1401, -0.7766],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4914, -2.5364],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1275,  2.2087],\n",
            "        [ 1.1401, -0.7765],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4917, -2.5366],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1278,  2.2090],\n",
            "        [ 1.1402, -0.7764],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4919, -2.5369],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1281,  2.2093],\n",
            "        [ 1.1402, -0.7764],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4922, -2.5371],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1285,  2.2096],\n",
            "        [ 1.1403, -0.7763],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4924, -2.5374],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1288,  2.2100],\n",
            "        [ 1.1403, -0.7762],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4927, -2.5377],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1291,  2.2103],\n",
            "        [ 1.1403, -0.7762],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4929, -2.5379],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1294,  2.2106],\n",
            "        [ 1.1404, -0.7761],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4932, -2.5382],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1297,  2.2109],\n",
            "        [ 1.1404, -0.7760],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4934, -2.5384],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1300,  2.2113],\n",
            "        [ 1.1405, -0.7760],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4937, -2.5387],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1303,  2.2116],\n",
            "        [ 1.1405, -0.7759],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4939, -2.5390],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1306,  2.2119],\n",
            "        [ 1.1405, -0.7758],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4942, -2.5392],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1309,  2.2122],\n",
            "        [ 1.1406, -0.7758],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4944, -2.5395],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1312,  2.2125],\n",
            "        [ 1.1406, -0.7757],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4946, -2.5397],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1315,  2.2129],\n",
            "        [ 1.1407, -0.7757],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4949, -2.5400],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1318,  2.2132],\n",
            "        [ 1.1407, -0.7756],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4951, -2.5403],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1321,  2.2135],\n",
            "        [ 1.1407, -0.7755],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4954, -2.5405],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1325,  2.2138],\n",
            "        [ 1.1408, -0.7755],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4956, -2.5408],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1328,  2.2141],\n",
            "        [ 1.1408, -0.7754],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4959, -2.5410],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1331,  2.2145],\n",
            "        [ 1.1409, -0.7753],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4961, -2.5413],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1334,  2.2148],\n",
            "        [ 1.1409, -0.7753],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4964, -2.5416],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1337,  2.2151],\n",
            "        [ 1.1409, -0.7752],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4966, -2.5418],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1340,  2.2154],\n",
            "        [ 1.1410, -0.7751],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4969, -2.5421],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1343,  2.2157],\n",
            "        [ 1.1410, -0.7751],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4971, -2.5423],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1346,  2.2161],\n",
            "        [ 1.1411, -0.7750],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4974, -2.5426],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1349,  2.2164],\n",
            "        [ 1.1411, -0.7749],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4976, -2.5429],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1352,  2.2167],\n",
            "        [ 1.1411, -0.7749],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4979, -2.5431],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1355,  2.2170],\n",
            "        [ 1.1412, -0.7748],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4981, -2.5434],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1358,  2.2173],\n",
            "        [ 1.1412, -0.7748],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4983, -2.5436],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1361,  2.2177],\n",
            "        [ 1.1413, -0.7747],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4986, -2.5439],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1364,  2.2180],\n",
            "        [ 1.1413, -0.7746],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4988, -2.5441],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1367,  2.2183],\n",
            "        [ 1.1413, -0.7746],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4991, -2.5444],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1370,  2.2186],\n",
            "        [ 1.1414, -0.7745],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4993, -2.5447],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1373,  2.2189],\n",
            "        [ 1.1414, -0.7744],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4996, -2.5449],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1376,  2.2192],\n",
            "        [ 1.1415, -0.7744],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4998, -2.5452],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1379,  2.2196],\n",
            "        [ 1.1415, -0.7743],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5001, -2.5454],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1382,  2.2199],\n",
            "        [ 1.1415, -0.7742],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5003, -2.5457],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1385,  2.2202],\n",
            "        [ 1.1416, -0.7742],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5005, -2.5459],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1388,  2.2205],\n",
            "        [ 1.1416, -0.7741],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5008, -2.5462],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1391,  2.2208],\n",
            "        [ 1.1417, -0.7741],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5010, -2.5464],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1394,  2.2211],\n",
            "        [ 1.1417, -0.7740],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5013, -2.5467],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1397,  2.2215],\n",
            "        [ 1.1417, -0.7739],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5015, -2.5470],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1400,  2.2218],\n",
            "        [ 1.1418, -0.7739],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5018, -2.5472],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1403,  2.2221],\n",
            "        [ 1.1418, -0.7738],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5020, -2.5475],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1406,  2.2224],\n",
            "        [ 1.1419, -0.7737],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5022, -2.5477],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1409,  2.2227],\n",
            "        [ 1.1419, -0.7737],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5025, -2.5480],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1412,  2.2230],\n",
            "        [ 1.1419, -0.7736],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5027, -2.5482],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1415,  2.2233],\n",
            "        [ 1.1420, -0.7736],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5030, -2.5485],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1418,  2.2237],\n",
            "        [ 1.1420, -0.7735],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5032, -2.5487],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1421,  2.2240],\n",
            "        [ 1.1421, -0.7734],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5035, -2.5490],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1424,  2.2243],\n",
            "        [ 1.1421, -0.7734],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5037, -2.5492],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1427,  2.2246],\n",
            "        [ 1.1421, -0.7733],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5039, -2.5495],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1430,  2.2249],\n",
            "        [ 1.1422, -0.7732],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5042, -2.5497],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1433,  2.2252],\n",
            "        [ 1.1422, -0.7732],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5044, -2.5500],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1436,  2.2255],\n",
            "        [ 1.1423, -0.7731],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5047, -2.5503],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1439,  2.2258],\n",
            "        [ 1.1423, -0.7730],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5049, -2.5505],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1442,  2.2262],\n",
            "        [ 1.1423, -0.7730],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5051, -2.5508],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1445,  2.2265],\n",
            "        [ 1.1424, -0.7729],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5054, -2.5510],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1448,  2.2268],\n",
            "        [ 1.1424, -0.7729],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5056, -2.5513],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1451,  2.2271],\n",
            "        [ 1.1425, -0.7728],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5059, -2.5515],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1454,  2.2274],\n",
            "        [ 1.1425, -0.7727],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5061, -2.5518],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1457,  2.2277],\n",
            "        [ 1.1425, -0.7727],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5063, -2.5520],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1460,  2.2280],\n",
            "        [ 1.1426, -0.7726],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5066, -2.5523],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1463,  2.2283],\n",
            "        [ 1.1426, -0.7725],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5068, -2.5525],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1466,  2.2286],\n",
            "        [ 1.1427, -0.7725],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5071, -2.5528],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1469,  2.2290],\n",
            "        [ 1.1427, -0.7724],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5073, -2.5530],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1472,  2.2293],\n",
            "        [ 1.1427, -0.7724],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5075, -2.5533],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1474,  2.2296],\n",
            "        [ 1.1428, -0.7723],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5078, -2.5535],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1477,  2.2299],\n",
            "        [ 1.1428, -0.7722],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5080, -2.5538],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1480,  2.2302],\n",
            "        [ 1.1428, -0.7722],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5083, -2.5540],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1483,  2.2305],\n",
            "        [ 1.1429, -0.7721],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5085, -2.5543],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1486,  2.2308],\n",
            "        [ 1.1429, -0.7720],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5087, -2.5545],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1489,  2.2311],\n",
            "        [ 1.1430, -0.7720],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5090, -2.5548],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1492,  2.2314],\n",
            "        [ 1.1430, -0.7719],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5092, -2.5550],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1495,  2.2317],\n",
            "        [ 1.1430, -0.7719],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5095, -2.5553],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1498,  2.2320],\n",
            "        [ 1.1431, -0.7718],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5097, -2.5555],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1501,  2.2323],\n",
            "        [ 1.1431, -0.7717],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5099, -2.5558],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1504,  2.2327],\n",
            "        [ 1.1432, -0.7717],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5102, -2.5560],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1507,  2.2330],\n",
            "        [ 1.1432, -0.7716],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5104, -2.5563],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1510,  2.2333],\n",
            "        [ 1.1432, -0.7715],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5106, -2.5565],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1513,  2.2336],\n",
            "        [ 1.1433, -0.7715],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5109, -2.5568],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1516,  2.2339],\n",
            "        [ 1.1433, -0.7714],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5111, -2.5570],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1518,  2.2342],\n",
            "        [ 1.1434, -0.7714],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5114, -2.5573],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1521,  2.2345],\n",
            "        [ 1.1434, -0.7713],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5116, -2.5575],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1524,  2.2348],\n",
            "        [ 1.1434, -0.7712],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5118, -2.5578],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1527,  2.2351],\n",
            "        [ 1.1435, -0.7712],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5121, -2.5580],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1530,  2.2354],\n",
            "        [ 1.1435, -0.7711],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5123, -2.5583],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1533,  2.2357],\n",
            "        [ 1.1436, -0.7711],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5125, -2.5585],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1536,  2.2360],\n",
            "        [ 1.1436, -0.7710],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5128, -2.5588],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1539,  2.2363],\n",
            "        [ 1.1436, -0.7709],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5130, -2.5590],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1542,  2.2366],\n",
            "        [ 1.1437, -0.7709],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5132, -2.5593],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1545,  2.2369],\n",
            "        [ 1.1437, -0.7708],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5135, -2.5595],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1547,  2.2372],\n",
            "        [ 1.1437, -0.7707],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5137, -2.5598],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1550,  2.2375],\n",
            "        [ 1.1438, -0.7707],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5140, -2.5600],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1553,  2.2378],\n",
            "        [ 1.1438, -0.7706],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5142, -2.5603],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1556,  2.2381],\n",
            "        [ 1.1439, -0.7706],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5144, -2.5605],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1559,  2.2384],\n",
            "        [ 1.1439, -0.7705],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5147, -2.5607],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1562,  2.2387],\n",
            "        [ 1.1439, -0.7704],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5149, -2.5610],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1565,  2.2390],\n",
            "        [ 1.1440, -0.7704],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5151, -2.5612],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1568,  2.2393],\n",
            "        [ 1.1440, -0.7703],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5154, -2.5615],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1570,  2.2396],\n",
            "        [ 1.1441, -0.7703],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5156, -2.5617],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1573,  2.2399],\n",
            "        [ 1.1441, -0.7702],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5158, -2.5620],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1576,  2.2402],\n",
            "        [ 1.1441, -0.7701],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5161, -2.5622],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1579,  2.2406],\n",
            "        [ 1.1442, -0.7701],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5163, -2.5625],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1582,  2.2409],\n",
            "        [ 1.1442, -0.7700],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5165, -2.5627],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1585,  2.2412],\n",
            "        [ 1.1442, -0.7699],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5168, -2.5630],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1588,  2.2415],\n",
            "        [ 1.1443, -0.7699],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5170, -2.5632],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1591,  2.2418],\n",
            "        [ 1.1443, -0.7698],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5172, -2.5635],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1593,  2.2421],\n",
            "        [ 1.1444, -0.7698],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5175, -2.5637],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1596,  2.2424],\n",
            "        [ 1.1444, -0.7697],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5177, -2.5639],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1599,  2.2427],\n",
            "        [ 1.1444, -0.7696],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5179, -2.5642],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1602,  2.2430],\n",
            "        [ 1.1445, -0.7696],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5182, -2.5644],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1605,  2.2433],\n",
            "        [ 1.1445, -0.7695],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5184, -2.5647],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1608,  2.2435],\n",
            "        [ 1.1446, -0.7695],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5186, -2.5649],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1611,  2.2438],\n",
            "        [ 1.1446, -0.7694],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5189, -2.5652],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1613,  2.2441],\n",
            "        [ 1.1446, -0.7693],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5191, -2.5654],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1616,  2.2444],\n",
            "        [ 1.1447, -0.7693],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5193, -2.5657],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1619,  2.2447],\n",
            "        [ 1.1447, -0.7692],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5196, -2.5659],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1622,  2.2450],\n",
            "        [ 1.1447, -0.7692],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5198, -2.5661],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1625,  2.2453],\n",
            "        [ 1.1448, -0.7691],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5200, -2.5664],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1628,  2.2456],\n",
            "        [ 1.1448, -0.7690],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5203, -2.5666],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1630,  2.2459],\n",
            "        [ 1.1449, -0.7690],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5205, -2.5669],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1633,  2.2462],\n",
            "        [ 1.1449, -0.7689],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5207, -2.5671],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1636,  2.2465],\n",
            "        [ 1.1449, -0.7688],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5210, -2.5674],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1639,  2.2468],\n",
            "        [ 1.1450, -0.7688],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5212, -2.5676],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1642,  2.2471],\n",
            "        [ 1.1450, -0.7687],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5214, -2.5678],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1645,  2.2474],\n",
            "        [ 1.1450, -0.7687],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5216, -2.5681],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1647,  2.2477],\n",
            "        [ 1.1451, -0.7686],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5219, -2.5683],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1650,  2.2480],\n",
            "        [ 1.1451, -0.7685],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5221, -2.5686],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1653,  2.2483],\n",
            "        [ 1.1452, -0.7685],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5223, -2.5688],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1656,  2.2486],\n",
            "        [ 1.1452, -0.7684],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5226, -2.5691],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1659,  2.2489],\n",
            "        [ 1.1452, -0.7684],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5228, -2.5693],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1661,  2.2492],\n",
            "        [ 1.1453, -0.7683],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5230, -2.5695],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1664,  2.2495],\n",
            "        [ 1.1453, -0.7682],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5233, -2.5698],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1667,  2.2498],\n",
            "        [ 1.1454, -0.7682],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5235, -2.5700],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1670,  2.2501],\n",
            "        [ 1.1454, -0.7681],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5237, -2.5703],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1673,  2.2504],\n",
            "        [ 1.1454, -0.7681],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5240, -2.5705],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1675,  2.2507],\n",
            "        [ 1.1455, -0.7680],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5242, -2.5707],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1678,  2.2510],\n",
            "        [ 1.1455, -0.7679],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5244, -2.5710],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1681,  2.2512],\n",
            "        [ 1.1455, -0.7679],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5246, -2.5712],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1684,  2.2515],\n",
            "        [ 1.1456, -0.7678],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5249, -2.5715],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1687,  2.2518],\n",
            "        [ 1.1456, -0.7678],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5251, -2.5717],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1689,  2.2521],\n",
            "        [ 1.1457, -0.7677],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5253, -2.5719],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1692,  2.2524],\n",
            "        [ 1.1457, -0.7676],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5256, -2.5722],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1695,  2.2527],\n",
            "        [ 1.1457, -0.7676],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5258, -2.5724],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1698,  2.2530],\n",
            "        [ 1.1458, -0.7675],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5260, -2.5727],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1701,  2.2533],\n",
            "        [ 1.1458, -0.7675],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5262, -2.5729],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1703,  2.2536],\n",
            "        [ 1.1458, -0.7674],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5265, -2.5731],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1706,  2.2539],\n",
            "        [ 1.1459, -0.7673],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5267, -2.5734],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1709,  2.2542],\n",
            "        [ 1.1459, -0.7673],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5269, -2.5736],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1712,  2.2545],\n",
            "        [ 1.1460, -0.7672],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5272, -2.5739],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1714,  2.2548],\n",
            "        [ 1.1460, -0.7672],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5274, -2.5741],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1717,  2.2550],\n",
            "        [ 1.1460, -0.7671],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5276, -2.5743],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1720,  2.2553],\n",
            "        [ 1.1461, -0.7670],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5278, -2.5746],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1723,  2.2556],\n",
            "        [ 1.1461, -0.7670],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5281, -2.5748],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1725,  2.2559],\n",
            "        [ 1.1461, -0.7669],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5283, -2.5751],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1728,  2.2562],\n",
            "        [ 1.1462, -0.7669],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5285, -2.5753],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1731,  2.2565],\n",
            "        [ 1.1462, -0.7668],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5287, -2.5755],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1734,  2.2568],\n",
            "        [ 1.1463, -0.7667],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5290, -2.5758],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1737,  2.2571],\n",
            "        [ 1.1463, -0.7667],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5292, -2.5760],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1739,  2.2574],\n",
            "        [ 1.1463, -0.7666],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5294, -2.5762],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1742,  2.2577],\n",
            "        [ 1.1464, -0.7666],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5297, -2.5765],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1745,  2.2579],\n",
            "        [ 1.1464, -0.7665],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5299, -2.5767],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1748,  2.2582],\n",
            "        [ 1.1464, -0.7664],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5301, -2.5770],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1750,  2.2585],\n",
            "        [ 1.1465, -0.7664],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5303, -2.5772],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1753,  2.2588],\n",
            "        [ 1.1465, -0.7663],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5306, -2.5774],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1756,  2.2591],\n",
            "        [ 1.1466, -0.7663],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5308, -2.5777],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1758,  2.2594],\n",
            "        [ 1.1466, -0.7662],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5310, -2.5779],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1761,  2.2597],\n",
            "        [ 1.1466, -0.7661],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5312, -2.5781],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1764,  2.2600],\n",
            "        [ 1.1467, -0.7661],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5315, -2.5784],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1767,  2.2602],\n",
            "        [ 1.1467, -0.7660],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5317, -2.5786],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1769,  2.2605],\n",
            "        [ 1.1467, -0.7660],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5319, -2.5789],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1772,  2.2608],\n",
            "        [ 1.1468, -0.7659],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5321, -2.5791],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1775,  2.2611],\n",
            "        [ 1.1468, -0.7659],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5324, -2.5793],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1778,  2.2614],\n",
            "        [ 1.1468, -0.7658],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5326, -2.5796],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1780,  2.2617],\n",
            "        [ 1.1469, -0.7657],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5328, -2.5798],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1783,  2.2620],\n",
            "        [ 1.1469, -0.7657],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5330, -2.5800],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1786,  2.2622],\n",
            "        [ 1.1470, -0.7656],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5333, -2.5803],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1789,  2.2625],\n",
            "        [ 1.1470, -0.7656],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5335, -2.5805],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1791,  2.2628],\n",
            "        [ 1.1470, -0.7655],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5337, -2.5807],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1794,  2.2631],\n",
            "        [ 1.1471, -0.7654],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5339, -2.5810],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1797,  2.2634],\n",
            "        [ 1.1471, -0.7654],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5342, -2.5812],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1799,  2.2637],\n",
            "        [ 1.1471, -0.7653],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5344, -2.5814],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1802,  2.2640],\n",
            "        [ 1.1472, -0.7653],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5346, -2.5817],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1805,  2.2642],\n",
            "        [ 1.1472, -0.7652],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5348, -2.5819],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1808,  2.2645],\n",
            "        [ 1.1473, -0.7651],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5351, -2.5822],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1810,  2.2648],\n",
            "        [ 1.1473, -0.7651],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5353, -2.5824],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1813,  2.2651],\n",
            "        [ 1.1473, -0.7650],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5355, -2.5826],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1816,  2.2654],\n",
            "        [ 1.1474, -0.7650],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5357, -2.5829],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1818,  2.2657],\n",
            "        [ 1.1474, -0.7649],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5359, -2.5831],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1821,  2.2659],\n",
            "        [ 1.1474, -0.7648],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5362, -2.5833],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1824,  2.2662],\n",
            "        [ 1.1475, -0.7648],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5364, -2.5836],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1826,  2.2665],\n",
            "        [ 1.1475, -0.7647],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5366, -2.5838],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1829,  2.2668],\n",
            "        [ 1.1476, -0.7647],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5368, -2.5840],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1832,  2.2671],\n",
            "        [ 1.1476, -0.7646],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5371, -2.5843],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1835,  2.2674],\n",
            "        [ 1.1476, -0.7646],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5373, -2.5845],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1837,  2.2676],\n",
            "        [ 1.1477, -0.7645],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5375, -2.5847],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1840,  2.2679],\n",
            "        [ 1.1477, -0.7644],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5377, -2.5850],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1843,  2.2682],\n",
            "        [ 1.1477, -0.7644],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5379, -2.5852],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1845,  2.2685],\n",
            "        [ 1.1478, -0.7643],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5382, -2.5854],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1848,  2.2688],\n",
            "        [ 1.1478, -0.7643],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5384, -2.5857],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1851,  2.2690],\n",
            "        [ 1.1478, -0.7642],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5386, -2.5859],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1853,  2.2693],\n",
            "        [ 1.1479, -0.7641],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5388, -2.5861],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1856,  2.2696],\n",
            "        [ 1.1479, -0.7641],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5390, -2.5863],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1859,  2.2699],\n",
            "        [ 1.1480, -0.7640],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5393, -2.5866],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1861,  2.2702],\n",
            "        [ 1.1480, -0.7640],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5395, -2.5868],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1864,  2.2705],\n",
            "        [ 1.1480, -0.7639],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5397, -2.5870],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1867,  2.2707],\n",
            "        [ 1.1481, -0.7639],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5399, -2.5873],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1869,  2.2710],\n",
            "        [ 1.1481, -0.7638],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5402, -2.5875],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1872,  2.2713],\n",
            "        [ 1.1481, -0.7637],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5404, -2.5877],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1875,  2.2716],\n",
            "        [ 1.1482, -0.7637],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5406, -2.5880],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1877,  2.2719],\n",
            "        [ 1.1482, -0.7636],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5408, -2.5882],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1880,  2.2721],\n",
            "        [ 1.1482, -0.7636],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5410, -2.5884],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1883,  2.2724],\n",
            "        [ 1.1483, -0.7635],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5413, -2.5887],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1885,  2.2727],\n",
            "        [ 1.1483, -0.7634],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5415, -2.5889],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1888,  2.2730],\n",
            "        [ 1.1484, -0.7634],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5417, -2.5891],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1891,  2.2732],\n",
            "        [ 1.1484, -0.7633],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5419, -2.5894],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1893,  2.2735],\n",
            "        [ 1.1484, -0.7633],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5421, -2.5896],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1896,  2.2738],\n",
            "        [ 1.1485, -0.7632],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5424, -2.5898],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1899,  2.2741],\n",
            "        [ 1.1485, -0.7632],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5426, -2.5900],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1901,  2.2744],\n",
            "        [ 1.1485, -0.7631],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5428, -2.5903],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1904,  2.2746],\n",
            "        [ 1.1486, -0.7630],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5430, -2.5905],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1906,  2.2749],\n",
            "        [ 1.1486, -0.7630],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5432, -2.5907],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1909,  2.2752],\n",
            "        [ 1.1486, -0.7629],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5434, -2.5910],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1912,  2.2755],\n",
            "        [ 1.1487, -0.7629],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5437, -2.5912],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1914,  2.2757],\n",
            "        [ 1.1487, -0.7628],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5439, -2.5914],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1917,  2.2760],\n",
            "        [ 1.1488, -0.7628],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5441, -2.5917],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1920,  2.2763],\n",
            "        [ 1.1488, -0.7627],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5443, -2.5919],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1922,  2.2766],\n",
            "        [ 1.1488, -0.7626],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5445, -2.5921],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1925,  2.2768],\n",
            "        [ 1.1489, -0.7626],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5448, -2.5923],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1928,  2.2771],\n",
            "        [ 1.1489, -0.7625],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5450, -2.5926],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1930,  2.2774],\n",
            "        [ 1.1489, -0.7625],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5452, -2.5928],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1933,  2.2777],\n",
            "        [ 1.1490, -0.7624],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5454, -2.5930],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1935,  2.2779],\n",
            "        [ 1.1490, -0.7624],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5456, -2.5933],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1938,  2.2782],\n",
            "        [ 1.1490, -0.7623],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5458, -2.5935],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1941,  2.2785],\n",
            "        [ 1.1491, -0.7622],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5461, -2.5937],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1943,  2.2788],\n",
            "        [ 1.1491, -0.7622],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5463, -2.5939],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1946,  2.2790],\n",
            "        [ 1.1492, -0.7621],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5465, -2.5942],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1948,  2.2793],\n",
            "        [ 1.1492, -0.7621],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5467, -2.5944],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1951,  2.2796],\n",
            "        [ 1.1492, -0.7620],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5469, -2.5946],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1954,  2.2799],\n",
            "        [ 1.1493, -0.7619],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5471, -2.5948],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1956,  2.2801],\n",
            "        [ 1.1493, -0.7619],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5474, -2.5951],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1959,  2.2804],\n",
            "        [ 1.1493, -0.7618],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5476, -2.5953],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1962,  2.2807],\n",
            "        [ 1.1494, -0.7618],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5478, -2.5955],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1964,  2.2810],\n",
            "        [ 1.1494, -0.7617],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5480, -2.5958],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1967,  2.2812],\n",
            "        [ 1.1494, -0.7617],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5482, -2.5960],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1969,  2.2815],\n",
            "        [ 1.1495, -0.7616],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5484, -2.5962],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1972,  2.2818],\n",
            "        [ 1.1495, -0.7615],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5487, -2.5964],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1975,  2.2821],\n",
            "        [ 1.1495, -0.7615],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5489, -2.5967],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1977,  2.2823],\n",
            "        [ 1.1496, -0.7614],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5491, -2.5969],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1980,  2.2826],\n",
            "        [ 1.1496, -0.7614],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5493, -2.5971],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1982,  2.2829],\n",
            "        [ 1.1497, -0.7613],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5495, -2.5973],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1985,  2.2831],\n",
            "        [ 1.1497, -0.7613],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5497, -2.5976],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1987,  2.2834],\n",
            "        [ 1.1497, -0.7612],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5500, -2.5978],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1990,  2.2837],\n",
            "        [ 1.1498, -0.7611],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5502, -2.5980],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1993,  2.2840],\n",
            "        [ 1.1498, -0.7611],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5504, -2.5982],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1995,  2.2842],\n",
            "        [ 1.1498, -0.7610],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5506, -2.5985],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1998,  2.2845],\n",
            "        [ 1.1499, -0.7610],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5508, -2.5987],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2000,  2.2848],\n",
            "        [ 1.1499, -0.7609],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5510, -2.5989],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2003,  2.2850],\n",
            "        [ 1.1499, -0.7609],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5512, -2.5991],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2006,  2.2853],\n",
            "        [ 1.1500, -0.7608],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5515, -2.5994],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2008,  2.2856],\n",
            "        [ 1.1500, -0.7608],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5517, -2.5996],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2011,  2.2859],\n",
            "        [ 1.1500, -0.7607],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5519, -2.5998],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2013,  2.2861],\n",
            "        [ 1.1501, -0.7606],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5521, -2.6000],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2016,  2.2864],\n",
            "        [ 1.1501, -0.7606],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5523, -2.6003],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2018,  2.2867],\n",
            "        [ 1.1502, -0.7605],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5525, -2.6005],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2021,  2.2869],\n",
            "        [ 1.1502, -0.7605],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5527, -2.6007],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2024,  2.2872],\n",
            "        [ 1.1502, -0.7604],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5530, -2.6009],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2026,  2.2875],\n",
            "        [ 1.1503, -0.7604],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5532, -2.6012],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2029,  2.2877],\n",
            "        [ 1.1503, -0.7603],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5534, -2.6014],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2031,  2.2880],\n",
            "        [ 1.1503, -0.7602],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5536, -2.6016],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2034,  2.2883],\n",
            "        [ 1.1504, -0.7602],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5538, -2.6018],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2036,  2.2885],\n",
            "        [ 1.1504, -0.7601],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5540, -2.6021],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2039,  2.2888],\n",
            "        [ 1.1504, -0.7601],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5542, -2.6023],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2041,  2.2891],\n",
            "        [ 1.1505, -0.7600],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5544, -2.6025],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2044,  2.2893],\n",
            "        [ 1.1505, -0.7600],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5547, -2.6027],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2047,  2.2896],\n",
            "        [ 1.1505, -0.7599],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "Gradient norm for W_0: 0.019236825903563848\n",
            "Gradient norm for b: 0.0006579592084753721\n",
            "Gradient norm for V_0: 0.2777855909850183\n",
            "Gradient norm for c: 0.15941364973995825\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_raw)\n",
        "print(trap_X)\n",
        "hessian_matrix_trap, eigenvalues_trap = compute_hessian_and_eigenvalues(nn_model_trap, trap_X, Y)\n",
        "\n",
        "print(eigenvalues_trap)\n",
        "check_local_minimum(eigenvalues_trap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0CYBTk5J-Fu",
        "outputId": "2bc038e3-d1d9-4050-aca8-4c1e48cecade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [-1.0000, -1.0000],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-1.5000, -0.5000],\n",
            "        [ 1.0000, -1.0000],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5549, -2.6030],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2049,  2.2899],\n",
            "        [ 1.1506, -0.7598],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([ 2.2362e-01+0.j,  4.8039e-02+0.j, -4.8185e-02+0.j, -6.2189e-02+0.j,\n",
            "        -8.2112e-03+0.j, -1.0625e-02+0.j, -2.0603e-07+0.j,  9.2078e-05+0.j,\n",
            "        -1.8258e-04+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_data_with_gradients(X_raw, Y, W_0, b, V_0, c, max_iterations=20, learning_rate=0.1, MC_num_samples=100, surrounding_proportion=0.5, max_deviation_for_weight=0.05, threshold=0.001):\n",
        "    \"\"\"\n",
        "    Optimize data using gradient calculations and Monte Carlo method.\n",
        "\n",
        "    :param nn_model: Neural Network Model class.\n",
        "    :param X_raw: Input data.\n",
        "    :param Y: Target data.\n",
        "    :param W_0, b, V_0, c: Initial weights and biases for the neural network.\n",
        "    :param max_iterations: Maximum number of iterations.\n",
        "    :param learning_rate: Learning rate for optimization.\n",
        "    :param MC_num_samples: Number of samples for Monte Carlo method.\n",
        "    :param surrounding_proportion: Proportion of surrounding points' gradients.\n",
        "    :param max_deviation_for_weight: Maximum deviation for weight perturbation.\n",
        "    :param threshold: Threshold for the norm of the second-order gradient.\n",
        "    :return: Optimized X_raw tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_raw_tensor = X_raw.clone().detach().requires_grad_(True) if X_raw.requires_grad else torch.tensor(X_raw, dtype=torch.float64, requires_grad=True)\n",
        "    Y_tensor = Y.clone().detach().requires_grad_(True) if Y.requires_grad else torch.tensor(Y, dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "\n",
        "    # Initialize the neural network with provided weights\n",
        "    nn_model_sur = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "    # Store original weights\n",
        "    original_weights = {\n",
        "        'W_0': nn_model_sur.W_0.data.clone(),\n",
        "        'b': nn_model_sur.b.data.clone(),\n",
        "        'V_0': nn_model_sur.V_0.data.clone(),\n",
        "        'c': nn_model_sur.c.data.clone()\n",
        "    }\n",
        "    print(\"Original weight is {}\".format(original_weights))\n",
        "    print(\"Initial X_raw_pre {}\".format(X_raw_tensor))\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # [Insert the existing logic of your loop here, using nn_model_instance, X_raw_tensor, Y_tensor, and other parameters]\n",
        "        # Calculate the gradient at the central point\n",
        "        central_grad = calculate_second_order_grad(nn_model_sur, X_raw_tensor, Y_tensor)\n",
        "        # Check if grad_X is None before proceeding\n",
        "        central_grad_norm = torch.norm(central_grad)\n",
        "        central_grad = central_grad / central_grad_norm\n",
        "        #print(central_grad)\n",
        "        # Surrouning points' grads\n",
        "        surrounding_grads_pre = []\n",
        "        norms_pre = []\n",
        "\n",
        "\n",
        "        # Calculate the gradient at the surrounding points by MC\n",
        "        for _ in range(MC_num_samples):\n",
        "\n",
        "            nn_model_sample_pre = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "            #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "            # Perturb weights\n",
        "            #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "            perturb_weights_uniform_fixed_range(nn_model_sample_pre, scale = max_deviation_for_weight)\n",
        "            #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "            grad_pre = calculate_second_order_grad(nn_model_sample_pre, X_raw_tensor, Y_tensor)\n",
        "            grad_norm = torch.norm(grad_pre)\n",
        "            grad_pre = grad_pre / grad_norm\n",
        "            surrounding_grads_pre.append(grad_pre)\n",
        "            #negative_eigenvalues.append(torch.norm(grad_pre).item())\n",
        "\n",
        "        sum_surrounding_grads_pre = sum(surrounding_grads_pre)\n",
        "\n",
        "        # Average the large norm gradients\n",
        "        average_surrounding_grads_pre = sum_surrounding_grads_pre / len(surrounding_grads_pre)\n",
        "        # Calculate average pre_norm\n",
        "        #average_negative_eigenvalues = sum(negative_eigenvalues) / len(negative_eigenvalues)\n",
        "\n",
        "        # Calculate the median of negative eigenvalues\n",
        "        #median_negative_eigenvalue = np.median([eigenvalue for eigenvalue in negative_eigenvalues if eigenvalue < 0])\n",
        "\n",
        "\n",
        "        # Filter gradients corresponding to the smallest 50% of negative eigenvalues\n",
        "        #above_average_negative_eigenvalues = [grad for grad, eigenvalue in zip(surrounding_grads_pre, negative_eigenvalues) if eigenvalue < median_negative_eigenvalue]\n",
        "        #above_average_negative_eigenvalues = [grad / torch.norm(grad) for grad in above_average_negative_eigenvalues]\n",
        "\n",
        "        #print(above_average_grads)\n",
        "        #sum_above_average_negative_eigenvalues = sum(above_average_negative_eigenvalues)\n",
        "\n",
        "        # Average the large norm gradients\n",
        "        #if above_average_negative_eigenvalues:\n",
        "          #average_above_average_negative_eigenvalues = sum_above_average_negative_eigenvalues / len(above_average_negative_eigenvalues)\n",
        "          #print(average_above_average_grad)\n",
        "        #else:\n",
        "          # Handle the case where no gradient is above average\n",
        "          #average_above_average_negative_eigenvalues = torch.zeros_like(X_raw_tensor)\n",
        "\n",
        "\n",
        "        #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "        # Combine gradients\n",
        "        combined_grad = (1-surrounding_proportion) * central_grad + surrounding_proportion * average_surrounding_grads_pre\n",
        "        #combined_grad =  average_surrounding_grad\n",
        "        #print(combined_grad)\n",
        "        # Calculate the norm of the combined gradient\n",
        "        combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "        # Check for a non-zero norm to avoid division by zero\n",
        "        if combined_grad_norm > 0:\n",
        "        # Normalize the gradient\n",
        "          normalized_grad = combined_grad / combined_grad_norm\n",
        "\n",
        "        else:\n",
        "          print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "        # Check if the norm of the second-order gradient is below the threshold\n",
        "        if torch.norm(combined_grad) < threshold:\n",
        "            print(f\"Convergence reached at iteration {i}\")\n",
        "            break\n",
        "        # Update X_raw using the normalized gradient and learning rate\n",
        "        X_raw_tensor.data -= learning_rate * normalized_grad\n",
        "\n",
        "        # Zero out gradients for the next iteration\n",
        "        nn_model_sur.zero_grad()\n",
        "        X_raw_tensor.grad = None\n",
        "        # Update and checks as per your original code\n",
        "\n",
        "\n",
        "\n",
        "    # Print final modified data\n",
        "    #print(surrounding_grads)\n",
        "    #print(\"Final modified X_raw:\")\n",
        "    #if len(above_average_negative_eigenvalues) < 0.5*MC_pre_num_samples:\n",
        "      #print(\"need more MC_pre_num_samples\")\n",
        "    #else:\n",
        "    #print(\"Used surrounding points: {}\".format(len(above_average_negative_eigenvalues)))\n",
        "\n",
        "    print(\"Output X is: {}\".format(X_raw_tensor))\n",
        "    #print(negative_eigenvalues)\n",
        "\n",
        "\n",
        "\n",
        "    # Return the optimized X_raw tensor\n",
        "    return X_raw_tensor\n",
        "\n",
        "# Example usage\n",
        "optimized_X_2 = optimize_data_with_gradients(trap_X, Y, W_0, b, V_0, c, max_iterations=50, learning_rate=0.1, MC_num_samples=100, surrounding_proportion=0.3, max_deviation_for_weight=0.01, threshold=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ug5G6aLkors",
        "outputId": "b59b5df0-4436-4512-c39f-90e4814308fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-5816f8b1477c>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_tensor = Y.clone().detach().requires_grad_(True) if Y.requires_grad else torch.tensor(Y, dtype=torch.float64, requires_grad=True)\n",
            "<ipython-input-3-2e4822dd5dc3>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-3-2e4822dd5dc3>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-3-2e4822dd5dc3>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-3-2e4822dd5dc3>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight is {'W_0': tensor([[-0.9854, -7.0746],\n",
            "        [-0.7950, -5.9667]], dtype=torch.float64), 'b': tensor([[2.9537, 2.3977]], dtype=torch.float64), 'V_0': tensor([[-8.6610],\n",
            "        [-6.9776]], dtype=torch.float64), 'c': tensor([[6.6996]], dtype=torch.float64)}\n",
            "Initial X_raw_pre tensor([[ 0.1222,  0.1974],\n",
            "        [ 0.6699,  1.4908],\n",
            "        [ 0.1828,  0.1296],\n",
            "        [-3.1624, -3.5892],\n",
            "        [ 0.2212,  0.0867],\n",
            "        [ 0.2715, -0.6380],\n",
            "        [ 0.7825,  1.3818],\n",
            "        [ 1.3966,  3.5302],\n",
            "        [ 0.2329,  0.0736],\n",
            "        [ 2.6141, -0.3701]], dtype=torch.float64, requires_grad=True)\n",
            "Output X is: tensor([[-0.1250, -0.0626],\n",
            "        [ 0.2896,  1.2047],\n",
            "        [-0.0676, -0.1333],\n",
            "        [-3.1624, -3.5891],\n",
            "        [-0.0313, -0.1780],\n",
            "        [ 0.2775, -0.6330],\n",
            "        [ 0.3866,  1.0816],\n",
            "        [ 4.3140,  5.7800],\n",
            "        [-0.0203, -0.1916],\n",
            "        [ 1.9599, -0.9091]], dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model_X2 = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_surrounding, eigenvalues_surrounding = compute_hessian_and_eigenvalues(nn_model_X2, optimized_X_2, Y)\n",
        "\n",
        "print(eigenvalues_surrounding)\n",
        "check_local_minimum(eigenvalues_surrounding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4_An8vDPjyE",
        "outputId": "20e0a19c-c0ae-4875-9371-0d09ca831f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.3254+0.j,  0.2093+0.j, -0.1277+0.j, -0.0679+0.j,  0.0189+0.j,  0.0015+0.j,\n",
            "        -0.0028+0.j, -0.0007+0.j, -0.0010+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_X_2 = optimize_data_with_gradients(optimized_X_1, Y, W_0, b, V_0, c, max_iterations=20, learning_rate=0.05, MC_num_samples=100, surrounding_proportion=0.9, max_deviation_for_weight=0.02, threshold=0.001)"
      ],
      "metadata": {
        "id": "mi-VQ5D7R535",
        "outputId": "7105b639-e3a4-4236-cfde-d2a8e4470466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1e9be29dee3d>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_tensor = torch.tensor(X_raw, requires_grad=True)\n",
            "<ipython-input-19-1e9be29dee3d>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_tensor = torch.tensor(Y)\n",
            "<ipython-input-3-a815e9696fa5>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight is {'W_0': tensor([[ 3.9979,  5.2638],\n",
            "        [ 1.0698, -1.8901]], dtype=torch.float64), 'b': tensor([[1.8545, 0.8022]], dtype=torch.float64), 'V_0': tensor([[-10.1389],\n",
            "        [-10.4242]], dtype=torch.float64), 'c': tensor([[5.0116]], dtype=torch.float64)}\n",
            "Initial X_raw_pre tensor([[11.0898, -7.6508],\n",
            "        [ 1.6931, -6.0680],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5432, -6.2103],\n",
            "        [ 1.1082,  0.1752],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-1.3351,  2.2657],\n",
            "        [-3.4568,  2.3168]], dtype=torch.float64, requires_grad=True)\n",
            "Used surrounding points: 50\n",
            "Output X is: tensor([[11.0898, -7.6508],\n",
            "        [ 1.6931, -6.0680],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5432, -6.2103],\n",
            "        [ 1.1082,  0.1752],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-1.3568,  2.0730],\n",
            "        [-3.4338,  2.3244]], dtype=torch.float64, requires_grad=True)\n",
            "[tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_X_3 = optimize_data_with_gradients(optimized_X_2, Y, W_0, b, V_0, c, max_iterations=20, learning_rate=0.02, MC_num_samples=100, surrounding_proportion=0.9, max_deviation_for_weight=0.01, threshold=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7uQAynJwxIu",
        "outputId": "f67d55c1-bd3b-4311-b085-90588d7c7964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1e9be29dee3d>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_tensor = torch.tensor(X_raw, requires_grad=True)\n",
            "<ipython-input-19-1e9be29dee3d>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_tensor = torch.tensor(Y)\n",
            "<ipython-input-3-a815e9696fa5>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight is {'W_0': tensor([[ 3.9979,  5.2638],\n",
            "        [ 1.0698, -1.8901]], dtype=torch.float64), 'b': tensor([[1.8545, 0.8022]], dtype=torch.float64), 'V_0': tensor([[-10.1389],\n",
            "        [-10.4242]], dtype=torch.float64), 'c': tensor([[5.0116]], dtype=torch.float64)}\n",
            "Initial X_raw_pre tensor([[11.0898, -7.6508],\n",
            "        [ 1.6931, -6.0680],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5432, -6.2103],\n",
            "        [ 1.1082,  0.1752],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-1.3568,  2.0730],\n",
            "        [-3.4338,  2.3244]], dtype=torch.float64, requires_grad=True)\n",
            "Used surrounding points: 50\n",
            "Output X is: tensor([[11.0898, -7.6508],\n",
            "        [ 1.6932, -6.0680],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5432, -6.2103],\n",
            "        [ 1.1082,  0.1752],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-1.2705,  1.9455],\n",
            "        [-3.4149,  2.3306]], dtype=torch.float64, requires_grad=True)\n",
            "[tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model_pre = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_pre, eigenvalues_pre = compute_hessian_and_eigenvalues(nn_model_pre, optimized_X_3, Y)\n",
        "\n",
        "print(eigenvalues_pre)\n",
        "check_local_minimum(eigenvalues_pre)"
      ],
      "metadata": {
        "id": "PRbJ8sIWR1ge",
        "outputId": "73e82f59-33e2-4458-ad9c-39c4afa3ba9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.9566e-01+0.j,  4.0277e-03+0.j, -2.7943e-04+0.j, -2.2500e-04+0.j,\n",
            "         3.3531e-05+0.j, -4.1906e-06+0.j,  1.2490e-06+0.j,  4.5318e-08+0.j,\n",
            "         2.1484e-10+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## use the eigenvalues of hessian to decide whether use the grad.\n",
        "\n",
        "def flatten_gradients(X_raw, Y, W_0, b, V_0, c, max_iterations=20, learning_rate=0.1, MC_num_samples=100, surrounding_proportion=0.5, max_deviation_for_weight=0.05, threshold=0.001):\n",
        "    \"\"\"\n",
        "    Optimize data using gradient calculations and Monte Carlo method.\n",
        "\n",
        "    :param nn_model: Neural Network Model class.\n",
        "    :param X_raw: Input data.\n",
        "    :param Y: Target data.\n",
        "    :param W_0, b, V_0, c: Initial weights and biases for the neural network.\n",
        "    :param max_iterations: Maximum number of iterations.\n",
        "    :param learning_rate: Learning rate for optimization.\n",
        "    :param MC_num_samples: Number of samples for Monte Carlo method.\n",
        "    :param surrounding_proportion: Proportion of surrounding points' gradients.\n",
        "    :param max_deviation_for_weight: Maximum deviation for weight perturbation.\n",
        "    :param threshold: Threshold for the norm of the second-order gradient.\n",
        "    :return: Optimized X_raw tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_raw_tensor = torch.tensor(X_raw, requires_grad=True)\n",
        "    Y_tensor = torch.tensor(Y)\n",
        "\n",
        "    # Initialize the neural network with provided weights\n",
        "    nn_model_instance = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "    # Store original weights\n",
        "    original_weights = {\n",
        "        'W_0': nn_model_instance.W_0.data.clone(),\n",
        "        'b': nn_model_instance.b.data.clone(),\n",
        "        'V_0': nn_model_instance.V_0.data.clone(),\n",
        "        'c': nn_model_instance.c.data.clone()\n",
        "    }\n",
        "    print(\"Original weight is {}\".format(original_weights))\n",
        "    print(\"Initial X_raw_pre {}\".format(X_raw_tensor))\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # [Insert the existing logic of your loop here, using nn_model_instance, X_raw_tensor, Y_tensor, and other parameters]\n",
        "        # Calculate the gradient at the central point\n",
        "        central_grad = calculate_second_order_grad(nn_model_instance, X_raw_tensor, Y_tensor)\n",
        "        central_grad_norm = torch.norm(central_grad)\n",
        "        central_grad = central_grad / central_grad_norm\n",
        "        #print(central_grad)\n",
        "        # Surrouning points' grads\n",
        "        surrounding_grads_pre = []\n",
        "        norms_pre = []\n",
        "        negative_eigenvalues = []\n",
        "\n",
        "\n",
        "        # Calculate the gradient at the surrounding points by MC\n",
        "        for _ in range(MC_num_samples):\n",
        "\n",
        "            nn_model_sample_pre = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "            #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "            # Perturb weights\n",
        "            #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "            perturb_weights_uniform_fixed_range(nn_model_sample_pre, scale = max_deviation_for_weight)\n",
        "            #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "            _, eigenvalues = compute_hessian_and_eigenvalues(nn_model_sample_pre, X_raw_tensor, Y_tensor)\n",
        "            # Filter out negative eigenvalues (considering the real part)\n",
        "            negative = [e.real for e in eigenvalues if e.real < 0]\n",
        "            if negative:\n",
        "              most_negative_eigenvalue = min(negative)\n",
        "            else:\n",
        "              # Return None if there are no negative eigenvalues\n",
        "              print(\"FOUND A LOCAM MINIMUM!\")\n",
        "              print(\"FOUND A LOCAM MINIMUM!at:{}\".format(X_raw_tensor))\n",
        "            negative_eigenvalues.append(most_negative_eigenvalue)\n",
        "            negative = []\n",
        "            grad_pre = calculate_second_order_grad(nn_model_sample_pre, X_raw_tensor, Y_tensor)\n",
        "            #grad_norm = torch.norm(grad)\n",
        "            #grad = grad / grad_norm\n",
        "            surrounding_grads_pre.append(grad_pre)\n",
        "            #negative_eigenvalues.append(torch.norm(grad_pre).item())\n",
        "\n",
        "\n",
        "        # Calculate average pre_norm\n",
        "        #average_negative_eigenvalues = sum(negative_eigenvalues) / len(negative_eigenvalues)\n",
        "\n",
        "        # Calculate the median of negative eigenvalues\n",
        "        median_negative_eigenvalue = np.median([eigenvalue for eigenvalue in negative_eigenvalues if eigenvalue < 0])\n",
        "\n",
        "\n",
        "        # Filter gradients corresponding to the smallest 50% of negative eigenvalues\n",
        "        above_average_negative_eigenvalues = [grad for grad, eigenvalue in zip(surrounding_grads_pre, negative_eigenvalues) if eigenvalue < median_negative_eigenvalue]\n",
        "        above_average_negative_eigenvalues = [grad / torch.norm(grad) for grad in above_average_negative_eigenvalues]\n",
        "\n",
        "        #print(above_average_grads)\n",
        "        sum_above_average_negative_eigenvalues = sum(above_average_negative_eigenvalues)\n",
        "\n",
        "        # Average the large norm gradients\n",
        "        if above_average_negative_eigenvalues:\n",
        "          average_above_average_negative_eigenvalues = sum_above_average_negative_eigenvalues / len(above_average_negative_eigenvalues)\n",
        "          #print(average_above_average_grad)\n",
        "        else:\n",
        "          # Handle the case where no gradient is above average\n",
        "          average_above_average_negative_eigenvalues = torch.zeros_like(X_raw_tensor)\n",
        "\n",
        "\n",
        "        #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "        # Combine gradients\n",
        "        combined_grad = (1-surrounding_proportion) * central_grad + surrounding_proportion * average_above_average_negative_eigenvalues\n",
        "        #combined_grad =  average_surrounding_grad\n",
        "        #print(combined_grad)\n",
        "        # Calculate the norm of the combined gradient\n",
        "        combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "        # Check for a non-zero norm to avoid division by zero\n",
        "        if combined_grad_norm > 0:\n",
        "        # Normalize the gradient\n",
        "          normalized_grad = combined_grad / combined_grad_norm\n",
        "\n",
        "        else:\n",
        "          print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "        # Check if the norm of the second-order gradient is below the threshold\n",
        "        if torch.norm(combined_grad) < threshold:\n",
        "            print(f\"Convergence reached at iteration {i}\")\n",
        "            break\n",
        "        # Update X_raw using the normalized gradient and learning rate\n",
        "        X_raw_tensor.data -= learning_rate * normalized_grad\n",
        "\n",
        "        # Zero out gradients for the next iteration\n",
        "        nn_model_instance.zero_grad()\n",
        "        X_raw_tensor.grad = None\n",
        "        # Update and checks as per your original code\n",
        "\n",
        "\n",
        "\n",
        "    # Print final modified data\n",
        "    #print(surrounding_grads)\n",
        "    #print(\"Final modified X_raw:\")\n",
        "    #if len(above_average_negative_eigenvalues) < 0.5*MC_pre_num_samples:\n",
        "      #print(\"need more MC_pre_num_samples\")\n",
        "    #else:\n",
        "    print(\"Used surrounding points: {}\".format(len(above_average_negative_eigenvalues)))\n",
        "\n",
        "    print(\"Output X is: {}\".format(X_raw_tensor))\n",
        "    print(negative_eigenvalues)\n",
        "\n",
        "\n",
        "\n",
        "    # Return the optimized X_raw tensor\n",
        "    return X_raw_tensor\n",
        "\n",
        "# Example usage\n",
        "optimized_X_1 = optimize_data_with_gradients(X_raw, Y, W_0, b, V_0, c, max_iterations=50, learning_rate=0.1, MC_num_samples=100, surrounding_proportion=0.9, max_deviation_for_weight=0.05, threshold=0.001)\n"
      ],
      "metadata": {
        "id": "hzb_VvUdxrG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_raw_torch = torch.tensor(optimized_X_3, requires_grad=True)\n",
        "Y_torch = torch.tensor(Y)\n",
        "\n",
        "\n",
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.001 # Adjust this threshold as needed\n",
        "max_iterations = 30 # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 200\n",
        "\n",
        "# Surrouning points' grads' propotion\n",
        "surrounding_propotion = 0.9\n",
        "\n",
        "# Weight perturbation\n",
        "max_deviation_for_weight = 0.01\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "#original_weights = W_0, b, V_0, c\n",
        "original_weights = {\n",
        "    'W_0': nn_model.W_0.data.clone(),\n",
        "    'b': nn_model.b.data.clone(),\n",
        "    'V_0': nn_model.V_0.data.clone(),\n",
        "    'c': nn_model.c.data.clone()\n",
        "}\n",
        "print(\"Original weight is {}\".format(original_weights))\n",
        "print(\"Initial X_raw {}\".format(X_raw_torch))\n",
        "#max_deviation_for_X = 0.02  # You can adjust this value as needed\n",
        "#perturb_data(X_raw_torch, max_deviation=max_deviation_for_X)\n",
        "#print(\"Perturbed X_raw {}\".format(X_raw_torch))\n",
        "\n",
        "for i in range(max_iterations):\n",
        "\n",
        "    # Calculate the gradient at the central point\n",
        "    central_grad = calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "    central_grad_norm = torch.norm(central_grad)\n",
        "    central_grad = central_grad / central_grad_norm\n",
        "    #print(central_grad)\n",
        "    # Surrouning points' grads\n",
        "    surrounding_grads = []\n",
        "    norms = []\n",
        "\n",
        "\n",
        "    # Calculate the gradient at the surrounding points by MC\n",
        "    for _ in range(MC_num_samples):\n",
        "\n",
        "      nn_model_sample = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "      #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Perturb weights\n",
        "      #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "      perturb_weights_uniform_fixed_range(nn_model_sample,scale = 0.05)\n",
        "      #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Calculate second-order gradient\n",
        "      grad = calculate_second_order_grad(nn_model_sample, X_raw_torch, Y_torch)\n",
        "      #grad_norm = torch.norm(grad)\n",
        "      #grad = grad / grad_norm\n",
        "      surrounding_grads.append(grad)\n",
        "      norms.append(torch.norm(grad).item())\n",
        "\n",
        "    # Calculate average norm\n",
        "    average_norm = sum(norms) / len(norms)\n",
        "\n",
        "    # Filter and sum gradients with norms above average\n",
        "    above_average_grads = [grad for grad, norm in zip(surrounding_grads, norms) if norm > 0.2 * average_norm]\n",
        "    above_average_grads = [grad / torch.norm(grad) for grad in above_average_grads]\n",
        "\n",
        "    #print(above_average_grads)\n",
        "    sum_above_average_grads = sum(above_average_grads)\n",
        "\n",
        "    # Average the large norm gradients\n",
        "    if above_average_grads:\n",
        "      average_above_average_grad = sum_above_average_grads / len(above_average_grads)\n",
        "      #print(average_above_average_grad)\n",
        "    else:\n",
        "    # Handle the case where no gradient is above average\n",
        "      average_above_average_grad = torch.zeros_like(X_raw_torch)\n",
        "\n",
        "\n",
        "    #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "    # Combine gradients\n",
        "    combined_grad = (1-surrounding_propotion) * central_grad + surrounding_propotion * average_above_average_grad\n",
        "    #combined_grad =  average_surrounding_grad\n",
        "    #print(combined_grad)\n",
        "    # Calculate the norm of the combined gradient\n",
        "    combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "    # Check for a non-zero norm to avoid division by zero\n",
        "    if combined_grad_norm > 0:\n",
        "    # Normalize the gradient\n",
        "      normalized_grad = combined_grad / combined_grad_norm\n",
        "\n",
        "    else:\n",
        "      print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "    # Check if the norm of the second-order gradient is below the threshold\n",
        "    if torch.norm(combined_grad) < threshold:\n",
        "        print(f\"Convergence reached at iteration {i}\")\n",
        "        break\n",
        "\n",
        "    # Update X_raw using gradient descent\n",
        "    X_raw_torch.data -= learning_rate * normalized_grad\n",
        "\n",
        "    # Zero out gradients for the next iteration\n",
        "    nn_model.zero_grad()\n",
        "    X_raw_torch.grad = None\n",
        "\n",
        "# Print final modified data\n",
        "#print(surrounding_grads)\n",
        "#print(\"Final modified X_raw:\")\n",
        "if len(above_average_grads) < 100:\n",
        "  print(\"need more MC_num_samples\")\n",
        "else:\n",
        "  print(\"Used surrounding points: {}\".format(len(above_average_grads)))\n",
        "\n",
        "print(X_raw_torch)\n",
        "print(negative_eigenvalues)"
      ],
      "metadata": {
        "id": "UUZ3a_stx46f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_raw_torch = torch.tensor(optimized_X_3, requires_grad=True)\n",
        "Y_torch = torch.tensor(Y)\n",
        "\n",
        "\n",
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.001 # Adjust this threshold as needed\n",
        "max_iterations = 30 # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 200\n",
        "\n",
        "# Surrouning points' grads' propotion\n",
        "surrounding_propotion = 0.9\n",
        "\n",
        "# Weight perturbation\n",
        "max_deviation_for_weight = 0.01\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "#original_weights = W_0, b, V_0, c\n",
        "original_weights = {\n",
        "    'W_0': nn_model.W_0.data.clone(),\n",
        "    'b': nn_model.b.data.clone(),\n",
        "    'V_0': nn_model.V_0.data.clone(),\n",
        "    'c': nn_model.c.data.clone()\n",
        "}\n",
        "print(\"Original weight is {}\".format(original_weights))\n",
        "print(\"Initial X_raw {}\".format(X_raw_torch))\n",
        "#max_deviation_for_X = 0.02  # You can adjust this value as needed\n",
        "#perturb_data(X_raw_torch, max_deviation=max_deviation_for_X)\n",
        "#print(\"Perturbed X_raw {}\".format(X_raw_torch))\n",
        "\n",
        "for i in range(max_iterations):\n",
        "\n",
        "    # Calculate the gradient at the central point\n",
        "    central_grad = calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "    central_grad_norm = torch.norm(central_grad)\n",
        "    central_grad = central_grad / central_grad_norm\n",
        "    #print(central_grad)\n",
        "    # Surrouning points' grads\n",
        "    surrounding_grads = []\n",
        "    norms = []\n",
        "\n",
        "\n",
        "    # Calculate the gradient at the surrounding points by MC\n",
        "    for _ in range(MC_num_samples):\n",
        "\n",
        "      nn_model_sample = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "      #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Perturb weights\n",
        "      #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "      perturb_weights_uniform_fixed_range(nn_model_sample,scale = 0.05)\n",
        "      #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Calculate second-order gradient\n",
        "      grad = calculate_second_order_grad(nn_model_sample, X_raw_torch, Y_torch)\n",
        "      #grad_norm = torch.norm(grad)\n",
        "      #grad = grad / grad_norm\n",
        "      surrounding_grads.append(grad)\n",
        "      norms.append(torch.norm(grad).item())\n",
        "\n",
        "    # Calculate average norm\n",
        "    average_norm = sum(norms) / len(norms)\n",
        "\n",
        "    # Filter and sum gradients with norms above average\n",
        "    above_average_grads = [grad for grad, norm in zip(surrounding_grads, norms) if norm > 0.2 * average_norm]\n",
        "    above_average_grads = [grad / torch.norm(grad) for grad in above_average_grads]\n",
        "\n",
        "    #print(above_average_grads)\n",
        "    sum_above_average_grads = sum(above_average_grads)\n",
        "\n",
        "    # Average the large norm gradients\n",
        "    if above_average_grads:\n",
        "      average_above_average_grad = sum_above_average_grads / len(above_average_grads)\n",
        "      #print(average_above_average_grad)\n",
        "    else:\n",
        "    # Handle the case where no gradient is above average\n",
        "      average_above_average_grad = torch.zeros_like(X_raw_torch)\n",
        "\n",
        "\n",
        "    #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "    # Combine gradients\n",
        "    combined_grad = (1-surrounding_propotion) * central_grad + surrounding_propotion * average_above_average_grad\n",
        "    #combined_grad =  average_surrounding_grad\n",
        "    #print(combined_grad)\n",
        "    # Calculate the norm of the combined gradient\n",
        "    combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "    # Check for a non-zero norm to avoid division by zero\n",
        "    if combined_grad_norm > 0:\n",
        "    # Normalize the gradient\n",
        "      normalized_grad = combined_grad / combined_grad_norm\n",
        "\n",
        "    else:\n",
        "      print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "    # Check if the norm of the second-order gradient is below the threshold\n",
        "    if torch.norm(combined_grad) < threshold:\n",
        "        print(f\"Convergence reached at iteration {i}\")\n",
        "        break\n",
        "\n",
        "    # Update X_raw using gradient descent\n",
        "    X_raw_torch.data -= learning_rate * normalized_grad\n",
        "\n",
        "    # Zero out gradients for the next iteration\n",
        "    nn_model.zero_grad()\n",
        "    X_raw_torch.grad = None\n",
        "\n",
        "# Print final modified data\n",
        "#print(surrounding_grads)\n",
        "#print(\"Final modified X_raw:\")\n",
        "if len(above_average_grads) < 100:\n",
        "  print(\"need more MC_num_samples\")\n",
        "else:\n",
        "  print(\"Used surrounding points: {}\".format(len(above_average_grads)))\n",
        "\n",
        "print(X_raw_torch)\n",
        "print(negative_eigenvalues)"
      ],
      "metadata": {
        "id": "fiqG3HcGV4u0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eead8b58-42e0-442a-f988-71507ff16df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-52c0fdd6e8f8>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_torch = torch.tensor(X_raw_pre, requires_grad=True)\n",
            "<ipython-input-49-52c0fdd6e8f8>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_torch = torch.tensor(Y_pre)\n",
            "<ipython-input-14-a815e9696fa5>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-14-a815e9696fa5>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-14-a815e9696fa5>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-14-a815e9696fa5>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight is {'W_0': tensor([[ 3.9979,  5.2638],\n",
            "        [ 1.0698, -1.8901]], dtype=torch.float64), 'b': tensor([[1.8545, 0.8022]], dtype=torch.float64), 'V_0': tensor([[-10.1389],\n",
            "        [-10.4242]], dtype=torch.float64), 'c': tensor([[5.0116]], dtype=torch.float64)}\n",
            "Initial X_raw tensor([[11.0898, -7.6508],\n",
            "        [ 2.0012, -5.9457],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5426, -6.2105],\n",
            "        [ 1.1080,  0.1753],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-0.8698,  0.8182],\n",
            "        [-3.4649,  2.3132]], dtype=torch.float64, requires_grad=True)\n",
            "Used surrounding points: 200\n",
            "tensor([[11.0898, -7.6508],\n",
            "        [ 2.0012, -5.9457],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5426, -6.2105],\n",
            "        [ 1.1080,  0.1753],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-0.7339,  0.4347],\n",
            "        [-3.4439,  2.3197]], dtype=torch.float64, requires_grad=True)\n",
            "[tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model_final = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_final, eigenvalues_final = compute_hessian_and_eigenvalues(nn_model, X_raw_torch, Y_torch)\n",
        "print(X_raw_torch)\n",
        "print(Y_torch)\n",
        "print(eigenvalues_final)\n",
        "check_local_minimum(eigenvalues_final)"
      ],
      "metadata": {
        "id": "nDgON5VoZ6_X",
        "outputId": "6be587a7-eb5d-4503-bf05-bb0fbe6f0322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[11.0898, -7.6508],\n",
            "        [ 2.0012, -5.9457],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5426, -6.2105],\n",
            "        [ 1.1080,  0.1753],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-0.7339,  0.4347],\n",
            "        [-3.4439,  2.3197]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]], dtype=torch.float64)\n",
            "tensor([ 2.0579e-01+0.j,  7.9276e-03+0.j,  4.7714e-03+0.j, -1.4180e-03+0.j,\n",
            "        -5.0715e-04+0.j, -4.3948e-05+0.j, -4.1439e-06+0.j,  1.1427e-07+0.j,\n",
            "        -1.8893e-09+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "def calculate_gradient_for_smallest_eigenvalue(model, X_raw_torch, Y_torch):\n",
        "    # Forward pass\n",
        "    output = model(X_raw_torch)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "\n",
        "    # First-order gradients (w.r.t weights)\n",
        "    first_order_grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "    # Flatten the first-order gradients\n",
        "    grads_flatten = torch.cat([g.contiguous().view(-1) for g in first_order_grads])\n",
        "\n",
        "    # Hessian computation\n",
        "    hessian = []\n",
        "    for grad in grads_flatten:\n",
        "        # Compute second-order gradients (w.r.t each element in the first-order gradients)\n",
        "        second_order_grads = torch.autograd.grad(grad, model.parameters(), retain_graph=True)\n",
        "\n",
        "        # Flatten and collect the second-order gradients\n",
        "        hessian_row = torch.cat([g.contiguous().view(-1) for g in second_order_grads])\n",
        "        hessian.append(hessian_row)\n",
        "\n",
        "    # Stack to form the Hessian matrix\n",
        "    hessian_matrix = torch.stack(hessian)\n",
        "\n",
        "    # Compute eigenvalues\n",
        "    eigenvalues, _ = torch.linalg.eig(hessian_matrix)\n",
        "    # Extract the real parts of eigenvalues\n",
        "    eigenvalues_real = eigenvalues.real\n",
        "\n",
        "    # Identify the smallest eigenvalue\n",
        "    smallest_eigenvalue = torch.min(eigenvalues_real)\n",
        "\n",
        "    # Check if the smallest eigenvalue requires gradients\n",
        "    if smallest_eigenvalue.requires_grad:\n",
        "      # Compute the gradient of the smallest eigenvalue (or its negative) with respect to X\n",
        "      if smallest_eigenvalue < 0:\n",
        "        grad_X = torch.autograd.grad(-smallest_eigenvalue, X_raw_torch, retain_graph=True)[0]\n",
        "      else:\n",
        "        print(\"FOUND A LOCAL MINIMUM!\")\n",
        "        print(f\"Local minimum at X: {X_raw_torch.detach().numpy()}\")\n",
        "        weights = {name: param.clone().detach().numpy() for name, param in model.named_parameters()}\n",
        "        print(f\"Local minimum at W: {weights}\")\n",
        "        grad_X = None  # or handle this case as you see fit\n",
        "    else:\n",
        "      print(\"Smallest eigenvalue does not require grad or is not part of the computation graph.\")\n",
        "      grad_X = None\n",
        "\n",
        "    return grad_X, eigenvalues, smallest_eigenvalue"
      ],
      "metadata": {
        "id": "E6mAQ3Hz-55q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.05 # Adjust this threshold as needed\n",
        "max_iterations = 10  # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.2\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 10\n",
        "\n",
        "# Surrouning points' grad\n",
        "surrounding_grads = []\n",
        "\n",
        "# parameters for the first layer\n",
        "W_0 = np.array([[1.05954587,-0.05625762],[-0.03749863,1.09518945]])\n",
        "b = np.array([[-0.050686,-0.06894291]])\n",
        "\n",
        "# parameters for the second layer\n",
        "\n",
        "V_0 = np.array([[3.76921058],[-3.72139955]])\n",
        "c = np.array([[-0.0148436]])\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "perturb_weights(nn_model, max_deviation=0.01)\n",
        "restore_weights(nn_model, original_weights)  # Assuming perturb_weights is defined as before\n",
        "print(perturb_weights)\n",
        "K=calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "print(K)\n",
        "print(\"W_0 (after perturbation):\", nn_model.W_0.data)\n",
        "print(\"b (after perturbation):\", nn_model.b.data)\n",
        "print(\"V_0 (after perturbation):\", nn_model.V_0.data)\n",
        "print(\"c (after perturbation):\", nn_model.c.data)"
      ],
      "metadata": {
        "id": "_qolZlVZ-x8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward pass\n",
        "output = nn_model(X_raw_torch)\n",
        "\n",
        "# Compute loss\n",
        "loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "print(loss)\n",
        "# Compute gradients of the loss w.r.t. weights\n",
        "loss.backward(create_graph=True)\n",
        "\n",
        "\n",
        "# Combine and compute the norm of all gradients\n",
        "all_grads = torch.cat([nn_model.W_0.grad.flatten(), nn_model.V_0.grad.flatten(), nn_model.b.grad.flatten(), nn_model.c.grad.flatten()])\n",
        "print(all_grads)\n",
        "grad_norm = torch.norm(all_grads)\n",
        "print(grad_norm)\n",
        "# Compute the derivative of the grad_norm with respect to X\n",
        "second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "print(torch.norm(second_order_grad))\n",
        "# If you want to perform gradient descent on X_raw\n",
        "learning_rate = 0.01\n",
        "#X_raw_torch.data -= learning_rate * second_order_grad"
      ],
      "metadata": {
        "id": "s_654-o11XCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_raw_pre_1 = torch.tensor(X_raw_pre, requires_grad=True)\n",
        "Y_pre_1 = torch.tensor(Y_pre_1)\n",
        "\n",
        "\n",
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.001 # Adjust this threshold as needed\n",
        "max_iterations_pre = 50 # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_pre_num_samples = 150\n",
        "\n",
        "# Surrouning points' grads' propotion\n",
        "surrounding_propotion_pre = 0.5\n",
        "\n",
        "# Weight perturbation\n",
        "max_deviation_for_weight_pre = 0.05\n",
        "\n",
        "nn_model_pre = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "#original_weights = W_0, b, V_0, c\n",
        "original_weights = {\n",
        "    'W_0': nn_model.W_0.data.clone(),\n",
        "    'b': nn_model.b.data.clone(),\n",
        "    'V_0': nn_model.V_0.data.clone(),\n",
        "    'c': nn_model.c.data.clone()\n",
        "}\n",
        "print(\"Original weight is {}\".format(original_weights))\n",
        "print(\"Initial X_raw_pre_1 {}\".format(X_raw_pre_1))\n",
        "#max_deviation_for_X = 0.02  # You can adjust this value as needed\n",
        "#perturb_data(X_raw_torch, max_deviation=max_deviation_for_X)\n",
        "#print(\"Perturbed X_raw {}\".format(X_raw_torch))\n",
        "\n",
        "for i in range(max_iterations_pre):\n",
        "\n",
        "    # Calculate the gradient at the central point\n",
        "    central_grad_pre = calculate_second_order_grad(nn_model_pre, X_raw_pre_1, Y_pre_1)\n",
        "    central_grad_pre_norm = torch.norm(central_grad_pre)\n",
        "    central_grad_pre = central_grad_pre / central_grad_pre_norm\n",
        "    #print(central_grad)\n",
        "    # Surrouning points' grads\n",
        "    surrounding_grads_pre = []\n",
        "    norms_pre = []\n",
        "    negative_eigenvalues = []\n",
        "\n",
        "\n",
        "    # Calculate the gradient at the surrounding points by MC\n",
        "    for _ in range(MC_pre_num_samples):\n",
        "\n",
        "      nn_model_sample_pre = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "      #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Perturb weights\n",
        "      #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "      perturb_weights_uniform_fixed_range(nn_model_sample_pre, scale = 0.1)\n",
        "      #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "      _, eigenvalues = compute_hessian_and_eigenvalues(nn_model_sample_pre, X_raw_pre_1, Y_pre_1)\n",
        "      most_negative_eigenvalue = select_most_negative_eigenvalue(eigenvalues)\n",
        "      negative_eigenvalues.append(most_negative_eigenvalue)\n",
        "      grad_pre = calculate_second_order_grad(nn_model_sample_pre, X_raw_pre_1, Y_pre_1)\n",
        "      #grad_norm = torch.norm(grad)\n",
        "      #grad = grad / grad_norm\n",
        "      surrounding_grads_pre.append(grad_pre)\n",
        "      #negative_eigenvalues.append(torch.norm(grad_pre).item())\n",
        "\n",
        "    # Calculate average pre_norm\n",
        "    average_negative_eigenvalues = sum(negative_eigenvalues) / len(negative_eigenvalues)\n",
        "\n",
        "    # Calculate the median of negative eigenvalues\n",
        "    median_negative_eigenvalue = np.median([eigenvalue for eigenvalue in negative_eigenvalues if eigenvalue < 0])\n",
        "\n",
        "    # Filter gradients corresponding to the smallest 50% of negative eigenvalues\n",
        "    above_average_negative_eigenvalues = [grad for grad, eigenvalue in zip(surrounding_grads_pre, negative_eigenvalues) if eigenvalue < median_negative_eigenvalue]\n",
        "    above_average_negative_eigenvalues = [grad / torch.norm(grad) for grad in above_average_negative_eigenvalues]\n",
        "\n",
        "    #print(above_average_grads)\n",
        "    sum_above_average_negative_eigenvalues = sum(above_average_negative_eigenvalues)\n",
        "\n",
        "    # Average the large norm gradients\n",
        "    if above_average_negative_eigenvalues:\n",
        "      average_above_average_negative_eigenvalues = sum_above_average_negative_eigenvalues / len(above_average_negative_eigenvalues)\n",
        "      #print(average_above_average_grad)\n",
        "    else:\n",
        "    # Handle the case where no gradient is above average\n",
        "      average_above_average_negative_eigenvalues = torch.zeros_like(X_raw_pre_1)\n",
        "\n",
        "\n",
        "    #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "    # Combine gradients\n",
        "    combined_grad_pre = (1-surrounding_propotion_pre) * central_grad_pre + surrounding_propotion_pre * average_above_average_negative_eigenvalues\n",
        "    #combined_grad =  average_surrounding_grad\n",
        "    #print(combined_grad)\n",
        "    # Calculate the norm of the combined gradient\n",
        "    combined_grad_pre_norm = torch.norm(combined_grad_pre)\n",
        "\n",
        "    # Check for a non-zero norm to avoid division by zero\n",
        "    if combined_grad_pre_norm > 0:\n",
        "    # Normalize the gradient\n",
        "      normalized_grad_pre = combined_grad_pre / combined_grad_pre_norm\n",
        "\n",
        "    else:\n",
        "      print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "    # Check if the norm of the second-order gradient is below the threshold\n",
        "    if torch.norm(combined_grad_pre) < threshold:\n",
        "        print(f\"Convergence reached at iteration {i}\")\n",
        "        break\n",
        "    # Update X_raw using the normalized gradient and learning rate\n",
        "    X_raw_pre_1.data -= learning_rate * normalized_grad_pre\n",
        "\n",
        "    # Zero out gradients for the next iteration\n",
        "    nn_model.zero_grad()\n",
        "    X_raw_pre_1.grad = None\n",
        "\n",
        "# Print final modified data\n",
        "#print(surrounding_grads)\n",
        "#print(\"Final modified X_raw:\")\n",
        "if len(above_average_negative_eigenvalues) < 0.5*MC_pre_num_samples:\n",
        "  print(\"need more MC_pre_num_samples\")\n",
        "else:\n",
        "  print(\"Used surrounding points: {}\".format(len(above_average_negative_eigenvalues)))\n",
        "\n",
        "print(X_raw_pre_1)\n",
        "print(negative_eigenvalues)"
      ],
      "metadata": {
        "id": "9UEa48L2rbb3"
      }
    }
  ]
}