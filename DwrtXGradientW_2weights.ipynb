{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPx1IgaFVXGrHM6gcRLpcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zw2788/LocalMinimaConstruction/blob/main/DwrtXGradientW_2weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import Image\n",
        "from torch.autograd import grad\n"
      ],
      "metadata": {
        "id": "6IhL4Cbb1mfH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, custom_W_0, custom_b, custom_V_0, custom_c):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        # Ensure that the custom weights are tensors\n",
        "        custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
        "        custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
        "        custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
        "        custom_c = torch.tensor(custom_c, dtype=torch.float64)\n",
        "\n",
        "        # Set the custom weights and biases\n",
        "        self.W_0 = nn.Parameter(custom_W_0)\n",
        "        self.b = nn.Parameter(custom_b)\n",
        "        self.V_0 = nn.Parameter(custom_V_0)\n",
        "        self.c = nn.Parameter(custom_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.W_0), self.b))\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.V_0), self.c))\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "#custom_W_0 = [[0.1, 0.2], [0.3, 0.4]]  # Replace with your own initial values\n",
        "#custom_b = [0.1, 0.2]  # Replace with your own initial values\n",
        "#custom_V_0 = [[0.1], [0.2]]  # Replace with your own initial values\n",
        "#custom_c = [0.1]  # Replace with your own initial values\n",
        "\n",
        "\n",
        "def calculate_second_order_grad(model, X_raw_torch, Y_torch):\n",
        "    # Forward pass\n",
        "    output = model(X_raw_torch)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "    # Compute gradients of the loss w.r.t. weights\n",
        "    loss.backward(create_graph=True)\n",
        "    # Combine and compute the norm of all gradients\n",
        "    all_grads = torch.cat([param.grad.flatten() for param in model.parameters()])\n",
        "    grad_norm = torch.norm(all_grads)\n",
        "    #print(all_grads)\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "    return second_order_grad\n",
        "\n",
        "def calculate_second_order_grad_trap(model, X_raw_torch, Y_torch):\n",
        "    # Forward pass\n",
        "    output = model(X_raw_torch)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "    # Compute gradients of the loss w.r.t. weights\n",
        "    loss.backward(create_graph=True)\n",
        "    # Combine and compute the norm of all gradients\n",
        "    all_grads = torch.cat([param.grad.flatten() for param in model.parameters()])\n",
        "    grad_norm = torch.norm(all_grads)\n",
        "    #print(all_grads)\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "    return second_order_grad\n",
        "\n",
        "def gradient_descent_on_input(model1, model2, X, Y, max_iterations, learning_rate=0.01):\n",
        "    X_modifiable = X.clone().detach().requires_grad_(True)\n",
        "    optimizer_x = optim.SGD([X_modifiable], lr=learning_rate)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        optimizer_x.zero_grad()\n",
        "        output1 = model1(X_modifiable)\n",
        "        output2 = model2(X_modifiable)\n",
        "        loss = -0.5*torch.mean(Y * torch.log(output1) + (1 - Y) * torch.log(1 - output1))-0.5*torch.mean(Y * torch.log(output2) + (1 - Y) * torch.log(1 - output2))\n",
        "        loss.backward()\n",
        "        optimizer_x.step()\n",
        "        print(i)\n",
        "\n",
        "    return X_modifiable\n"
      ],
      "metadata": {
        "id": "vBoW060Y1pZB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb_weights_normal(model, max_deviation=0.01):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            std_dev = param.abs().mean() * max_deviation\n",
        "            noise = torch.randn(param.size()) * std_dev\n",
        "            param[:] = param + noise\n",
        "\n",
        "def perturb_weights_uniform(model, max_deviation=0.01):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            scale_factor = param.abs().mean() * max_deviation\n",
        "            # Generate uniform noise in the range [-scale_factor, scale_factor]\n",
        "            noise = (torch.rand(param.size()) * 2 - 1) * scale_factor\n",
        "            param[:] = param + noise\n",
        "def perturb_weights_uniform_fixed_range(model, scale):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            # Generate uniform noise in the range [-0.1, 0.1]\n",
        "            noise = (torch.rand(param.size()) * 2 - 1) * scale\n",
        "            param[:] = param + noise\n",
        "\n",
        "def restore_weights(model, saved_state):\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            param[:] = saved_state[name]\n",
        "\n",
        "def perturb_data(X, max_deviation=0.01):\n",
        "    \"\"\"Perturb the data tensor X.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        std_dev = X.abs().mean() * max_deviation\n",
        "        noise = torch.randn(X.size()) * std_dev\n",
        "        X.add_(noise)"
      ],
      "metadata": {
        "id": "iuOrsYOiJo91"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-executing the code to define the function for computing the Hessian matrix and its eigenvalues\n",
        "\n",
        "def compute_hessian_and_eigenvalues(model, data, target):\n",
        "    \"\"\"\n",
        "    Compute the Hessian matrix and its eigenvalues for the weights of a neural network model.\n",
        "\n",
        "    :param model: The neural network model.\n",
        "    :param data: Input data (X).\n",
        "    :param target: Target data (Y).\n",
        "    :return: Hessian matrix and its eigenvalues.\n",
        "    \"\"\"\n",
        "    # Forward pass\n",
        "    output = model(data)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(target * torch.log(output) + (1 - target) * torch.log(1 - output))\n",
        "\n",
        "    # First-order gradients (w.r.t weights)\n",
        "    first_order_grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "    # Flatten the first-order gradients\n",
        "    grads_flatten = torch.cat([g.contiguous().view(-1) for g in first_order_grads])\n",
        "\n",
        "    # Hessian computation\n",
        "    hessian = []\n",
        "    for grad in grads_flatten:\n",
        "        # Compute second-order gradients (w.r.t each element in the first-order gradients)\n",
        "        second_order_grads = torch.autograd.grad(grad, model.parameters(), retain_graph=True)\n",
        "\n",
        "        # Flatten and collect the second-order gradients\n",
        "        hessian_row = torch.cat([g.contiguous().view(-1) for g in second_order_grads])\n",
        "        hessian.append(hessian_row)\n",
        "\n",
        "    # Stack to form the Hessian matrix\n",
        "    hessian_matrix = torch.stack(hessian)\n",
        "\n",
        "    # Compute eigenvalues\n",
        "    eigenvalues, _ = torch.linalg.eig(hessian_matrix)\n",
        "\n",
        "    return hessian_matrix, eigenvalues\n",
        "\n",
        "# Note: To use this function, you'll need to provide your neural network model, the input data (X), and the target data (Y).\n",
        "\n",
        "\n",
        "def check_local_minimum(eigenvalues):\n",
        "    # Check if all eigenvalues have a positive real part\n",
        "    if all(eig.real > 0 for eig in eigenvalues):\n",
        "        print(\"This is a local minimum.\")\n",
        "    else:\n",
        "        print(\"This is not a local minimum.\")\n"
      ],
      "metadata": {
        "id": "DG4bccfJA4Jk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot a single tensor\n",
        "def plot_tensor(tensor_data, label='Data', marker='o', color='blue'):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "\n",
        "    # Plotting the tensor\n",
        "    plt.scatter(tensor_data[:, 0].detach().numpy(), tensor_data[:, 1].detach().numpy(), label=label, marker=marker, color=color)\n",
        "\n",
        "    plt.xlabel('X Axis')\n",
        "    plt.ylabel('Y Axis')\n",
        "    plt.title(f'{label} Plot')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tq4AJolQU59J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/zw2788/LocalMinimaConstruction/main/8shape.csv\")\n",
        "\n",
        "data.head()\n",
        "\n",
        "# data , drop NaN values\n",
        "X_raw,  Y, W_0, b, V_0, c = data[['x_2dvec']].dropna().values, data['y'].dropna().values, data[['W_0']].dropna().values, data[['b']].dropna().values, data[['V_0']].dropna().values, data[['c']].dropna().values\n",
        "\n",
        "#convert string to array\n",
        "\n",
        "X_raw = np.array([eval(s[0]) for s in X_raw])\n",
        "\n",
        "W_0 = np.array([eval(s[0]) for s in W_0])\n",
        "\n",
        "b = np.array([eval(s[0]) for s in b])\n",
        "\n",
        "V_0 = np.array([eval(s[0]) for s in V_0])\n",
        "\n",
        "c = np.array([eval(s[0]) for s in c])\n",
        "\n",
        "# Standardize the input\n",
        "# Leave blank to match the example in paper\n",
        "\n",
        "# formatting\n",
        "Y = Y.reshape((-1, 1))\n",
        "print(X_raw)\n",
        "print(Y)\n",
        "print(W_0)\n",
        "#print(X_raw.shape[0])\n",
        "X_raw = torch.tensor(X_raw, requires_grad=True)\n",
        "Y = torch.tensor(Y)\n",
        "print(W_0, b, V_0, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0h9evkU59mR",
        "outputId": "eb602175-ce50-4d12-da6a-c97b50f0a77a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-4.  -4. ]\n",
            " [-4.  -1. ]\n",
            " [-1.  -1. ]\n",
            " [-1.   2. ]\n",
            " [ 2.   2. ]\n",
            " [-3.  -4. ]\n",
            " [-1.5 -4. ]\n",
            " [-1.5 -0.5]\n",
            " [ 1.  -1. ]\n",
            " [ 1.5  2.5]]\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[-7.60825117  3.76327948]\n",
            " [-4.83862577  6.00303144]]\n",
            "[[-7.60825117  3.76327948]\n",
            " [-4.83862577  6.00303144]] [[-14.73302609   7.71711836]] [[8.94303407]\n",
            " [6.73296582]] [[-6.73776928]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/zw2788/LocalMinimaConstruction/main/8shape2.csv\")\n",
        "\n",
        "data2.head()\n",
        "\n",
        "# data , drop NaN values\n",
        "X_raw2,  Y2, W_02, b2, V_02, c2 = data2[['x_2dvec']].dropna().values, data2['y'].dropna().values, data2[['W_0']].dropna().values, data2[['b']].dropna().values, data2[['V_0']].dropna().values, data2[['c']].dropna().values\n",
        "\n",
        "#convert string to array\n",
        "\n",
        "X_raw2 = np.array([eval(s[0]) for s in X_raw2])\n",
        "\n",
        "W_02 = np.array([eval(s[0]) for s in W_02])\n",
        "\n",
        "b2 = np.array([eval(s[0]) for s in b2])\n",
        "\n",
        "V_02 = np.array([eval(s[0]) for s in V_02])\n",
        "\n",
        "c2 = np.array([eval(s[0]) for s in c2])\n",
        "\n",
        "# Standardize the input\n",
        "# Leave blank to match the example in paper\n",
        "\n",
        "# formatting\n",
        "Y2 = Y2.reshape((-1, 1))\n",
        "print(X_raw2)\n",
        "print(Y2)\n",
        "print(W_02)\n",
        "#print(X_raw.shape[0])\n",
        "X_raw2 = torch.tensor(X_raw2, requires_grad=True)\n",
        "Y2 = torch.tensor(Y2)\n",
        "print(W_02, b2, V_02, c2)"
      ],
      "metadata": {
        "id": "6Fn0a_oaEcoU",
        "outputId": "116b7d7d-b41c-486d-ce50-9f786903a34a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-4.  -4. ]\n",
            " [-4.  -1. ]\n",
            " [-1.  -1. ]\n",
            " [-1.   2. ]\n",
            " [ 2.   2. ]\n",
            " [-3.  -4. ]\n",
            " [-1.5 -4. ]\n",
            " [-1.5 -0.5]\n",
            " [ 1.  -1. ]\n",
            " [ 1.5  2.5]]\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[-7.66574487  3.76099758]\n",
            " [-0.90854956 -4.37151541]]\n",
            "[[-7.66574487  3.76099758]\n",
            " [-0.90854956 -4.37151541]] [[-14.75920603   2.49352328]] [[10.41674876]\n",
            " [-1.40030991]] [[0.002344]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_initial, eigenvalues_initial = compute_hessian_and_eigenvalues(nn_model, X_raw, Y)\n",
        "\n",
        "print(eigenvalues_initial)\n",
        "check_local_minimum(eigenvalues_initial)"
      ],
      "metadata": {
        "id": "zkVm-srfFTQZ",
        "outputId": "d2eba929-c930-446f-b8eb-f6040d1af074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.3788e-01+0.j, -1.4928e-01+0.j, -6.5649e-02+0.j,  7.3909e-02+0.j,\n",
            "         4.5694e-02+0.j,  2.4847e-03+0.j, -1.4595e-03+0.j, -2.0535e-07+0.j,\n",
            "         1.2317e-03+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model2 = SimpleNN(W_02, b2, V_02, c2)\n",
        "hessian_matrix_initial2, eigenvalues_initial2 = compute_hessian_and_eigenvalues(nn_model2, X_raw2, Y2)\n",
        "\n",
        "print(eigenvalues_initial2)\n",
        "check_local_minimum(eigenvalues_initial2)"
      ],
      "metadata": {
        "id": "Rb_5Tk0UFKb9",
        "outputId": "b6dd6a7b-dee9-410b-feae-ccc999017da3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-7.6122e-01+0.j,  1.6606e-01+0.j, -1.3180e-01+0.j,  7.4267e-02+0.j,\n",
            "         2.8923e-02+0.j, -2.1644e-02+0.j,  1.4523e-02+0.j, -1.4199e-04+0.j,\n",
            "        -3.5624e-03+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model_GD1 = SimpleNN(W_0, b, V_0, c)\n",
        "nn_model_GD2 = SimpleNN(W_02, b2, V_02, c2)\n",
        "X_GD = gradient_descent_on_input(nn_model_GD1, nn_model_GD2, X_raw, Y, max_iterations=50000, learning_rate=0.01)\n",
        "print(\"X_raw is {}\".format(X_raw))\n",
        "print(\"X_GD is {}\".format(X_GD))\n",
        "\n",
        "hessian_matrix_GD1, eigenvalues_GD1 = compute_hessian_and_eigenvalues(nn_model_GD1, X_GD, Y)\n",
        "hessian_matrix_GD2, eigenvalues_GD2 = compute_hessian_and_eigenvalues(nn_model_GD2, X_GD, Y)\n",
        "\n",
        "print(eigenvalues_GD1)\n",
        "check_local_minimum(eigenvalues_GD1)\n",
        "print(eigenvalues_GD2)\n",
        "check_local_minimum(eigenvalues_GD2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiZXsWOXbYkq",
        "outputId": "39908f18-8448-43d0-f72f-c1c173cd68a7"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "45028\n",
            "45029\n",
            "45030\n",
            "45031\n",
            "45032\n",
            "45033\n",
            "45034\n",
            "45035\n",
            "45036\n",
            "45037\n",
            "45038\n",
            "45039\n",
            "45040\n",
            "45041\n",
            "45042\n",
            "45043\n",
            "45044\n",
            "45045\n",
            "45046\n",
            "45047\n",
            "45048\n",
            "45049\n",
            "45050\n",
            "45051\n",
            "45052\n",
            "45053\n",
            "45054\n",
            "45055\n",
            "45056\n",
            "45057\n",
            "45058\n",
            "45059\n",
            "45060\n",
            "45061\n",
            "45062\n",
            "45063\n",
            "45064\n",
            "45065\n",
            "45066\n",
            "45067\n",
            "45068\n",
            "45069\n",
            "45070\n",
            "45071\n",
            "45072\n",
            "45073\n",
            "45074\n",
            "45075\n",
            "45076\n",
            "45077\n",
            "45078\n",
            "45079\n",
            "45080\n",
            "45081\n",
            "45082\n",
            "45083\n",
            "45084\n",
            "45085\n",
            "45086\n",
            "45087\n",
            "45088\n",
            "45089\n",
            "45090\n",
            "45091\n",
            "45092\n",
            "45093\n",
            "45094\n",
            "45095\n",
            "45096\n",
            "45097\n",
            "45098\n",
            "45099\n",
            "45100\n",
            "45101\n",
            "45102\n",
            "45103\n",
            "45104\n",
            "45105\n",
            "45106\n",
            "45107\n",
            "45108\n",
            "45109\n",
            "45110\n",
            "45111\n",
            "45112\n",
            "45113\n",
            "45114\n",
            "45115\n",
            "45116\n",
            "45117\n",
            "45118\n",
            "45119\n",
            "45120\n",
            "45121\n",
            "45122\n",
            "45123\n",
            "45124\n",
            "45125\n",
            "45126\n",
            "45127\n",
            "45128\n",
            "45129\n",
            "45130\n",
            "45131\n",
            "45132\n",
            "45133\n",
            "45134\n",
            "45135\n",
            "45136\n",
            "45137\n",
            "45138\n",
            "45139\n",
            "45140\n",
            "45141\n",
            "45142\n",
            "45143\n",
            "45144\n",
            "45145\n",
            "45146\n",
            "45147\n",
            "45148\n",
            "45149\n",
            "45150\n",
            "45151\n",
            "45152\n",
            "45153\n",
            "45154\n",
            "45155\n",
            "45156\n",
            "45157\n",
            "45158\n",
            "45159\n",
            "45160\n",
            "45161\n",
            "45162\n",
            "45163\n",
            "45164\n",
            "45165\n",
            "45166\n",
            "45167\n",
            "45168\n",
            "45169\n",
            "45170\n",
            "45171\n",
            "45172\n",
            "45173\n",
            "45174\n",
            "45175\n",
            "45176\n",
            "45177\n",
            "45178\n",
            "45179\n",
            "45180\n",
            "45181\n",
            "45182\n",
            "45183\n",
            "45184\n",
            "45185\n",
            "45186\n",
            "45187\n",
            "45188\n",
            "45189\n",
            "45190\n",
            "45191\n",
            "45192\n",
            "45193\n",
            "45194\n",
            "45195\n",
            "45196\n",
            "45197\n",
            "45198\n",
            "45199\n",
            "45200\n",
            "45201\n",
            "45202\n",
            "45203\n",
            "45204\n",
            "45205\n",
            "45206\n",
            "45207\n",
            "45208\n",
            "45209\n",
            "45210\n",
            "45211\n",
            "45212\n",
            "45213\n",
            "45214\n",
            "45215\n",
            "45216\n",
            "45217\n",
            "45218\n",
            "45219\n",
            "45220\n",
            "45221\n",
            "45222\n",
            "45223\n",
            "45224\n",
            "45225\n",
            "45226\n",
            "45227\n",
            "45228\n",
            "45229\n",
            "45230\n",
            "45231\n",
            "45232\n",
            "45233\n",
            "45234\n",
            "45235\n",
            "45236\n",
            "45237\n",
            "45238\n",
            "45239\n",
            "45240\n",
            "45241\n",
            "45242\n",
            "45243\n",
            "45244\n",
            "45245\n",
            "45246\n",
            "45247\n",
            "45248\n",
            "45249\n",
            "45250\n",
            "45251\n",
            "45252\n",
            "45253\n",
            "45254\n",
            "45255\n",
            "45256\n",
            "45257\n",
            "45258\n",
            "45259\n",
            "45260\n",
            "45261\n",
            "45262\n",
            "45263\n",
            "45264\n",
            "45265\n",
            "45266\n",
            "45267\n",
            "45268\n",
            "45269\n",
            "45270\n",
            "45271\n",
            "45272\n",
            "45273\n",
            "45274\n",
            "45275\n",
            "45276\n",
            "45277\n",
            "45278\n",
            "45279\n",
            "45280\n",
            "45281\n",
            "45282\n",
            "45283\n",
            "45284\n",
            "45285\n",
            "45286\n",
            "45287\n",
            "45288\n",
            "45289\n",
            "45290\n",
            "45291\n",
            "45292\n",
            "45293\n",
            "45294\n",
            "45295\n",
            "45296\n",
            "45297\n",
            "45298\n",
            "45299\n",
            "45300\n",
            "45301\n",
            "45302\n",
            "45303\n",
            "45304\n",
            "45305\n",
            "45306\n",
            "45307\n",
            "45308\n",
            "45309\n",
            "45310\n",
            "45311\n",
            "45312\n",
            "45313\n",
            "45314\n",
            "45315\n",
            "45316\n",
            "45317\n",
            "45318\n",
            "45319\n",
            "45320\n",
            "45321\n",
            "45322\n",
            "45323\n",
            "45324\n",
            "45325\n",
            "45326\n",
            "45327\n",
            "45328\n",
            "45329\n",
            "45330\n",
            "45331\n",
            "45332\n",
            "45333\n",
            "45334\n",
            "45335\n",
            "45336\n",
            "45337\n",
            "45338\n",
            "45339\n",
            "45340\n",
            "45341\n",
            "45342\n",
            "45343\n",
            "45344\n",
            "45345\n",
            "45346\n",
            "45347\n",
            "45348\n",
            "45349\n",
            "45350\n",
            "45351\n",
            "45352\n",
            "45353\n",
            "45354\n",
            "45355\n",
            "45356\n",
            "45357\n",
            "45358\n",
            "45359\n",
            "45360\n",
            "45361\n",
            "45362\n",
            "45363\n",
            "45364\n",
            "45365\n",
            "45366\n",
            "45367\n",
            "45368\n",
            "45369\n",
            "45370\n",
            "45371\n",
            "45372\n",
            "45373\n",
            "45374\n",
            "45375\n",
            "45376\n",
            "45377\n",
            "45378\n",
            "45379\n",
            "45380\n",
            "45381\n",
            "45382\n",
            "45383\n",
            "45384\n",
            "45385\n",
            "45386\n",
            "45387\n",
            "45388\n",
            "45389\n",
            "45390\n",
            "45391\n",
            "45392\n",
            "45393\n",
            "45394\n",
            "45395\n",
            "45396\n",
            "45397\n",
            "45398\n",
            "45399\n",
            "45400\n",
            "45401\n",
            "45402\n",
            "45403\n",
            "45404\n",
            "45405\n",
            "45406\n",
            "45407\n",
            "45408\n",
            "45409\n",
            "45410\n",
            "45411\n",
            "45412\n",
            "45413\n",
            "45414\n",
            "45415\n",
            "45416\n",
            "45417\n",
            "45418\n",
            "45419\n",
            "45420\n",
            "45421\n",
            "45422\n",
            "45423\n",
            "45424\n",
            "45425\n",
            "45426\n",
            "45427\n",
            "45428\n",
            "45429\n",
            "45430\n",
            "45431\n",
            "45432\n",
            "45433\n",
            "45434\n",
            "45435\n",
            "45436\n",
            "45437\n",
            "45438\n",
            "45439\n",
            "45440\n",
            "45441\n",
            "45442\n",
            "45443\n",
            "45444\n",
            "45445\n",
            "45446\n",
            "45447\n",
            "45448\n",
            "45449\n",
            "45450\n",
            "45451\n",
            "45452\n",
            "45453\n",
            "45454\n",
            "45455\n",
            "45456\n",
            "45457\n",
            "45458\n",
            "45459\n",
            "45460\n",
            "45461\n",
            "45462\n",
            "45463\n",
            "45464\n",
            "45465\n",
            "45466\n",
            "45467\n",
            "45468\n",
            "45469\n",
            "45470\n",
            "45471\n",
            "45472\n",
            "45473\n",
            "45474\n",
            "45475\n",
            "45476\n",
            "45477\n",
            "45478\n",
            "45479\n",
            "45480\n",
            "45481\n",
            "45482\n",
            "45483\n",
            "45484\n",
            "45485\n",
            "45486\n",
            "45487\n",
            "45488\n",
            "45489\n",
            "45490\n",
            "45491\n",
            "45492\n",
            "45493\n",
            "45494\n",
            "45495\n",
            "45496\n",
            "45497\n",
            "45498\n",
            "45499\n",
            "45500\n",
            "45501\n",
            "45502\n",
            "45503\n",
            "45504\n",
            "45505\n",
            "45506\n",
            "45507\n",
            "45508\n",
            "45509\n",
            "45510\n",
            "45511\n",
            "45512\n",
            "45513\n",
            "45514\n",
            "45515\n",
            "45516\n",
            "45517\n",
            "45518\n",
            "45519\n",
            "45520\n",
            "45521\n",
            "45522\n",
            "45523\n",
            "45524\n",
            "45525\n",
            "45526\n",
            "45527\n",
            "45528\n",
            "45529\n",
            "45530\n",
            "45531\n",
            "45532\n",
            "45533\n",
            "45534\n",
            "45535\n",
            "45536\n",
            "45537\n",
            "45538\n",
            "45539\n",
            "45540\n",
            "45541\n",
            "45542\n",
            "45543\n",
            "45544\n",
            "45545\n",
            "45546\n",
            "45547\n",
            "45548\n",
            "45549\n",
            "45550\n",
            "45551\n",
            "45552\n",
            "45553\n",
            "45554\n",
            "45555\n",
            "45556\n",
            "45557\n",
            "45558\n",
            "45559\n",
            "45560\n",
            "45561\n",
            "45562\n",
            "45563\n",
            "45564\n",
            "45565\n",
            "45566\n",
            "45567\n",
            "45568\n",
            "45569\n",
            "45570\n",
            "45571\n",
            "45572\n",
            "45573\n",
            "45574\n",
            "45575\n",
            "45576\n",
            "45577\n",
            "45578\n",
            "45579\n",
            "45580\n",
            "45581\n",
            "45582\n",
            "45583\n",
            "45584\n",
            "45585\n",
            "45586\n",
            "45587\n",
            "45588\n",
            "45589\n",
            "45590\n",
            "45591\n",
            "45592\n",
            "45593\n",
            "45594\n",
            "45595\n",
            "45596\n",
            "45597\n",
            "45598\n",
            "45599\n",
            "45600\n",
            "45601\n",
            "45602\n",
            "45603\n",
            "45604\n",
            "45605\n",
            "45606\n",
            "45607\n",
            "45608\n",
            "45609\n",
            "45610\n",
            "45611\n",
            "45612\n",
            "45613\n",
            "45614\n",
            "45615\n",
            "45616\n",
            "45617\n",
            "45618\n",
            "45619\n",
            "45620\n",
            "45621\n",
            "45622\n",
            "45623\n",
            "45624\n",
            "45625\n",
            "45626\n",
            "45627\n",
            "45628\n",
            "45629\n",
            "45630\n",
            "45631\n",
            "45632\n",
            "45633\n",
            "45634\n",
            "45635\n",
            "45636\n",
            "45637\n",
            "45638\n",
            "45639\n",
            "45640\n",
            "45641\n",
            "45642\n",
            "45643\n",
            "45644\n",
            "45645\n",
            "45646\n",
            "45647\n",
            "45648\n",
            "45649\n",
            "45650\n",
            "45651\n",
            "45652\n",
            "45653\n",
            "45654\n",
            "45655\n",
            "45656\n",
            "45657\n",
            "45658\n",
            "45659\n",
            "45660\n",
            "45661\n",
            "45662\n",
            "45663\n",
            "45664\n",
            "45665\n",
            "45666\n",
            "45667\n",
            "45668\n",
            "45669\n",
            "45670\n",
            "45671\n",
            "45672\n",
            "45673\n",
            "45674\n",
            "45675\n",
            "45676\n",
            "45677\n",
            "45678\n",
            "45679\n",
            "45680\n",
            "45681\n",
            "45682\n",
            "45683\n",
            "45684\n",
            "45685\n",
            "45686\n",
            "45687\n",
            "45688\n",
            "45689\n",
            "45690\n",
            "45691\n",
            "45692\n",
            "45693\n",
            "45694\n",
            "45695\n",
            "45696\n",
            "45697\n",
            "45698\n",
            "45699\n",
            "45700\n",
            "45701\n",
            "45702\n",
            "45703\n",
            "45704\n",
            "45705\n",
            "45706\n",
            "45707\n",
            "45708\n",
            "45709\n",
            "45710\n",
            "45711\n",
            "45712\n",
            "45713\n",
            "45714\n",
            "45715\n",
            "45716\n",
            "45717\n",
            "45718\n",
            "45719\n",
            "45720\n",
            "45721\n",
            "45722\n",
            "45723\n",
            "45724\n",
            "45725\n",
            "45726\n",
            "45727\n",
            "45728\n",
            "45729\n",
            "45730\n",
            "45731\n",
            "45732\n",
            "45733\n",
            "45734\n",
            "45735\n",
            "45736\n",
            "45737\n",
            "45738\n",
            "45739\n",
            "45740\n",
            "45741\n",
            "45742\n",
            "45743\n",
            "45744\n",
            "45745\n",
            "45746\n",
            "45747\n",
            "45748\n",
            "45749\n",
            "45750\n",
            "45751\n",
            "45752\n",
            "45753\n",
            "45754\n",
            "45755\n",
            "45756\n",
            "45757\n",
            "45758\n",
            "45759\n",
            "45760\n",
            "45761\n",
            "45762\n",
            "45763\n",
            "45764\n",
            "45765\n",
            "45766\n",
            "45767\n",
            "45768\n",
            "45769\n",
            "45770\n",
            "45771\n",
            "45772\n",
            "45773\n",
            "45774\n",
            "45775\n",
            "45776\n",
            "45777\n",
            "45778\n",
            "45779\n",
            "45780\n",
            "45781\n",
            "45782\n",
            "45783\n",
            "45784\n",
            "45785\n",
            "45786\n",
            "45787\n",
            "45788\n",
            "45789\n",
            "45790\n",
            "45791\n",
            "45792\n",
            "45793\n",
            "45794\n",
            "45795\n",
            "45796\n",
            "45797\n",
            "45798\n",
            "45799\n",
            "45800\n",
            "45801\n",
            "45802\n",
            "45803\n",
            "45804\n",
            "45805\n",
            "45806\n",
            "45807\n",
            "45808\n",
            "45809\n",
            "45810\n",
            "45811\n",
            "45812\n",
            "45813\n",
            "45814\n",
            "45815\n",
            "45816\n",
            "45817\n",
            "45818\n",
            "45819\n",
            "45820\n",
            "45821\n",
            "45822\n",
            "45823\n",
            "45824\n",
            "45825\n",
            "45826\n",
            "45827\n",
            "45828\n",
            "45829\n",
            "45830\n",
            "45831\n",
            "45832\n",
            "45833\n",
            "45834\n",
            "45835\n",
            "45836\n",
            "45837\n",
            "45838\n",
            "45839\n",
            "45840\n",
            "45841\n",
            "45842\n",
            "45843\n",
            "45844\n",
            "45845\n",
            "45846\n",
            "45847\n",
            "45848\n",
            "45849\n",
            "45850\n",
            "45851\n",
            "45852\n",
            "45853\n",
            "45854\n",
            "45855\n",
            "45856\n",
            "45857\n",
            "45858\n",
            "45859\n",
            "45860\n",
            "45861\n",
            "45862\n",
            "45863\n",
            "45864\n",
            "45865\n",
            "45866\n",
            "45867\n",
            "45868\n",
            "45869\n",
            "45870\n",
            "45871\n",
            "45872\n",
            "45873\n",
            "45874\n",
            "45875\n",
            "45876\n",
            "45877\n",
            "45878\n",
            "45879\n",
            "45880\n",
            "45881\n",
            "45882\n",
            "45883\n",
            "45884\n",
            "45885\n",
            "45886\n",
            "45887\n",
            "45888\n",
            "45889\n",
            "45890\n",
            "45891\n",
            "45892\n",
            "45893\n",
            "45894\n",
            "45895\n",
            "45896\n",
            "45897\n",
            "45898\n",
            "45899\n",
            "45900\n",
            "45901\n",
            "45902\n",
            "45903\n",
            "45904\n",
            "45905\n",
            "45906\n",
            "45907\n",
            "45908\n",
            "45909\n",
            "45910\n",
            "45911\n",
            "45912\n",
            "45913\n",
            "45914\n",
            "45915\n",
            "45916\n",
            "45917\n",
            "45918\n",
            "45919\n",
            "45920\n",
            "45921\n",
            "45922\n",
            "45923\n",
            "45924\n",
            "45925\n",
            "45926\n",
            "45927\n",
            "45928\n",
            "45929\n",
            "45930\n",
            "45931\n",
            "45932\n",
            "45933\n",
            "45934\n",
            "45935\n",
            "45936\n",
            "45937\n",
            "45938\n",
            "45939\n",
            "45940\n",
            "45941\n",
            "45942\n",
            "45943\n",
            "45944\n",
            "45945\n",
            "45946\n",
            "45947\n",
            "45948\n",
            "45949\n",
            "45950\n",
            "45951\n",
            "45952\n",
            "45953\n",
            "45954\n",
            "45955\n",
            "45956\n",
            "45957\n",
            "45958\n",
            "45959\n",
            "45960\n",
            "45961\n",
            "45962\n",
            "45963\n",
            "45964\n",
            "45965\n",
            "45966\n",
            "45967\n",
            "45968\n",
            "45969\n",
            "45970\n",
            "45971\n",
            "45972\n",
            "45973\n",
            "45974\n",
            "45975\n",
            "45976\n",
            "45977\n",
            "45978\n",
            "45979\n",
            "45980\n",
            "45981\n",
            "45982\n",
            "45983\n",
            "45984\n",
            "45985\n",
            "45986\n",
            "45987\n",
            "45988\n",
            "45989\n",
            "45990\n",
            "45991\n",
            "45992\n",
            "45993\n",
            "45994\n",
            "45995\n",
            "45996\n",
            "45997\n",
            "45998\n",
            "45999\n",
            "46000\n",
            "46001\n",
            "46002\n",
            "46003\n",
            "46004\n",
            "46005\n",
            "46006\n",
            "46007\n",
            "46008\n",
            "46009\n",
            "46010\n",
            "46011\n",
            "46012\n",
            "46013\n",
            "46014\n",
            "46015\n",
            "46016\n",
            "46017\n",
            "46018\n",
            "46019\n",
            "46020\n",
            "46021\n",
            "46022\n",
            "46023\n",
            "46024\n",
            "46025\n",
            "46026\n",
            "46027\n",
            "46028\n",
            "46029\n",
            "46030\n",
            "46031\n",
            "46032\n",
            "46033\n",
            "46034\n",
            "46035\n",
            "46036\n",
            "46037\n",
            "46038\n",
            "46039\n",
            "46040\n",
            "46041\n",
            "46042\n",
            "46043\n",
            "46044\n",
            "46045\n",
            "46046\n",
            "46047\n",
            "46048\n",
            "46049\n",
            "46050\n",
            "46051\n",
            "46052\n",
            "46053\n",
            "46054\n",
            "46055\n",
            "46056\n",
            "46057\n",
            "46058\n",
            "46059\n",
            "46060\n",
            "46061\n",
            "46062\n",
            "46063\n",
            "46064\n",
            "46065\n",
            "46066\n",
            "46067\n",
            "46068\n",
            "46069\n",
            "46070\n",
            "46071\n",
            "46072\n",
            "46073\n",
            "46074\n",
            "46075\n",
            "46076\n",
            "46077\n",
            "46078\n",
            "46079\n",
            "46080\n",
            "46081\n",
            "46082\n",
            "46083\n",
            "46084\n",
            "46085\n",
            "46086\n",
            "46087\n",
            "46088\n",
            "46089\n",
            "46090\n",
            "46091\n",
            "46092\n",
            "46093\n",
            "46094\n",
            "46095\n",
            "46096\n",
            "46097\n",
            "46098\n",
            "46099\n",
            "46100\n",
            "46101\n",
            "46102\n",
            "46103\n",
            "46104\n",
            "46105\n",
            "46106\n",
            "46107\n",
            "46108\n",
            "46109\n",
            "46110\n",
            "46111\n",
            "46112\n",
            "46113\n",
            "46114\n",
            "46115\n",
            "46116\n",
            "46117\n",
            "46118\n",
            "46119\n",
            "46120\n",
            "46121\n",
            "46122\n",
            "46123\n",
            "46124\n",
            "46125\n",
            "46126\n",
            "46127\n",
            "46128\n",
            "46129\n",
            "46130\n",
            "46131\n",
            "46132\n",
            "46133\n",
            "46134\n",
            "46135\n",
            "46136\n",
            "46137\n",
            "46138\n",
            "46139\n",
            "46140\n",
            "46141\n",
            "46142\n",
            "46143\n",
            "46144\n",
            "46145\n",
            "46146\n",
            "46147\n",
            "46148\n",
            "46149\n",
            "46150\n",
            "46151\n",
            "46152\n",
            "46153\n",
            "46154\n",
            "46155\n",
            "46156\n",
            "46157\n",
            "46158\n",
            "46159\n",
            "46160\n",
            "46161\n",
            "46162\n",
            "46163\n",
            "46164\n",
            "46165\n",
            "46166\n",
            "46167\n",
            "46168\n",
            "46169\n",
            "46170\n",
            "46171\n",
            "46172\n",
            "46173\n",
            "46174\n",
            "46175\n",
            "46176\n",
            "46177\n",
            "46178\n",
            "46179\n",
            "46180\n",
            "46181\n",
            "46182\n",
            "46183\n",
            "46184\n",
            "46185\n",
            "46186\n",
            "46187\n",
            "46188\n",
            "46189\n",
            "46190\n",
            "46191\n",
            "46192\n",
            "46193\n",
            "46194\n",
            "46195\n",
            "46196\n",
            "46197\n",
            "46198\n",
            "46199\n",
            "46200\n",
            "46201\n",
            "46202\n",
            "46203\n",
            "46204\n",
            "46205\n",
            "46206\n",
            "46207\n",
            "46208\n",
            "46209\n",
            "46210\n",
            "46211\n",
            "46212\n",
            "46213\n",
            "46214\n",
            "46215\n",
            "46216\n",
            "46217\n",
            "46218\n",
            "46219\n",
            "46220\n",
            "46221\n",
            "46222\n",
            "46223\n",
            "46224\n",
            "46225\n",
            "46226\n",
            "46227\n",
            "46228\n",
            "46229\n",
            "46230\n",
            "46231\n",
            "46232\n",
            "46233\n",
            "46234\n",
            "46235\n",
            "46236\n",
            "46237\n",
            "46238\n",
            "46239\n",
            "46240\n",
            "46241\n",
            "46242\n",
            "46243\n",
            "46244\n",
            "46245\n",
            "46246\n",
            "46247\n",
            "46248\n",
            "46249\n",
            "46250\n",
            "46251\n",
            "46252\n",
            "46253\n",
            "46254\n",
            "46255\n",
            "46256\n",
            "46257\n",
            "46258\n",
            "46259\n",
            "46260\n",
            "46261\n",
            "46262\n",
            "46263\n",
            "46264\n",
            "46265\n",
            "46266\n",
            "46267\n",
            "46268\n",
            "46269\n",
            "46270\n",
            "46271\n",
            "46272\n",
            "46273\n",
            "46274\n",
            "46275\n",
            "46276\n",
            "46277\n",
            "46278\n",
            "46279\n",
            "46280\n",
            "46281\n",
            "46282\n",
            "46283\n",
            "46284\n",
            "46285\n",
            "46286\n",
            "46287\n",
            "46288\n",
            "46289\n",
            "46290\n",
            "46291\n",
            "46292\n",
            "46293\n",
            "46294\n",
            "46295\n",
            "46296\n",
            "46297\n",
            "46298\n",
            "46299\n",
            "46300\n",
            "46301\n",
            "46302\n",
            "46303\n",
            "46304\n",
            "46305\n",
            "46306\n",
            "46307\n",
            "46308\n",
            "46309\n",
            "46310\n",
            "46311\n",
            "46312\n",
            "46313\n",
            "46314\n",
            "46315\n",
            "46316\n",
            "46317\n",
            "46318\n",
            "46319\n",
            "46320\n",
            "46321\n",
            "46322\n",
            "46323\n",
            "46324\n",
            "46325\n",
            "46326\n",
            "46327\n",
            "46328\n",
            "46329\n",
            "46330\n",
            "46331\n",
            "46332\n",
            "46333\n",
            "46334\n",
            "46335\n",
            "46336\n",
            "46337\n",
            "46338\n",
            "46339\n",
            "46340\n",
            "46341\n",
            "46342\n",
            "46343\n",
            "46344\n",
            "46345\n",
            "46346\n",
            "46347\n",
            "46348\n",
            "46349\n",
            "46350\n",
            "46351\n",
            "46352\n",
            "46353\n",
            "46354\n",
            "46355\n",
            "46356\n",
            "46357\n",
            "46358\n",
            "46359\n",
            "46360\n",
            "46361\n",
            "46362\n",
            "46363\n",
            "46364\n",
            "46365\n",
            "46366\n",
            "46367\n",
            "46368\n",
            "46369\n",
            "46370\n",
            "46371\n",
            "46372\n",
            "46373\n",
            "46374\n",
            "46375\n",
            "46376\n",
            "46377\n",
            "46378\n",
            "46379\n",
            "46380\n",
            "46381\n",
            "46382\n",
            "46383\n",
            "46384\n",
            "46385\n",
            "46386\n",
            "46387\n",
            "46388\n",
            "46389\n",
            "46390\n",
            "46391\n",
            "46392\n",
            "46393\n",
            "46394\n",
            "46395\n",
            "46396\n",
            "46397\n",
            "46398\n",
            "46399\n",
            "46400\n",
            "46401\n",
            "46402\n",
            "46403\n",
            "46404\n",
            "46405\n",
            "46406\n",
            "46407\n",
            "46408\n",
            "46409\n",
            "46410\n",
            "46411\n",
            "46412\n",
            "46413\n",
            "46414\n",
            "46415\n",
            "46416\n",
            "46417\n",
            "46418\n",
            "46419\n",
            "46420\n",
            "46421\n",
            "46422\n",
            "46423\n",
            "46424\n",
            "46425\n",
            "46426\n",
            "46427\n",
            "46428\n",
            "46429\n",
            "46430\n",
            "46431\n",
            "46432\n",
            "46433\n",
            "46434\n",
            "46435\n",
            "46436\n",
            "46437\n",
            "46438\n",
            "46439\n",
            "46440\n",
            "46441\n",
            "46442\n",
            "46443\n",
            "46444\n",
            "46445\n",
            "46446\n",
            "46447\n",
            "46448\n",
            "46449\n",
            "46450\n",
            "46451\n",
            "46452\n",
            "46453\n",
            "46454\n",
            "46455\n",
            "46456\n",
            "46457\n",
            "46458\n",
            "46459\n",
            "46460\n",
            "46461\n",
            "46462\n",
            "46463\n",
            "46464\n",
            "46465\n",
            "46466\n",
            "46467\n",
            "46468\n",
            "46469\n",
            "46470\n",
            "46471\n",
            "46472\n",
            "46473\n",
            "46474\n",
            "46475\n",
            "46476\n",
            "46477\n",
            "46478\n",
            "46479\n",
            "46480\n",
            "46481\n",
            "46482\n",
            "46483\n",
            "46484\n",
            "46485\n",
            "46486\n",
            "46487\n",
            "46488\n",
            "46489\n",
            "46490\n",
            "46491\n",
            "46492\n",
            "46493\n",
            "46494\n",
            "46495\n",
            "46496\n",
            "46497\n",
            "46498\n",
            "46499\n",
            "46500\n",
            "46501\n",
            "46502\n",
            "46503\n",
            "46504\n",
            "46505\n",
            "46506\n",
            "46507\n",
            "46508\n",
            "46509\n",
            "46510\n",
            "46511\n",
            "46512\n",
            "46513\n",
            "46514\n",
            "46515\n",
            "46516\n",
            "46517\n",
            "46518\n",
            "46519\n",
            "46520\n",
            "46521\n",
            "46522\n",
            "46523\n",
            "46524\n",
            "46525\n",
            "46526\n",
            "46527\n",
            "46528\n",
            "46529\n",
            "46530\n",
            "46531\n",
            "46532\n",
            "46533\n",
            "46534\n",
            "46535\n",
            "46536\n",
            "46537\n",
            "46538\n",
            "46539\n",
            "46540\n",
            "46541\n",
            "46542\n",
            "46543\n",
            "46544\n",
            "46545\n",
            "46546\n",
            "46547\n",
            "46548\n",
            "46549\n",
            "46550\n",
            "46551\n",
            "46552\n",
            "46553\n",
            "46554\n",
            "46555\n",
            "46556\n",
            "46557\n",
            "46558\n",
            "46559\n",
            "46560\n",
            "46561\n",
            "46562\n",
            "46563\n",
            "46564\n",
            "46565\n",
            "46566\n",
            "46567\n",
            "46568\n",
            "46569\n",
            "46570\n",
            "46571\n",
            "46572\n",
            "46573\n",
            "46574\n",
            "46575\n",
            "46576\n",
            "46577\n",
            "46578\n",
            "46579\n",
            "46580\n",
            "46581\n",
            "46582\n",
            "46583\n",
            "46584\n",
            "46585\n",
            "46586\n",
            "46587\n",
            "46588\n",
            "46589\n",
            "46590\n",
            "46591\n",
            "46592\n",
            "46593\n",
            "46594\n",
            "46595\n",
            "46596\n",
            "46597\n",
            "46598\n",
            "46599\n",
            "46600\n",
            "46601\n",
            "46602\n",
            "46603\n",
            "46604\n",
            "46605\n",
            "46606\n",
            "46607\n",
            "46608\n",
            "46609\n",
            "46610\n",
            "46611\n",
            "46612\n",
            "46613\n",
            "46614\n",
            "46615\n",
            "46616\n",
            "46617\n",
            "46618\n",
            "46619\n",
            "46620\n",
            "46621\n",
            "46622\n",
            "46623\n",
            "46624\n",
            "46625\n",
            "46626\n",
            "46627\n",
            "46628\n",
            "46629\n",
            "46630\n",
            "46631\n",
            "46632\n",
            "46633\n",
            "46634\n",
            "46635\n",
            "46636\n",
            "46637\n",
            "46638\n",
            "46639\n",
            "46640\n",
            "46641\n",
            "46642\n",
            "46643\n",
            "46644\n",
            "46645\n",
            "46646\n",
            "46647\n",
            "46648\n",
            "46649\n",
            "46650\n",
            "46651\n",
            "46652\n",
            "46653\n",
            "46654\n",
            "46655\n",
            "46656\n",
            "46657\n",
            "46658\n",
            "46659\n",
            "46660\n",
            "46661\n",
            "46662\n",
            "46663\n",
            "46664\n",
            "46665\n",
            "46666\n",
            "46667\n",
            "46668\n",
            "46669\n",
            "46670\n",
            "46671\n",
            "46672\n",
            "46673\n",
            "46674\n",
            "46675\n",
            "46676\n",
            "46677\n",
            "46678\n",
            "46679\n",
            "46680\n",
            "46681\n",
            "46682\n",
            "46683\n",
            "46684\n",
            "46685\n",
            "46686\n",
            "46687\n",
            "46688\n",
            "46689\n",
            "46690\n",
            "46691\n",
            "46692\n",
            "46693\n",
            "46694\n",
            "46695\n",
            "46696\n",
            "46697\n",
            "46698\n",
            "46699\n",
            "46700\n",
            "46701\n",
            "46702\n",
            "46703\n",
            "46704\n",
            "46705\n",
            "46706\n",
            "46707\n",
            "46708\n",
            "46709\n",
            "46710\n",
            "46711\n",
            "46712\n",
            "46713\n",
            "46714\n",
            "46715\n",
            "46716\n",
            "46717\n",
            "46718\n",
            "46719\n",
            "46720\n",
            "46721\n",
            "46722\n",
            "46723\n",
            "46724\n",
            "46725\n",
            "46726\n",
            "46727\n",
            "46728\n",
            "46729\n",
            "46730\n",
            "46731\n",
            "46732\n",
            "46733\n",
            "46734\n",
            "46735\n",
            "46736\n",
            "46737\n",
            "46738\n",
            "46739\n",
            "46740\n",
            "46741\n",
            "46742\n",
            "46743\n",
            "46744\n",
            "46745\n",
            "46746\n",
            "46747\n",
            "46748\n",
            "46749\n",
            "46750\n",
            "46751\n",
            "46752\n",
            "46753\n",
            "46754\n",
            "46755\n",
            "46756\n",
            "46757\n",
            "46758\n",
            "46759\n",
            "46760\n",
            "46761\n",
            "46762\n",
            "46763\n",
            "46764\n",
            "46765\n",
            "46766\n",
            "46767\n",
            "46768\n",
            "46769\n",
            "46770\n",
            "46771\n",
            "46772\n",
            "46773\n",
            "46774\n",
            "46775\n",
            "46776\n",
            "46777\n",
            "46778\n",
            "46779\n",
            "46780\n",
            "46781\n",
            "46782\n",
            "46783\n",
            "46784\n",
            "46785\n",
            "46786\n",
            "46787\n",
            "46788\n",
            "46789\n",
            "46790\n",
            "46791\n",
            "46792\n",
            "46793\n",
            "46794\n",
            "46795\n",
            "46796\n",
            "46797\n",
            "46798\n",
            "46799\n",
            "46800\n",
            "46801\n",
            "46802\n",
            "46803\n",
            "46804\n",
            "46805\n",
            "46806\n",
            "46807\n",
            "46808\n",
            "46809\n",
            "46810\n",
            "46811\n",
            "46812\n",
            "46813\n",
            "46814\n",
            "46815\n",
            "46816\n",
            "46817\n",
            "46818\n",
            "46819\n",
            "46820\n",
            "46821\n",
            "46822\n",
            "46823\n",
            "46824\n",
            "46825\n",
            "46826\n",
            "46827\n",
            "46828\n",
            "46829\n",
            "46830\n",
            "46831\n",
            "46832\n",
            "46833\n",
            "46834\n",
            "46835\n",
            "46836\n",
            "46837\n",
            "46838\n",
            "46839\n",
            "46840\n",
            "46841\n",
            "46842\n",
            "46843\n",
            "46844\n",
            "46845\n",
            "46846\n",
            "46847\n",
            "46848\n",
            "46849\n",
            "46850\n",
            "46851\n",
            "46852\n",
            "46853\n",
            "46854\n",
            "46855\n",
            "46856\n",
            "46857\n",
            "46858\n",
            "46859\n",
            "46860\n",
            "46861\n",
            "46862\n",
            "46863\n",
            "46864\n",
            "46865\n",
            "46866\n",
            "46867\n",
            "46868\n",
            "46869\n",
            "46870\n",
            "46871\n",
            "46872\n",
            "46873\n",
            "46874\n",
            "46875\n",
            "46876\n",
            "46877\n",
            "46878\n",
            "46879\n",
            "46880\n",
            "46881\n",
            "46882\n",
            "46883\n",
            "46884\n",
            "46885\n",
            "46886\n",
            "46887\n",
            "46888\n",
            "46889\n",
            "46890\n",
            "46891\n",
            "46892\n",
            "46893\n",
            "46894\n",
            "46895\n",
            "46896\n",
            "46897\n",
            "46898\n",
            "46899\n",
            "46900\n",
            "46901\n",
            "46902\n",
            "46903\n",
            "46904\n",
            "46905\n",
            "46906\n",
            "46907\n",
            "46908\n",
            "46909\n",
            "46910\n",
            "46911\n",
            "46912\n",
            "46913\n",
            "46914\n",
            "46915\n",
            "46916\n",
            "46917\n",
            "46918\n",
            "46919\n",
            "46920\n",
            "46921\n",
            "46922\n",
            "46923\n",
            "46924\n",
            "46925\n",
            "46926\n",
            "46927\n",
            "46928\n",
            "46929\n",
            "46930\n",
            "46931\n",
            "46932\n",
            "46933\n",
            "46934\n",
            "46935\n",
            "46936\n",
            "46937\n",
            "46938\n",
            "46939\n",
            "46940\n",
            "46941\n",
            "46942\n",
            "46943\n",
            "46944\n",
            "46945\n",
            "46946\n",
            "46947\n",
            "46948\n",
            "46949\n",
            "46950\n",
            "46951\n",
            "46952\n",
            "46953\n",
            "46954\n",
            "46955\n",
            "46956\n",
            "46957\n",
            "46958\n",
            "46959\n",
            "46960\n",
            "46961\n",
            "46962\n",
            "46963\n",
            "46964\n",
            "46965\n",
            "46966\n",
            "46967\n",
            "46968\n",
            "46969\n",
            "46970\n",
            "46971\n",
            "46972\n",
            "46973\n",
            "46974\n",
            "46975\n",
            "46976\n",
            "46977\n",
            "46978\n",
            "46979\n",
            "46980\n",
            "46981\n",
            "46982\n",
            "46983\n",
            "46984\n",
            "46985\n",
            "46986\n",
            "46987\n",
            "46988\n",
            "46989\n",
            "46990\n",
            "46991\n",
            "46992\n",
            "46993\n",
            "46994\n",
            "46995\n",
            "46996\n",
            "46997\n",
            "46998\n",
            "46999\n",
            "47000\n",
            "47001\n",
            "47002\n",
            "47003\n",
            "47004\n",
            "47005\n",
            "47006\n",
            "47007\n",
            "47008\n",
            "47009\n",
            "47010\n",
            "47011\n",
            "47012\n",
            "47013\n",
            "47014\n",
            "47015\n",
            "47016\n",
            "47017\n",
            "47018\n",
            "47019\n",
            "47020\n",
            "47021\n",
            "47022\n",
            "47023\n",
            "47024\n",
            "47025\n",
            "47026\n",
            "47027\n",
            "47028\n",
            "47029\n",
            "47030\n",
            "47031\n",
            "47032\n",
            "47033\n",
            "47034\n",
            "47035\n",
            "47036\n",
            "47037\n",
            "47038\n",
            "47039\n",
            "47040\n",
            "47041\n",
            "47042\n",
            "47043\n",
            "47044\n",
            "47045\n",
            "47046\n",
            "47047\n",
            "47048\n",
            "47049\n",
            "47050\n",
            "47051\n",
            "47052\n",
            "47053\n",
            "47054\n",
            "47055\n",
            "47056\n",
            "47057\n",
            "47058\n",
            "47059\n",
            "47060\n",
            "47061\n",
            "47062\n",
            "47063\n",
            "47064\n",
            "47065\n",
            "47066\n",
            "47067\n",
            "47068\n",
            "47069\n",
            "47070\n",
            "47071\n",
            "47072\n",
            "47073\n",
            "47074\n",
            "47075\n",
            "47076\n",
            "47077\n",
            "47078\n",
            "47079\n",
            "47080\n",
            "47081\n",
            "47082\n",
            "47083\n",
            "47084\n",
            "47085\n",
            "47086\n",
            "47087\n",
            "47088\n",
            "47089\n",
            "47090\n",
            "47091\n",
            "47092\n",
            "47093\n",
            "47094\n",
            "47095\n",
            "47096\n",
            "47097\n",
            "47098\n",
            "47099\n",
            "47100\n",
            "47101\n",
            "47102\n",
            "47103\n",
            "47104\n",
            "47105\n",
            "47106\n",
            "47107\n",
            "47108\n",
            "47109\n",
            "47110\n",
            "47111\n",
            "47112\n",
            "47113\n",
            "47114\n",
            "47115\n",
            "47116\n",
            "47117\n",
            "47118\n",
            "47119\n",
            "47120\n",
            "47121\n",
            "47122\n",
            "47123\n",
            "47124\n",
            "47125\n",
            "47126\n",
            "47127\n",
            "47128\n",
            "47129\n",
            "47130\n",
            "47131\n",
            "47132\n",
            "47133\n",
            "47134\n",
            "47135\n",
            "47136\n",
            "47137\n",
            "47138\n",
            "47139\n",
            "47140\n",
            "47141\n",
            "47142\n",
            "47143\n",
            "47144\n",
            "47145\n",
            "47146\n",
            "47147\n",
            "47148\n",
            "47149\n",
            "47150\n",
            "47151\n",
            "47152\n",
            "47153\n",
            "47154\n",
            "47155\n",
            "47156\n",
            "47157\n",
            "47158\n",
            "47159\n",
            "47160\n",
            "47161\n",
            "47162\n",
            "47163\n",
            "47164\n",
            "47165\n",
            "47166\n",
            "47167\n",
            "47168\n",
            "47169\n",
            "47170\n",
            "47171\n",
            "47172\n",
            "47173\n",
            "47174\n",
            "47175\n",
            "47176\n",
            "47177\n",
            "47178\n",
            "47179\n",
            "47180\n",
            "47181\n",
            "47182\n",
            "47183\n",
            "47184\n",
            "47185\n",
            "47186\n",
            "47187\n",
            "47188\n",
            "47189\n",
            "47190\n",
            "47191\n",
            "47192\n",
            "47193\n",
            "47194\n",
            "47195\n",
            "47196\n",
            "47197\n",
            "47198\n",
            "47199\n",
            "47200\n",
            "47201\n",
            "47202\n",
            "47203\n",
            "47204\n",
            "47205\n",
            "47206\n",
            "47207\n",
            "47208\n",
            "47209\n",
            "47210\n",
            "47211\n",
            "47212\n",
            "47213\n",
            "47214\n",
            "47215\n",
            "47216\n",
            "47217\n",
            "47218\n",
            "47219\n",
            "47220\n",
            "47221\n",
            "47222\n",
            "47223\n",
            "47224\n",
            "47225\n",
            "47226\n",
            "47227\n",
            "47228\n",
            "47229\n",
            "47230\n",
            "47231\n",
            "47232\n",
            "47233\n",
            "47234\n",
            "47235\n",
            "47236\n",
            "47237\n",
            "47238\n",
            "47239\n",
            "47240\n",
            "47241\n",
            "47242\n",
            "47243\n",
            "47244\n",
            "47245\n",
            "47246\n",
            "47247\n",
            "47248\n",
            "47249\n",
            "47250\n",
            "47251\n",
            "47252\n",
            "47253\n",
            "47254\n",
            "47255\n",
            "47256\n",
            "47257\n",
            "47258\n",
            "47259\n",
            "47260\n",
            "47261\n",
            "47262\n",
            "47263\n",
            "47264\n",
            "47265\n",
            "47266\n",
            "47267\n",
            "47268\n",
            "47269\n",
            "47270\n",
            "47271\n",
            "47272\n",
            "47273\n",
            "47274\n",
            "47275\n",
            "47276\n",
            "47277\n",
            "47278\n",
            "47279\n",
            "47280\n",
            "47281\n",
            "47282\n",
            "47283\n",
            "47284\n",
            "47285\n",
            "47286\n",
            "47287\n",
            "47288\n",
            "47289\n",
            "47290\n",
            "47291\n",
            "47292\n",
            "47293\n",
            "47294\n",
            "47295\n",
            "47296\n",
            "47297\n",
            "47298\n",
            "47299\n",
            "47300\n",
            "47301\n",
            "47302\n",
            "47303\n",
            "47304\n",
            "47305\n",
            "47306\n",
            "47307\n",
            "47308\n",
            "47309\n",
            "47310\n",
            "47311\n",
            "47312\n",
            "47313\n",
            "47314\n",
            "47315\n",
            "47316\n",
            "47317\n",
            "47318\n",
            "47319\n",
            "47320\n",
            "47321\n",
            "47322\n",
            "47323\n",
            "47324\n",
            "47325\n",
            "47326\n",
            "47327\n",
            "47328\n",
            "47329\n",
            "47330\n",
            "47331\n",
            "47332\n",
            "47333\n",
            "47334\n",
            "47335\n",
            "47336\n",
            "47337\n",
            "47338\n",
            "47339\n",
            "47340\n",
            "47341\n",
            "47342\n",
            "47343\n",
            "47344\n",
            "47345\n",
            "47346\n",
            "47347\n",
            "47348\n",
            "47349\n",
            "47350\n",
            "47351\n",
            "47352\n",
            "47353\n",
            "47354\n",
            "47355\n",
            "47356\n",
            "47357\n",
            "47358\n",
            "47359\n",
            "47360\n",
            "47361\n",
            "47362\n",
            "47363\n",
            "47364\n",
            "47365\n",
            "47366\n",
            "47367\n",
            "47368\n",
            "47369\n",
            "47370\n",
            "47371\n",
            "47372\n",
            "47373\n",
            "47374\n",
            "47375\n",
            "47376\n",
            "47377\n",
            "47378\n",
            "47379\n",
            "47380\n",
            "47381\n",
            "47382\n",
            "47383\n",
            "47384\n",
            "47385\n",
            "47386\n",
            "47387\n",
            "47388\n",
            "47389\n",
            "47390\n",
            "47391\n",
            "47392\n",
            "47393\n",
            "47394\n",
            "47395\n",
            "47396\n",
            "47397\n",
            "47398\n",
            "47399\n",
            "47400\n",
            "47401\n",
            "47402\n",
            "47403\n",
            "47404\n",
            "47405\n",
            "47406\n",
            "47407\n",
            "47408\n",
            "47409\n",
            "47410\n",
            "47411\n",
            "47412\n",
            "47413\n",
            "47414\n",
            "47415\n",
            "47416\n",
            "47417\n",
            "47418\n",
            "47419\n",
            "47420\n",
            "47421\n",
            "47422\n",
            "47423\n",
            "47424\n",
            "47425\n",
            "47426\n",
            "47427\n",
            "47428\n",
            "47429\n",
            "47430\n",
            "47431\n",
            "47432\n",
            "47433\n",
            "47434\n",
            "47435\n",
            "47436\n",
            "47437\n",
            "47438\n",
            "47439\n",
            "47440\n",
            "47441\n",
            "47442\n",
            "47443\n",
            "47444\n",
            "47445\n",
            "47446\n",
            "47447\n",
            "47448\n",
            "47449\n",
            "47450\n",
            "47451\n",
            "47452\n",
            "47453\n",
            "47454\n",
            "47455\n",
            "47456\n",
            "47457\n",
            "47458\n",
            "47459\n",
            "47460\n",
            "47461\n",
            "47462\n",
            "47463\n",
            "47464\n",
            "47465\n",
            "47466\n",
            "47467\n",
            "47468\n",
            "47469\n",
            "47470\n",
            "47471\n",
            "47472\n",
            "47473\n",
            "47474\n",
            "47475\n",
            "47476\n",
            "47477\n",
            "47478\n",
            "47479\n",
            "47480\n",
            "47481\n",
            "47482\n",
            "47483\n",
            "47484\n",
            "47485\n",
            "47486\n",
            "47487\n",
            "47488\n",
            "47489\n",
            "47490\n",
            "47491\n",
            "47492\n",
            "47493\n",
            "47494\n",
            "47495\n",
            "47496\n",
            "47497\n",
            "47498\n",
            "47499\n",
            "47500\n",
            "47501\n",
            "47502\n",
            "47503\n",
            "47504\n",
            "47505\n",
            "47506\n",
            "47507\n",
            "47508\n",
            "47509\n",
            "47510\n",
            "47511\n",
            "47512\n",
            "47513\n",
            "47514\n",
            "47515\n",
            "47516\n",
            "47517\n",
            "47518\n",
            "47519\n",
            "47520\n",
            "47521\n",
            "47522\n",
            "47523\n",
            "47524\n",
            "47525\n",
            "47526\n",
            "47527\n",
            "47528\n",
            "47529\n",
            "47530\n",
            "47531\n",
            "47532\n",
            "47533\n",
            "47534\n",
            "47535\n",
            "47536\n",
            "47537\n",
            "47538\n",
            "47539\n",
            "47540\n",
            "47541\n",
            "47542\n",
            "47543\n",
            "47544\n",
            "47545\n",
            "47546\n",
            "47547\n",
            "47548\n",
            "47549\n",
            "47550\n",
            "47551\n",
            "47552\n",
            "47553\n",
            "47554\n",
            "47555\n",
            "47556\n",
            "47557\n",
            "47558\n",
            "47559\n",
            "47560\n",
            "47561\n",
            "47562\n",
            "47563\n",
            "47564\n",
            "47565\n",
            "47566\n",
            "47567\n",
            "47568\n",
            "47569\n",
            "47570\n",
            "47571\n",
            "47572\n",
            "47573\n",
            "47574\n",
            "47575\n",
            "47576\n",
            "47577\n",
            "47578\n",
            "47579\n",
            "47580\n",
            "47581\n",
            "47582\n",
            "47583\n",
            "47584\n",
            "47585\n",
            "47586\n",
            "47587\n",
            "47588\n",
            "47589\n",
            "47590\n",
            "47591\n",
            "47592\n",
            "47593\n",
            "47594\n",
            "47595\n",
            "47596\n",
            "47597\n",
            "47598\n",
            "47599\n",
            "47600\n",
            "47601\n",
            "47602\n",
            "47603\n",
            "47604\n",
            "47605\n",
            "47606\n",
            "47607\n",
            "47608\n",
            "47609\n",
            "47610\n",
            "47611\n",
            "47612\n",
            "47613\n",
            "47614\n",
            "47615\n",
            "47616\n",
            "47617\n",
            "47618\n",
            "47619\n",
            "47620\n",
            "47621\n",
            "47622\n",
            "47623\n",
            "47624\n",
            "47625\n",
            "47626\n",
            "47627\n",
            "47628\n",
            "47629\n",
            "47630\n",
            "47631\n",
            "47632\n",
            "47633\n",
            "47634\n",
            "47635\n",
            "47636\n",
            "47637\n",
            "47638\n",
            "47639\n",
            "47640\n",
            "47641\n",
            "47642\n",
            "47643\n",
            "47644\n",
            "47645\n",
            "47646\n",
            "47647\n",
            "47648\n",
            "47649\n",
            "47650\n",
            "47651\n",
            "47652\n",
            "47653\n",
            "47654\n",
            "47655\n",
            "47656\n",
            "47657\n",
            "47658\n",
            "47659\n",
            "47660\n",
            "47661\n",
            "47662\n",
            "47663\n",
            "47664\n",
            "47665\n",
            "47666\n",
            "47667\n",
            "47668\n",
            "47669\n",
            "47670\n",
            "47671\n",
            "47672\n",
            "47673\n",
            "47674\n",
            "47675\n",
            "47676\n",
            "47677\n",
            "47678\n",
            "47679\n",
            "47680\n",
            "47681\n",
            "47682\n",
            "47683\n",
            "47684\n",
            "47685\n",
            "47686\n",
            "47687\n",
            "47688\n",
            "47689\n",
            "47690\n",
            "47691\n",
            "47692\n",
            "47693\n",
            "47694\n",
            "47695\n",
            "47696\n",
            "47697\n",
            "47698\n",
            "47699\n",
            "47700\n",
            "47701\n",
            "47702\n",
            "47703\n",
            "47704\n",
            "47705\n",
            "47706\n",
            "47707\n",
            "47708\n",
            "47709\n",
            "47710\n",
            "47711\n",
            "47712\n",
            "47713\n",
            "47714\n",
            "47715\n",
            "47716\n",
            "47717\n",
            "47718\n",
            "47719\n",
            "47720\n",
            "47721\n",
            "47722\n",
            "47723\n",
            "47724\n",
            "47725\n",
            "47726\n",
            "47727\n",
            "47728\n",
            "47729\n",
            "47730\n",
            "47731\n",
            "47732\n",
            "47733\n",
            "47734\n",
            "47735\n",
            "47736\n",
            "47737\n",
            "47738\n",
            "47739\n",
            "47740\n",
            "47741\n",
            "47742\n",
            "47743\n",
            "47744\n",
            "47745\n",
            "47746\n",
            "47747\n",
            "47748\n",
            "47749\n",
            "47750\n",
            "47751\n",
            "47752\n",
            "47753\n",
            "47754\n",
            "47755\n",
            "47756\n",
            "47757\n",
            "47758\n",
            "47759\n",
            "47760\n",
            "47761\n",
            "47762\n",
            "47763\n",
            "47764\n",
            "47765\n",
            "47766\n",
            "47767\n",
            "47768\n",
            "47769\n",
            "47770\n",
            "47771\n",
            "47772\n",
            "47773\n",
            "47774\n",
            "47775\n",
            "47776\n",
            "47777\n",
            "47778\n",
            "47779\n",
            "47780\n",
            "47781\n",
            "47782\n",
            "47783\n",
            "47784\n",
            "47785\n",
            "47786\n",
            "47787\n",
            "47788\n",
            "47789\n",
            "47790\n",
            "47791\n",
            "47792\n",
            "47793\n",
            "47794\n",
            "47795\n",
            "47796\n",
            "47797\n",
            "47798\n",
            "47799\n",
            "47800\n",
            "47801\n",
            "47802\n",
            "47803\n",
            "47804\n",
            "47805\n",
            "47806\n",
            "47807\n",
            "47808\n",
            "47809\n",
            "47810\n",
            "47811\n",
            "47812\n",
            "47813\n",
            "47814\n",
            "47815\n",
            "47816\n",
            "47817\n",
            "47818\n",
            "47819\n",
            "47820\n",
            "47821\n",
            "47822\n",
            "47823\n",
            "47824\n",
            "47825\n",
            "47826\n",
            "47827\n",
            "47828\n",
            "47829\n",
            "47830\n",
            "47831\n",
            "47832\n",
            "47833\n",
            "47834\n",
            "47835\n",
            "47836\n",
            "47837\n",
            "47838\n",
            "47839\n",
            "47840\n",
            "47841\n",
            "47842\n",
            "47843\n",
            "47844\n",
            "47845\n",
            "47846\n",
            "47847\n",
            "47848\n",
            "47849\n",
            "47850\n",
            "47851\n",
            "47852\n",
            "47853\n",
            "47854\n",
            "47855\n",
            "47856\n",
            "47857\n",
            "47858\n",
            "47859\n",
            "47860\n",
            "47861\n",
            "47862\n",
            "47863\n",
            "47864\n",
            "47865\n",
            "47866\n",
            "47867\n",
            "47868\n",
            "47869\n",
            "47870\n",
            "47871\n",
            "47872\n",
            "47873\n",
            "47874\n",
            "47875\n",
            "47876\n",
            "47877\n",
            "47878\n",
            "47879\n",
            "47880\n",
            "47881\n",
            "47882\n",
            "47883\n",
            "47884\n",
            "47885\n",
            "47886\n",
            "47887\n",
            "47888\n",
            "47889\n",
            "47890\n",
            "47891\n",
            "47892\n",
            "47893\n",
            "47894\n",
            "47895\n",
            "47896\n",
            "47897\n",
            "47898\n",
            "47899\n",
            "47900\n",
            "47901\n",
            "47902\n",
            "47903\n",
            "47904\n",
            "47905\n",
            "47906\n",
            "47907\n",
            "47908\n",
            "47909\n",
            "47910\n",
            "47911\n",
            "47912\n",
            "47913\n",
            "47914\n",
            "47915\n",
            "47916\n",
            "47917\n",
            "47918\n",
            "47919\n",
            "47920\n",
            "47921\n",
            "47922\n",
            "47923\n",
            "47924\n",
            "47925\n",
            "47926\n",
            "47927\n",
            "47928\n",
            "47929\n",
            "47930\n",
            "47931\n",
            "47932\n",
            "47933\n",
            "47934\n",
            "47935\n",
            "47936\n",
            "47937\n",
            "47938\n",
            "47939\n",
            "47940\n",
            "47941\n",
            "47942\n",
            "47943\n",
            "47944\n",
            "47945\n",
            "47946\n",
            "47947\n",
            "47948\n",
            "47949\n",
            "47950\n",
            "47951\n",
            "47952\n",
            "47953\n",
            "47954\n",
            "47955\n",
            "47956\n",
            "47957\n",
            "47958\n",
            "47959\n",
            "47960\n",
            "47961\n",
            "47962\n",
            "47963\n",
            "47964\n",
            "47965\n",
            "47966\n",
            "47967\n",
            "47968\n",
            "47969\n",
            "47970\n",
            "47971\n",
            "47972\n",
            "47973\n",
            "47974\n",
            "47975\n",
            "47976\n",
            "47977\n",
            "47978\n",
            "47979\n",
            "47980\n",
            "47981\n",
            "47982\n",
            "47983\n",
            "47984\n",
            "47985\n",
            "47986\n",
            "47987\n",
            "47988\n",
            "47989\n",
            "47990\n",
            "47991\n",
            "47992\n",
            "47993\n",
            "47994\n",
            "47995\n",
            "47996\n",
            "47997\n",
            "47998\n",
            "47999\n",
            "48000\n",
            "48001\n",
            "48002\n",
            "48003\n",
            "48004\n",
            "48005\n",
            "48006\n",
            "48007\n",
            "48008\n",
            "48009\n",
            "48010\n",
            "48011\n",
            "48012\n",
            "48013\n",
            "48014\n",
            "48015\n",
            "48016\n",
            "48017\n",
            "48018\n",
            "48019\n",
            "48020\n",
            "48021\n",
            "48022\n",
            "48023\n",
            "48024\n",
            "48025\n",
            "48026\n",
            "48027\n",
            "48028\n",
            "48029\n",
            "48030\n",
            "48031\n",
            "48032\n",
            "48033\n",
            "48034\n",
            "48035\n",
            "48036\n",
            "48037\n",
            "48038\n",
            "48039\n",
            "48040\n",
            "48041\n",
            "48042\n",
            "48043\n",
            "48044\n",
            "48045\n",
            "48046\n",
            "48047\n",
            "48048\n",
            "48049\n",
            "48050\n",
            "48051\n",
            "48052\n",
            "48053\n",
            "48054\n",
            "48055\n",
            "48056\n",
            "48057\n",
            "48058\n",
            "48059\n",
            "48060\n",
            "48061\n",
            "48062\n",
            "48063\n",
            "48064\n",
            "48065\n",
            "48066\n",
            "48067\n",
            "48068\n",
            "48069\n",
            "48070\n",
            "48071\n",
            "48072\n",
            "48073\n",
            "48074\n",
            "48075\n",
            "48076\n",
            "48077\n",
            "48078\n",
            "48079\n",
            "48080\n",
            "48081\n",
            "48082\n",
            "48083\n",
            "48084\n",
            "48085\n",
            "48086\n",
            "48087\n",
            "48088\n",
            "48089\n",
            "48090\n",
            "48091\n",
            "48092\n",
            "48093\n",
            "48094\n",
            "48095\n",
            "48096\n",
            "48097\n",
            "48098\n",
            "48099\n",
            "48100\n",
            "48101\n",
            "48102\n",
            "48103\n",
            "48104\n",
            "48105\n",
            "48106\n",
            "48107\n",
            "48108\n",
            "48109\n",
            "48110\n",
            "48111\n",
            "48112\n",
            "48113\n",
            "48114\n",
            "48115\n",
            "48116\n",
            "48117\n",
            "48118\n",
            "48119\n",
            "48120\n",
            "48121\n",
            "48122\n",
            "48123\n",
            "48124\n",
            "48125\n",
            "48126\n",
            "48127\n",
            "48128\n",
            "48129\n",
            "48130\n",
            "48131\n",
            "48132\n",
            "48133\n",
            "48134\n",
            "48135\n",
            "48136\n",
            "48137\n",
            "48138\n",
            "48139\n",
            "48140\n",
            "48141\n",
            "48142\n",
            "48143\n",
            "48144\n",
            "48145\n",
            "48146\n",
            "48147\n",
            "48148\n",
            "48149\n",
            "48150\n",
            "48151\n",
            "48152\n",
            "48153\n",
            "48154\n",
            "48155\n",
            "48156\n",
            "48157\n",
            "48158\n",
            "48159\n",
            "48160\n",
            "48161\n",
            "48162\n",
            "48163\n",
            "48164\n",
            "48165\n",
            "48166\n",
            "48167\n",
            "48168\n",
            "48169\n",
            "48170\n",
            "48171\n",
            "48172\n",
            "48173\n",
            "48174\n",
            "48175\n",
            "48176\n",
            "48177\n",
            "48178\n",
            "48179\n",
            "48180\n",
            "48181\n",
            "48182\n",
            "48183\n",
            "48184\n",
            "48185\n",
            "48186\n",
            "48187\n",
            "48188\n",
            "48189\n",
            "48190\n",
            "48191\n",
            "48192\n",
            "48193\n",
            "48194\n",
            "48195\n",
            "48196\n",
            "48197\n",
            "48198\n",
            "48199\n",
            "48200\n",
            "48201\n",
            "48202\n",
            "48203\n",
            "48204\n",
            "48205\n",
            "48206\n",
            "48207\n",
            "48208\n",
            "48209\n",
            "48210\n",
            "48211\n",
            "48212\n",
            "48213\n",
            "48214\n",
            "48215\n",
            "48216\n",
            "48217\n",
            "48218\n",
            "48219\n",
            "48220\n",
            "48221\n",
            "48222\n",
            "48223\n",
            "48224\n",
            "48225\n",
            "48226\n",
            "48227\n",
            "48228\n",
            "48229\n",
            "48230\n",
            "48231\n",
            "48232\n",
            "48233\n",
            "48234\n",
            "48235\n",
            "48236\n",
            "48237\n",
            "48238\n",
            "48239\n",
            "48240\n",
            "48241\n",
            "48242\n",
            "48243\n",
            "48244\n",
            "48245\n",
            "48246\n",
            "48247\n",
            "48248\n",
            "48249\n",
            "48250\n",
            "48251\n",
            "48252\n",
            "48253\n",
            "48254\n",
            "48255\n",
            "48256\n",
            "48257\n",
            "48258\n",
            "48259\n",
            "48260\n",
            "48261\n",
            "48262\n",
            "48263\n",
            "48264\n",
            "48265\n",
            "48266\n",
            "48267\n",
            "48268\n",
            "48269\n",
            "48270\n",
            "48271\n",
            "48272\n",
            "48273\n",
            "48274\n",
            "48275\n",
            "48276\n",
            "48277\n",
            "48278\n",
            "48279\n",
            "48280\n",
            "48281\n",
            "48282\n",
            "48283\n",
            "48284\n",
            "48285\n",
            "48286\n",
            "48287\n",
            "48288\n",
            "48289\n",
            "48290\n",
            "48291\n",
            "48292\n",
            "48293\n",
            "48294\n",
            "48295\n",
            "48296\n",
            "48297\n",
            "48298\n",
            "48299\n",
            "48300\n",
            "48301\n",
            "48302\n",
            "48303\n",
            "48304\n",
            "48305\n",
            "48306\n",
            "48307\n",
            "48308\n",
            "48309\n",
            "48310\n",
            "48311\n",
            "48312\n",
            "48313\n",
            "48314\n",
            "48315\n",
            "48316\n",
            "48317\n",
            "48318\n",
            "48319\n",
            "48320\n",
            "48321\n",
            "48322\n",
            "48323\n",
            "48324\n",
            "48325\n",
            "48326\n",
            "48327\n",
            "48328\n",
            "48329\n",
            "48330\n",
            "48331\n",
            "48332\n",
            "48333\n",
            "48334\n",
            "48335\n",
            "48336\n",
            "48337\n",
            "48338\n",
            "48339\n",
            "48340\n",
            "48341\n",
            "48342\n",
            "48343\n",
            "48344\n",
            "48345\n",
            "48346\n",
            "48347\n",
            "48348\n",
            "48349\n",
            "48350\n",
            "48351\n",
            "48352\n",
            "48353\n",
            "48354\n",
            "48355\n",
            "48356\n",
            "48357\n",
            "48358\n",
            "48359\n",
            "48360\n",
            "48361\n",
            "48362\n",
            "48363\n",
            "48364\n",
            "48365\n",
            "48366\n",
            "48367\n",
            "48368\n",
            "48369\n",
            "48370\n",
            "48371\n",
            "48372\n",
            "48373\n",
            "48374\n",
            "48375\n",
            "48376\n",
            "48377\n",
            "48378\n",
            "48379\n",
            "48380\n",
            "48381\n",
            "48382\n",
            "48383\n",
            "48384\n",
            "48385\n",
            "48386\n",
            "48387\n",
            "48388\n",
            "48389\n",
            "48390\n",
            "48391\n",
            "48392\n",
            "48393\n",
            "48394\n",
            "48395\n",
            "48396\n",
            "48397\n",
            "48398\n",
            "48399\n",
            "48400\n",
            "48401\n",
            "48402\n",
            "48403\n",
            "48404\n",
            "48405\n",
            "48406\n",
            "48407\n",
            "48408\n",
            "48409\n",
            "48410\n",
            "48411\n",
            "48412\n",
            "48413\n",
            "48414\n",
            "48415\n",
            "48416\n",
            "48417\n",
            "48418\n",
            "48419\n",
            "48420\n",
            "48421\n",
            "48422\n",
            "48423\n",
            "48424\n",
            "48425\n",
            "48426\n",
            "48427\n",
            "48428\n",
            "48429\n",
            "48430\n",
            "48431\n",
            "48432\n",
            "48433\n",
            "48434\n",
            "48435\n",
            "48436\n",
            "48437\n",
            "48438\n",
            "48439\n",
            "48440\n",
            "48441\n",
            "48442\n",
            "48443\n",
            "48444\n",
            "48445\n",
            "48446\n",
            "48447\n",
            "48448\n",
            "48449\n",
            "48450\n",
            "48451\n",
            "48452\n",
            "48453\n",
            "48454\n",
            "48455\n",
            "48456\n",
            "48457\n",
            "48458\n",
            "48459\n",
            "48460\n",
            "48461\n",
            "48462\n",
            "48463\n",
            "48464\n",
            "48465\n",
            "48466\n",
            "48467\n",
            "48468\n",
            "48469\n",
            "48470\n",
            "48471\n",
            "48472\n",
            "48473\n",
            "48474\n",
            "48475\n",
            "48476\n",
            "48477\n",
            "48478\n",
            "48479\n",
            "48480\n",
            "48481\n",
            "48482\n",
            "48483\n",
            "48484\n",
            "48485\n",
            "48486\n",
            "48487\n",
            "48488\n",
            "48489\n",
            "48490\n",
            "48491\n",
            "48492\n",
            "48493\n",
            "48494\n",
            "48495\n",
            "48496\n",
            "48497\n",
            "48498\n",
            "48499\n",
            "48500\n",
            "48501\n",
            "48502\n",
            "48503\n",
            "48504\n",
            "48505\n",
            "48506\n",
            "48507\n",
            "48508\n",
            "48509\n",
            "48510\n",
            "48511\n",
            "48512\n",
            "48513\n",
            "48514\n",
            "48515\n",
            "48516\n",
            "48517\n",
            "48518\n",
            "48519\n",
            "48520\n",
            "48521\n",
            "48522\n",
            "48523\n",
            "48524\n",
            "48525\n",
            "48526\n",
            "48527\n",
            "48528\n",
            "48529\n",
            "48530\n",
            "48531\n",
            "48532\n",
            "48533\n",
            "48534\n",
            "48535\n",
            "48536\n",
            "48537\n",
            "48538\n",
            "48539\n",
            "48540\n",
            "48541\n",
            "48542\n",
            "48543\n",
            "48544\n",
            "48545\n",
            "48546\n",
            "48547\n",
            "48548\n",
            "48549\n",
            "48550\n",
            "48551\n",
            "48552\n",
            "48553\n",
            "48554\n",
            "48555\n",
            "48556\n",
            "48557\n",
            "48558\n",
            "48559\n",
            "48560\n",
            "48561\n",
            "48562\n",
            "48563\n",
            "48564\n",
            "48565\n",
            "48566\n",
            "48567\n",
            "48568\n",
            "48569\n",
            "48570\n",
            "48571\n",
            "48572\n",
            "48573\n",
            "48574\n",
            "48575\n",
            "48576\n",
            "48577\n",
            "48578\n",
            "48579\n",
            "48580\n",
            "48581\n",
            "48582\n",
            "48583\n",
            "48584\n",
            "48585\n",
            "48586\n",
            "48587\n",
            "48588\n",
            "48589\n",
            "48590\n",
            "48591\n",
            "48592\n",
            "48593\n",
            "48594\n",
            "48595\n",
            "48596\n",
            "48597\n",
            "48598\n",
            "48599\n",
            "48600\n",
            "48601\n",
            "48602\n",
            "48603\n",
            "48604\n",
            "48605\n",
            "48606\n",
            "48607\n",
            "48608\n",
            "48609\n",
            "48610\n",
            "48611\n",
            "48612\n",
            "48613\n",
            "48614\n",
            "48615\n",
            "48616\n",
            "48617\n",
            "48618\n",
            "48619\n",
            "48620\n",
            "48621\n",
            "48622\n",
            "48623\n",
            "48624\n",
            "48625\n",
            "48626\n",
            "48627\n",
            "48628\n",
            "48629\n",
            "48630\n",
            "48631\n",
            "48632\n",
            "48633\n",
            "48634\n",
            "48635\n",
            "48636\n",
            "48637\n",
            "48638\n",
            "48639\n",
            "48640\n",
            "48641\n",
            "48642\n",
            "48643\n",
            "48644\n",
            "48645\n",
            "48646\n",
            "48647\n",
            "48648\n",
            "48649\n",
            "48650\n",
            "48651\n",
            "48652\n",
            "48653\n",
            "48654\n",
            "48655\n",
            "48656\n",
            "48657\n",
            "48658\n",
            "48659\n",
            "48660\n",
            "48661\n",
            "48662\n",
            "48663\n",
            "48664\n",
            "48665\n",
            "48666\n",
            "48667\n",
            "48668\n",
            "48669\n",
            "48670\n",
            "48671\n",
            "48672\n",
            "48673\n",
            "48674\n",
            "48675\n",
            "48676\n",
            "48677\n",
            "48678\n",
            "48679\n",
            "48680\n",
            "48681\n",
            "48682\n",
            "48683\n",
            "48684\n",
            "48685\n",
            "48686\n",
            "48687\n",
            "48688\n",
            "48689\n",
            "48690\n",
            "48691\n",
            "48692\n",
            "48693\n",
            "48694\n",
            "48695\n",
            "48696\n",
            "48697\n",
            "48698\n",
            "48699\n",
            "48700\n",
            "48701\n",
            "48702\n",
            "48703\n",
            "48704\n",
            "48705\n",
            "48706\n",
            "48707\n",
            "48708\n",
            "48709\n",
            "48710\n",
            "48711\n",
            "48712\n",
            "48713\n",
            "48714\n",
            "48715\n",
            "48716\n",
            "48717\n",
            "48718\n",
            "48719\n",
            "48720\n",
            "48721\n",
            "48722\n",
            "48723\n",
            "48724\n",
            "48725\n",
            "48726\n",
            "48727\n",
            "48728\n",
            "48729\n",
            "48730\n",
            "48731\n",
            "48732\n",
            "48733\n",
            "48734\n",
            "48735\n",
            "48736\n",
            "48737\n",
            "48738\n",
            "48739\n",
            "48740\n",
            "48741\n",
            "48742\n",
            "48743\n",
            "48744\n",
            "48745\n",
            "48746\n",
            "48747\n",
            "48748\n",
            "48749\n",
            "48750\n",
            "48751\n",
            "48752\n",
            "48753\n",
            "48754\n",
            "48755\n",
            "48756\n",
            "48757\n",
            "48758\n",
            "48759\n",
            "48760\n",
            "48761\n",
            "48762\n",
            "48763\n",
            "48764\n",
            "48765\n",
            "48766\n",
            "48767\n",
            "48768\n",
            "48769\n",
            "48770\n",
            "48771\n",
            "48772\n",
            "48773\n",
            "48774\n",
            "48775\n",
            "48776\n",
            "48777\n",
            "48778\n",
            "48779\n",
            "48780\n",
            "48781\n",
            "48782\n",
            "48783\n",
            "48784\n",
            "48785\n",
            "48786\n",
            "48787\n",
            "48788\n",
            "48789\n",
            "48790\n",
            "48791\n",
            "48792\n",
            "48793\n",
            "48794\n",
            "48795\n",
            "48796\n",
            "48797\n",
            "48798\n",
            "48799\n",
            "48800\n",
            "48801\n",
            "48802\n",
            "48803\n",
            "48804\n",
            "48805\n",
            "48806\n",
            "48807\n",
            "48808\n",
            "48809\n",
            "48810\n",
            "48811\n",
            "48812\n",
            "48813\n",
            "48814\n",
            "48815\n",
            "48816\n",
            "48817\n",
            "48818\n",
            "48819\n",
            "48820\n",
            "48821\n",
            "48822\n",
            "48823\n",
            "48824\n",
            "48825\n",
            "48826\n",
            "48827\n",
            "48828\n",
            "48829\n",
            "48830\n",
            "48831\n",
            "48832\n",
            "48833\n",
            "48834\n",
            "48835\n",
            "48836\n",
            "48837\n",
            "48838\n",
            "48839\n",
            "48840\n",
            "48841\n",
            "48842\n",
            "48843\n",
            "48844\n",
            "48845\n",
            "48846\n",
            "48847\n",
            "48848\n",
            "48849\n",
            "48850\n",
            "48851\n",
            "48852\n",
            "48853\n",
            "48854\n",
            "48855\n",
            "48856\n",
            "48857\n",
            "48858\n",
            "48859\n",
            "48860\n",
            "48861\n",
            "48862\n",
            "48863\n",
            "48864\n",
            "48865\n",
            "48866\n",
            "48867\n",
            "48868\n",
            "48869\n",
            "48870\n",
            "48871\n",
            "48872\n",
            "48873\n",
            "48874\n",
            "48875\n",
            "48876\n",
            "48877\n",
            "48878\n",
            "48879\n",
            "48880\n",
            "48881\n",
            "48882\n",
            "48883\n",
            "48884\n",
            "48885\n",
            "48886\n",
            "48887\n",
            "48888\n",
            "48889\n",
            "48890\n",
            "48891\n",
            "48892\n",
            "48893\n",
            "48894\n",
            "48895\n",
            "48896\n",
            "48897\n",
            "48898\n",
            "48899\n",
            "48900\n",
            "48901\n",
            "48902\n",
            "48903\n",
            "48904\n",
            "48905\n",
            "48906\n",
            "48907\n",
            "48908\n",
            "48909\n",
            "48910\n",
            "48911\n",
            "48912\n",
            "48913\n",
            "48914\n",
            "48915\n",
            "48916\n",
            "48917\n",
            "48918\n",
            "48919\n",
            "48920\n",
            "48921\n",
            "48922\n",
            "48923\n",
            "48924\n",
            "48925\n",
            "48926\n",
            "48927\n",
            "48928\n",
            "48929\n",
            "48930\n",
            "48931\n",
            "48932\n",
            "48933\n",
            "48934\n",
            "48935\n",
            "48936\n",
            "48937\n",
            "48938\n",
            "48939\n",
            "48940\n",
            "48941\n",
            "48942\n",
            "48943\n",
            "48944\n",
            "48945\n",
            "48946\n",
            "48947\n",
            "48948\n",
            "48949\n",
            "48950\n",
            "48951\n",
            "48952\n",
            "48953\n",
            "48954\n",
            "48955\n",
            "48956\n",
            "48957\n",
            "48958\n",
            "48959\n",
            "48960\n",
            "48961\n",
            "48962\n",
            "48963\n",
            "48964\n",
            "48965\n",
            "48966\n",
            "48967\n",
            "48968\n",
            "48969\n",
            "48970\n",
            "48971\n",
            "48972\n",
            "48973\n",
            "48974\n",
            "48975\n",
            "48976\n",
            "48977\n",
            "48978\n",
            "48979\n",
            "48980\n",
            "48981\n",
            "48982\n",
            "48983\n",
            "48984\n",
            "48985\n",
            "48986\n",
            "48987\n",
            "48988\n",
            "48989\n",
            "48990\n",
            "48991\n",
            "48992\n",
            "48993\n",
            "48994\n",
            "48995\n",
            "48996\n",
            "48997\n",
            "48998\n",
            "48999\n",
            "49000\n",
            "49001\n",
            "49002\n",
            "49003\n",
            "49004\n",
            "49005\n",
            "49006\n",
            "49007\n",
            "49008\n",
            "49009\n",
            "49010\n",
            "49011\n",
            "49012\n",
            "49013\n",
            "49014\n",
            "49015\n",
            "49016\n",
            "49017\n",
            "49018\n",
            "49019\n",
            "49020\n",
            "49021\n",
            "49022\n",
            "49023\n",
            "49024\n",
            "49025\n",
            "49026\n",
            "49027\n",
            "49028\n",
            "49029\n",
            "49030\n",
            "49031\n",
            "49032\n",
            "49033\n",
            "49034\n",
            "49035\n",
            "49036\n",
            "49037\n",
            "49038\n",
            "49039\n",
            "49040\n",
            "49041\n",
            "49042\n",
            "49043\n",
            "49044\n",
            "49045\n",
            "49046\n",
            "49047\n",
            "49048\n",
            "49049\n",
            "49050\n",
            "49051\n",
            "49052\n",
            "49053\n",
            "49054\n",
            "49055\n",
            "49056\n",
            "49057\n",
            "49058\n",
            "49059\n",
            "49060\n",
            "49061\n",
            "49062\n",
            "49063\n",
            "49064\n",
            "49065\n",
            "49066\n",
            "49067\n",
            "49068\n",
            "49069\n",
            "49070\n",
            "49071\n",
            "49072\n",
            "49073\n",
            "49074\n",
            "49075\n",
            "49076\n",
            "49077\n",
            "49078\n",
            "49079\n",
            "49080\n",
            "49081\n",
            "49082\n",
            "49083\n",
            "49084\n",
            "49085\n",
            "49086\n",
            "49087\n",
            "49088\n",
            "49089\n",
            "49090\n",
            "49091\n",
            "49092\n",
            "49093\n",
            "49094\n",
            "49095\n",
            "49096\n",
            "49097\n",
            "49098\n",
            "49099\n",
            "49100\n",
            "49101\n",
            "49102\n",
            "49103\n",
            "49104\n",
            "49105\n",
            "49106\n",
            "49107\n",
            "49108\n",
            "49109\n",
            "49110\n",
            "49111\n",
            "49112\n",
            "49113\n",
            "49114\n",
            "49115\n",
            "49116\n",
            "49117\n",
            "49118\n",
            "49119\n",
            "49120\n",
            "49121\n",
            "49122\n",
            "49123\n",
            "49124\n",
            "49125\n",
            "49126\n",
            "49127\n",
            "49128\n",
            "49129\n",
            "49130\n",
            "49131\n",
            "49132\n",
            "49133\n",
            "49134\n",
            "49135\n",
            "49136\n",
            "49137\n",
            "49138\n",
            "49139\n",
            "49140\n",
            "49141\n",
            "49142\n",
            "49143\n",
            "49144\n",
            "49145\n",
            "49146\n",
            "49147\n",
            "49148\n",
            "49149\n",
            "49150\n",
            "49151\n",
            "49152\n",
            "49153\n",
            "49154\n",
            "49155\n",
            "49156\n",
            "49157\n",
            "49158\n",
            "49159\n",
            "49160\n",
            "49161\n",
            "49162\n",
            "49163\n",
            "49164\n",
            "49165\n",
            "49166\n",
            "49167\n",
            "49168\n",
            "49169\n",
            "49170\n",
            "49171\n",
            "49172\n",
            "49173\n",
            "49174\n",
            "49175\n",
            "49176\n",
            "49177\n",
            "49178\n",
            "49179\n",
            "49180\n",
            "49181\n",
            "49182\n",
            "49183\n",
            "49184\n",
            "49185\n",
            "49186\n",
            "49187\n",
            "49188\n",
            "49189\n",
            "49190\n",
            "49191\n",
            "49192\n",
            "49193\n",
            "49194\n",
            "49195\n",
            "49196\n",
            "49197\n",
            "49198\n",
            "49199\n",
            "49200\n",
            "49201\n",
            "49202\n",
            "49203\n",
            "49204\n",
            "49205\n",
            "49206\n",
            "49207\n",
            "49208\n",
            "49209\n",
            "49210\n",
            "49211\n",
            "49212\n",
            "49213\n",
            "49214\n",
            "49215\n",
            "49216\n",
            "49217\n",
            "49218\n",
            "49219\n",
            "49220\n",
            "49221\n",
            "49222\n",
            "49223\n",
            "49224\n",
            "49225\n",
            "49226\n",
            "49227\n",
            "49228\n",
            "49229\n",
            "49230\n",
            "49231\n",
            "49232\n",
            "49233\n",
            "49234\n",
            "49235\n",
            "49236\n",
            "49237\n",
            "49238\n",
            "49239\n",
            "49240\n",
            "49241\n",
            "49242\n",
            "49243\n",
            "49244\n",
            "49245\n",
            "49246\n",
            "49247\n",
            "49248\n",
            "49249\n",
            "49250\n",
            "49251\n",
            "49252\n",
            "49253\n",
            "49254\n",
            "49255\n",
            "49256\n",
            "49257\n",
            "49258\n",
            "49259\n",
            "49260\n",
            "49261\n",
            "49262\n",
            "49263\n",
            "49264\n",
            "49265\n",
            "49266\n",
            "49267\n",
            "49268\n",
            "49269\n",
            "49270\n",
            "49271\n",
            "49272\n",
            "49273\n",
            "49274\n",
            "49275\n",
            "49276\n",
            "49277\n",
            "49278\n",
            "49279\n",
            "49280\n",
            "49281\n",
            "49282\n",
            "49283\n",
            "49284\n",
            "49285\n",
            "49286\n",
            "49287\n",
            "49288\n",
            "49289\n",
            "49290\n",
            "49291\n",
            "49292\n",
            "49293\n",
            "49294\n",
            "49295\n",
            "49296\n",
            "49297\n",
            "49298\n",
            "49299\n",
            "49300\n",
            "49301\n",
            "49302\n",
            "49303\n",
            "49304\n",
            "49305\n",
            "49306\n",
            "49307\n",
            "49308\n",
            "49309\n",
            "49310\n",
            "49311\n",
            "49312\n",
            "49313\n",
            "49314\n",
            "49315\n",
            "49316\n",
            "49317\n",
            "49318\n",
            "49319\n",
            "49320\n",
            "49321\n",
            "49322\n",
            "49323\n",
            "49324\n",
            "49325\n",
            "49326\n",
            "49327\n",
            "49328\n",
            "49329\n",
            "49330\n",
            "49331\n",
            "49332\n",
            "49333\n",
            "49334\n",
            "49335\n",
            "49336\n",
            "49337\n",
            "49338\n",
            "49339\n",
            "49340\n",
            "49341\n",
            "49342\n",
            "49343\n",
            "49344\n",
            "49345\n",
            "49346\n",
            "49347\n",
            "49348\n",
            "49349\n",
            "49350\n",
            "49351\n",
            "49352\n",
            "49353\n",
            "49354\n",
            "49355\n",
            "49356\n",
            "49357\n",
            "49358\n",
            "49359\n",
            "49360\n",
            "49361\n",
            "49362\n",
            "49363\n",
            "49364\n",
            "49365\n",
            "49366\n",
            "49367\n",
            "49368\n",
            "49369\n",
            "49370\n",
            "49371\n",
            "49372\n",
            "49373\n",
            "49374\n",
            "49375\n",
            "49376\n",
            "49377\n",
            "49378\n",
            "49379\n",
            "49380\n",
            "49381\n",
            "49382\n",
            "49383\n",
            "49384\n",
            "49385\n",
            "49386\n",
            "49387\n",
            "49388\n",
            "49389\n",
            "49390\n",
            "49391\n",
            "49392\n",
            "49393\n",
            "49394\n",
            "49395\n",
            "49396\n",
            "49397\n",
            "49398\n",
            "49399\n",
            "49400\n",
            "49401\n",
            "49402\n",
            "49403\n",
            "49404\n",
            "49405\n",
            "49406\n",
            "49407\n",
            "49408\n",
            "49409\n",
            "49410\n",
            "49411\n",
            "49412\n",
            "49413\n",
            "49414\n",
            "49415\n",
            "49416\n",
            "49417\n",
            "49418\n",
            "49419\n",
            "49420\n",
            "49421\n",
            "49422\n",
            "49423\n",
            "49424\n",
            "49425\n",
            "49426\n",
            "49427\n",
            "49428\n",
            "49429\n",
            "49430\n",
            "49431\n",
            "49432\n",
            "49433\n",
            "49434\n",
            "49435\n",
            "49436\n",
            "49437\n",
            "49438\n",
            "49439\n",
            "49440\n",
            "49441\n",
            "49442\n",
            "49443\n",
            "49444\n",
            "49445\n",
            "49446\n",
            "49447\n",
            "49448\n",
            "49449\n",
            "49450\n",
            "49451\n",
            "49452\n",
            "49453\n",
            "49454\n",
            "49455\n",
            "49456\n",
            "49457\n",
            "49458\n",
            "49459\n",
            "49460\n",
            "49461\n",
            "49462\n",
            "49463\n",
            "49464\n",
            "49465\n",
            "49466\n",
            "49467\n",
            "49468\n",
            "49469\n",
            "49470\n",
            "49471\n",
            "49472\n",
            "49473\n",
            "49474\n",
            "49475\n",
            "49476\n",
            "49477\n",
            "49478\n",
            "49479\n",
            "49480\n",
            "49481\n",
            "49482\n",
            "49483\n",
            "49484\n",
            "49485\n",
            "49486\n",
            "49487\n",
            "49488\n",
            "49489\n",
            "49490\n",
            "49491\n",
            "49492\n",
            "49493\n",
            "49494\n",
            "49495\n",
            "49496\n",
            "49497\n",
            "49498\n",
            "49499\n",
            "49500\n",
            "49501\n",
            "49502\n",
            "49503\n",
            "49504\n",
            "49505\n",
            "49506\n",
            "49507\n",
            "49508\n",
            "49509\n",
            "49510\n",
            "49511\n",
            "49512\n",
            "49513\n",
            "49514\n",
            "49515\n",
            "49516\n",
            "49517\n",
            "49518\n",
            "49519\n",
            "49520\n",
            "49521\n",
            "49522\n",
            "49523\n",
            "49524\n",
            "49525\n",
            "49526\n",
            "49527\n",
            "49528\n",
            "49529\n",
            "49530\n",
            "49531\n",
            "49532\n",
            "49533\n",
            "49534\n",
            "49535\n",
            "49536\n",
            "49537\n",
            "49538\n",
            "49539\n",
            "49540\n",
            "49541\n",
            "49542\n",
            "49543\n",
            "49544\n",
            "49545\n",
            "49546\n",
            "49547\n",
            "49548\n",
            "49549\n",
            "49550\n",
            "49551\n",
            "49552\n",
            "49553\n",
            "49554\n",
            "49555\n",
            "49556\n",
            "49557\n",
            "49558\n",
            "49559\n",
            "49560\n",
            "49561\n",
            "49562\n",
            "49563\n",
            "49564\n",
            "49565\n",
            "49566\n",
            "49567\n",
            "49568\n",
            "49569\n",
            "49570\n",
            "49571\n",
            "49572\n",
            "49573\n",
            "49574\n",
            "49575\n",
            "49576\n",
            "49577\n",
            "49578\n",
            "49579\n",
            "49580\n",
            "49581\n",
            "49582\n",
            "49583\n",
            "49584\n",
            "49585\n",
            "49586\n",
            "49587\n",
            "49588\n",
            "49589\n",
            "49590\n",
            "49591\n",
            "49592\n",
            "49593\n",
            "49594\n",
            "49595\n",
            "49596\n",
            "49597\n",
            "49598\n",
            "49599\n",
            "49600\n",
            "49601\n",
            "49602\n",
            "49603\n",
            "49604\n",
            "49605\n",
            "49606\n",
            "49607\n",
            "49608\n",
            "49609\n",
            "49610\n",
            "49611\n",
            "49612\n",
            "49613\n",
            "49614\n",
            "49615\n",
            "49616\n",
            "49617\n",
            "49618\n",
            "49619\n",
            "49620\n",
            "49621\n",
            "49622\n",
            "49623\n",
            "49624\n",
            "49625\n",
            "49626\n",
            "49627\n",
            "49628\n",
            "49629\n",
            "49630\n",
            "49631\n",
            "49632\n",
            "49633\n",
            "49634\n",
            "49635\n",
            "49636\n",
            "49637\n",
            "49638\n",
            "49639\n",
            "49640\n",
            "49641\n",
            "49642\n",
            "49643\n",
            "49644\n",
            "49645\n",
            "49646\n",
            "49647\n",
            "49648\n",
            "49649\n",
            "49650\n",
            "49651\n",
            "49652\n",
            "49653\n",
            "49654\n",
            "49655\n",
            "49656\n",
            "49657\n",
            "49658\n",
            "49659\n",
            "49660\n",
            "49661\n",
            "49662\n",
            "49663\n",
            "49664\n",
            "49665\n",
            "49666\n",
            "49667\n",
            "49668\n",
            "49669\n",
            "49670\n",
            "49671\n",
            "49672\n",
            "49673\n",
            "49674\n",
            "49675\n",
            "49676\n",
            "49677\n",
            "49678\n",
            "49679\n",
            "49680\n",
            "49681\n",
            "49682\n",
            "49683\n",
            "49684\n",
            "49685\n",
            "49686\n",
            "49687\n",
            "49688\n",
            "49689\n",
            "49690\n",
            "49691\n",
            "49692\n",
            "49693\n",
            "49694\n",
            "49695\n",
            "49696\n",
            "49697\n",
            "49698\n",
            "49699\n",
            "49700\n",
            "49701\n",
            "49702\n",
            "49703\n",
            "49704\n",
            "49705\n",
            "49706\n",
            "49707\n",
            "49708\n",
            "49709\n",
            "49710\n",
            "49711\n",
            "49712\n",
            "49713\n",
            "49714\n",
            "49715\n",
            "49716\n",
            "49717\n",
            "49718\n",
            "49719\n",
            "49720\n",
            "49721\n",
            "49722\n",
            "49723\n",
            "49724\n",
            "49725\n",
            "49726\n",
            "49727\n",
            "49728\n",
            "49729\n",
            "49730\n",
            "49731\n",
            "49732\n",
            "49733\n",
            "49734\n",
            "49735\n",
            "49736\n",
            "49737\n",
            "49738\n",
            "49739\n",
            "49740\n",
            "49741\n",
            "49742\n",
            "49743\n",
            "49744\n",
            "49745\n",
            "49746\n",
            "49747\n",
            "49748\n",
            "49749\n",
            "49750\n",
            "49751\n",
            "49752\n",
            "49753\n",
            "49754\n",
            "49755\n",
            "49756\n",
            "49757\n",
            "49758\n",
            "49759\n",
            "49760\n",
            "49761\n",
            "49762\n",
            "49763\n",
            "49764\n",
            "49765\n",
            "49766\n",
            "49767\n",
            "49768\n",
            "49769\n",
            "49770\n",
            "49771\n",
            "49772\n",
            "49773\n",
            "49774\n",
            "49775\n",
            "49776\n",
            "49777\n",
            "49778\n",
            "49779\n",
            "49780\n",
            "49781\n",
            "49782\n",
            "49783\n",
            "49784\n",
            "49785\n",
            "49786\n",
            "49787\n",
            "49788\n",
            "49789\n",
            "49790\n",
            "49791\n",
            "49792\n",
            "49793\n",
            "49794\n",
            "49795\n",
            "49796\n",
            "49797\n",
            "49798\n",
            "49799\n",
            "49800\n",
            "49801\n",
            "49802\n",
            "49803\n",
            "49804\n",
            "49805\n",
            "49806\n",
            "49807\n",
            "49808\n",
            "49809\n",
            "49810\n",
            "49811\n",
            "49812\n",
            "49813\n",
            "49814\n",
            "49815\n",
            "49816\n",
            "49817\n",
            "49818\n",
            "49819\n",
            "49820\n",
            "49821\n",
            "49822\n",
            "49823\n",
            "49824\n",
            "49825\n",
            "49826\n",
            "49827\n",
            "49828\n",
            "49829\n",
            "49830\n",
            "49831\n",
            "49832\n",
            "49833\n",
            "49834\n",
            "49835\n",
            "49836\n",
            "49837\n",
            "49838\n",
            "49839\n",
            "49840\n",
            "49841\n",
            "49842\n",
            "49843\n",
            "49844\n",
            "49845\n",
            "49846\n",
            "49847\n",
            "49848\n",
            "49849\n",
            "49850\n",
            "49851\n",
            "49852\n",
            "49853\n",
            "49854\n",
            "49855\n",
            "49856\n",
            "49857\n",
            "49858\n",
            "49859\n",
            "49860\n",
            "49861\n",
            "49862\n",
            "49863\n",
            "49864\n",
            "49865\n",
            "49866\n",
            "49867\n",
            "49868\n",
            "49869\n",
            "49870\n",
            "49871\n",
            "49872\n",
            "49873\n",
            "49874\n",
            "49875\n",
            "49876\n",
            "49877\n",
            "49878\n",
            "49879\n",
            "49880\n",
            "49881\n",
            "49882\n",
            "49883\n",
            "49884\n",
            "49885\n",
            "49886\n",
            "49887\n",
            "49888\n",
            "49889\n",
            "49890\n",
            "49891\n",
            "49892\n",
            "49893\n",
            "49894\n",
            "49895\n",
            "49896\n",
            "49897\n",
            "49898\n",
            "49899\n",
            "49900\n",
            "49901\n",
            "49902\n",
            "49903\n",
            "49904\n",
            "49905\n",
            "49906\n",
            "49907\n",
            "49908\n",
            "49909\n",
            "49910\n",
            "49911\n",
            "49912\n",
            "49913\n",
            "49914\n",
            "49915\n",
            "49916\n",
            "49917\n",
            "49918\n",
            "49919\n",
            "49920\n",
            "49921\n",
            "49922\n",
            "49923\n",
            "49924\n",
            "49925\n",
            "49926\n",
            "49927\n",
            "49928\n",
            "49929\n",
            "49930\n",
            "49931\n",
            "49932\n",
            "49933\n",
            "49934\n",
            "49935\n",
            "49936\n",
            "49937\n",
            "49938\n",
            "49939\n",
            "49940\n",
            "49941\n",
            "49942\n",
            "49943\n",
            "49944\n",
            "49945\n",
            "49946\n",
            "49947\n",
            "49948\n",
            "49949\n",
            "49950\n",
            "49951\n",
            "49952\n",
            "49953\n",
            "49954\n",
            "49955\n",
            "49956\n",
            "49957\n",
            "49958\n",
            "49959\n",
            "49960\n",
            "49961\n",
            "49962\n",
            "49963\n",
            "49964\n",
            "49965\n",
            "49966\n",
            "49967\n",
            "49968\n",
            "49969\n",
            "49970\n",
            "49971\n",
            "49972\n",
            "49973\n",
            "49974\n",
            "49975\n",
            "49976\n",
            "49977\n",
            "49978\n",
            "49979\n",
            "49980\n",
            "49981\n",
            "49982\n",
            "49983\n",
            "49984\n",
            "49985\n",
            "49986\n",
            "49987\n",
            "49988\n",
            "49989\n",
            "49990\n",
            "49991\n",
            "49992\n",
            "49993\n",
            "49994\n",
            "49995\n",
            "49996\n",
            "49997\n",
            "49998\n",
            "49999\n",
            "X_raw is tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [-1.0000, -1.0000],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-1.5000, -0.5000],\n",
            "        [ 1.0000, -1.0000],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "X_GD is tensor([[-4.0001, -3.9999],\n",
            "        [-3.9999, -0.9998],\n",
            "        [-2.1119, -1.2889],\n",
            "        [-2.2003,  1.8486],\n",
            "        [ 1.1386,  3.0013],\n",
            "        [-2.9645, -4.0210],\n",
            "        [ 0.5143, -3.2021],\n",
            "        [-0.6192, -1.3305],\n",
            "        [ 0.3772, -1.9945],\n",
            "        [ 2.4241,  1.4258]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([ 1.7617e-01+0.j,  4.5817e-02+0.j, -3.6518e-03+0.j,  1.0910e-03+0.j,\n",
            "         6.9949e-04+0.j,  1.2934e-04+0.j,  3.2439e-05+0.j,  5.8298e-05+0.j,\n",
            "        -3.3090e-05+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n",
            "tensor([ 1.4107e-01+0.j,  1.2186e-02+0.j,  1.0435e-02+0.j,  2.7387e-03+0.j,\n",
            "        -1.0255e-04+0.j,  2.9304e-04+0.j,  1.9066e-04+0.j,  3.0481e-05+0.j,\n",
            "         6.7025e-05+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tensor(X_raw, label='X_raw', marker='o', color='blue')\n",
        "plot_tensor(X_GD, label='X_GD', marker='o', color='blue')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "yCHAe32ZenAY",
        "outputId": "6a207b07-abfb-492e-8dda-41ab9c087324"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_data_with_centralgradients(X_raw, Y, model1, model2, max_iterations=20, learning_rate=0.1,  threshold=0.001):\n",
        "    \"\"\"\n",
        "    Optimize data using gradient calculations and Monte Carlo method.\n",
        "\n",
        "    :param nn_model: Neural Network Model class.\n",
        "    :param X_raw: Input data.\n",
        "    :param Y: Target data.\n",
        "    :param W_0, b, V_0, c: Initial weights and biases for the neural network.\n",
        "    :param max_iterations: Maximum number of iterations.\n",
        "    :param learning_rate: Learning rate for optimization.\n",
        "    :param MC_num_samples: Number of samples for Monte Carlo method.\n",
        "    :param surrounding_proportion: Proportion of surrounding points' gradients.\n",
        "    :param max_deviation_for_weight: Maximum deviation for weight perturbation.\n",
        "    :param threshold: Threshold for the norm of the second-order gradient.\n",
        "    :return: Optimized X_raw tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_raw_tensor = X_raw.clone().detach().requires_grad_(True) if X_raw.requires_grad else torch.tensor(X_raw, dtype=torch.float64, requires_grad=True)\n",
        "    Y_tensor = Y.clone().detach().requires_grad_(True) if Y.requires_grad else torch.tensor(Y, dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "\n",
        "    # Initialize the neural network with provided weights\n",
        "    #nn_model_instance1 = SimpleNN(W_0, b, V_0, c)\n",
        "    nn_model_instance1 = model1\n",
        "    nn_model_instance2 = model2\n",
        "\n",
        "    # Store original weights\n",
        "    original_weights1 = {\n",
        "        'W_0': nn_model_instance1.W_0.data.clone(),\n",
        "        'b': nn_model_instance1.b.data.clone(),\n",
        "        'V_0': nn_model_instance1.V_0.data.clone(),\n",
        "        'c': nn_model_instance1.c.data.clone()\n",
        "    }\n",
        "    original_weights2 = {\n",
        "        'W_0': nn_model_instance2.W_0.data.clone(),\n",
        "        'b': nn_model_instance2.b.data.clone(),\n",
        "        'V_0': nn_model_instance2.V_0.data.clone(),\n",
        "        'c': nn_model_instance2.c.data.clone()\n",
        "    }\n",
        "    print(\"Original weight1 is {}\".format(original_weights1))\n",
        "    print(\"Original weight2 is {}\".format(original_weights2))\n",
        "    print(\"Initial X_GD {}\".format(X_raw_tensor))\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # [Insert the existing logic of your loop here, using nn_model_instance, X_raw_tensor, Y_tensor, and other parameters]\n",
        "        # Calculate the gradient at the central point\n",
        "        central_grad1 = calculate_second_order_grad(nn_model_instance1, X_raw_tensor, Y_tensor)\n",
        "        central_grad2 = calculate_second_order_grad(nn_model_instance2, X_raw_tensor, Y_tensor)\n",
        "        # Check if grad_X is None before proceeding\n",
        "        central_grad_norm1 = torch.norm(central_grad1)\n",
        "        central_grad1 = central_grad1 / central_grad_norm1\n",
        "        central_grad_norm2 = torch.norm(central_grad2)\n",
        "        central_grad2 = central_grad2 / central_grad_norm2\n",
        "        #print(central_grad)\n",
        "        # Surrouning points' grads\n",
        "        #surrounding_grads_pre = []\n",
        "        #norms_pre = []\n",
        "        #negative_eigenvalues = []\n",
        "\n",
        "\n",
        "        # Combine gradients\n",
        "        combined_grad = (central_grad1 + central_grad2)/2\n",
        "        #combined_grad =  average_surrounding_grad\n",
        "        #print(combined_grad)\n",
        "        # Calculate the norm of the combined gradient\n",
        "        combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "        # Check for a non-zero norm to avoid division by zero\n",
        "        if combined_grad_norm == 0:\n",
        "        # Normalize the gradient\n",
        "          print(\"Gradient is zero; no update required.\")\n",
        "\n",
        "###############\n",
        "\n",
        "        # Check if the norm of the second-order gradient is below the threshold\n",
        "        if torch.norm(combined_grad) < threshold:\n",
        "            print(f\"Convergence reached at iteration {i}\")\n",
        "            break\n",
        "        # Update X_raw using the normalized gradient and learning rate\n",
        "        X_raw_tensor.data -= learning_rate * combined_grad_norm\n",
        "\n",
        "        # Zero out gradients for the next iteration\n",
        "        nn_model_instance1.zero_grad()\n",
        "        nn_model_instance2.zero_grad()\n",
        "        X_raw_tensor.grad = None\n",
        "        # Update and checks as per your original code\n",
        "\n",
        "\n",
        "\n",
        "    # Print final modified data\n",
        "\n",
        "    print(\"Output X is: {}\".format(X_raw_tensor))\n",
        "\n",
        "\n",
        "    print(central_grad_norm1)\n",
        "    print(central_grad_norm2)\n",
        "    # Return the optimized X_raw tensor\n",
        "    return X_raw_tensor\n",
        "\n",
        "# Example usage\n",
        "nn_model_F1 = SimpleNN(W_0, b, V_0, c)\n",
        "nn_model_F2 = SimpleNN(W_02, b2, V_02, c2)\n",
        "optimized_X_1 = optimize_data_with_centralgradients(X_GD, Y, nn_model_F1, nn_model_F2, max_iterations=826, learning_rate=0.001,  threshold=0.0001)\n"
      ],
      "metadata": {
        "id": "QR7lCrl3d2un",
        "outputId": "7073a9cf-a1bc-4514-8043-d77dc05bd7e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-153-ffe13a9271a5>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_tensor = Y.clone().detach().requires_grad_(True) if Y.requires_grad else torch.tensor(Y, dtype=torch.float64, requires_grad=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight1 is {'W_0': tensor([[-7.6083,  3.7633],\n",
            "        [-4.8386,  6.0030]], dtype=torch.float64), 'b': tensor([[-14.7330,   7.7171]], dtype=torch.float64), 'V_0': tensor([[8.9430],\n",
            "        [6.7330]], dtype=torch.float64), 'c': tensor([[-6.7378]], dtype=torch.float64)}\n",
            "Original weight2 is {'W_0': tensor([[-7.6657,  3.7610],\n",
            "        [-0.9085, -4.3715]], dtype=torch.float64), 'b': tensor([[-14.7592,   2.4935]], dtype=torch.float64), 'V_0': tensor([[10.4167],\n",
            "        [-1.4003]], dtype=torch.float64), 'c': tensor([[0.0023]], dtype=torch.float64)}\n",
            "Initial X_GD tensor([[-4.0001, -3.9999],\n",
            "        [-3.9999, -0.9998],\n",
            "        [-2.1119, -1.2889],\n",
            "        [-2.2003,  1.8486],\n",
            "        [ 1.1386,  3.0013],\n",
            "        [-2.9645, -4.0210],\n",
            "        [ 0.5143, -3.2021],\n",
            "        [-0.6192, -1.3305],\n",
            "        [ 0.3772, -1.9945],\n",
            "        [ 2.4241,  1.4258]], dtype=torch.float64, requires_grad=True)\n",
            "Output X is: tensor([[-4.5267, -4.5264],\n",
            "        [-4.5264, -1.5264],\n",
            "        [-2.6384, -1.8154],\n",
            "        [-2.7269,  1.3221],\n",
            "        [ 0.6120,  2.4747],\n",
            "        [-3.4910, -4.5475],\n",
            "        [-0.0123, -3.7286],\n",
            "        [-1.1457, -1.8570],\n",
            "        [-0.1493, -2.5211],\n",
            "        [ 1.8976,  0.8993]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.3713, dtype=torch.float64)\n",
            "tensor(0.0265, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model1 = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_central1, eigenvalues_central1 = compute_hessian_and_eigenvalues(nn_model1, optimized_X_1, Y)\n",
        "\n",
        "print(eigenvalues_central1)\n",
        "check_local_minimum(eigenvalues_central1)\n",
        "\n",
        "\n",
        "nn_model2 = SimpleNN(W_02, b2, V_02, c2)\n",
        "hessian_matrix_central2, eigenvalues_central2 = compute_hessian_and_eigenvalues(nn_model2, optimized_X_1, Y)\n",
        "\n",
        "print(eigenvalues_central2)\n",
        "check_local_minimum(eigenvalues_central2)"
      ],
      "metadata": {
        "id": "LOlqYSsAhqW5",
        "outputId": "4217cc01-849e-400f-b5cc-f18bf3ddcf72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-4.5436e-01+0.j,  1.7779e-01+0.j,  7.6419e-02+0.j,  5.1844e-02+0.j,\n",
            "         1.5234e-03+0.j,  7.4462e-04+0.j,  3.9259e-04+0.j, -2.8857e-05+0.j,\n",
            "         7.2256e-08+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n",
            "tensor([ 1.4415e-01+0.j,  1.8839e-02+0.j,  1.1159e-02+0.j,  2.0681e-03+0.j,\n",
            "        -1.0963e-07+0.j,  1.6380e-05+0.j,  7.9199e-06+0.j,  2.5782e-04+0.j,\n",
            "         2.1626e-04+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable anomaly detection\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "def make_model_copy(original_model):\n",
        "    # Create a new instance of the model\n",
        "    model_copy = SimpleNN(W_0, b, V_0, c)  # Use the same parameters as used to create the original model\n",
        "    # Copy the initial weights from the original model\n",
        "    model_copy.load_state_dict(original_model.state_dict())\n",
        "    return model_copy\n",
        "\n",
        "def trap_model(original_model, X, Y, max_iterations, number_of_x_iterations, weightlr=0.01, xlr=0.01):\n",
        "    X_modifiable = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    for current_iteration in range(max_iterations):\n",
        "        # Use a fresh copy of the model\n",
        "        model = make_model_copy(original_model)\n",
        "\n",
        "        optimizer_weights = optim.SGD(model.parameters(), lr=weightlr)\n",
        "\n",
        "    # Optimizer for the weights\n",
        "    #optimizer_weights = optim.SGD(model.parameters(), lr=weightlr)\n",
        "\n",
        "        # Save initial state of the model\n",
        "        initial_state_dict = model.state_dict()\n",
        "\n",
        "        # Load initial weights using the deep copied state\n",
        "        model.load_state_dict(copy.deepcopy(initial_state_dict))\n",
        "\n",
        "        # Reset the optimizer state\n",
        "        optimizer_weights = optim.SGD(model.parameters(), lr=weightlr)\n",
        "\n",
        "        # Forward pass with current X_modifiable and initial weights\n",
        "        model_output = model(X_modifiable)\n",
        "\n",
        "        old_loss = -torch.mean(Y * torch.log(model_output) + (1 - Y) * torch.log(1 - model_output))\n",
        "\n",
        "        # Gradient descent step for weights\n",
        "        optimizer_weights.zero_grad()\n",
        "        old_loss.backward()\n",
        "        optimizer_weights.step()\n",
        "        # Gather all gradients into a list after flattening\n",
        "        gradients = [param.grad.view(-1) for param in model.parameters() if param.grad is not None]\n",
        "\n",
        "        # Concatenate all gradients to form a single vector and calculate its norm\n",
        "        total_gradient = torch.cat(gradients)\n",
        "        total_gradient_norm = total_gradient.norm()\n",
        "\n",
        "        # Check if the combined gradient norm is below the threshold\n",
        "        if total_gradient_norm < 1e-8:\n",
        "            print(\"Combined gradient norm below threshold, stopping optimization.\")\n",
        "            break\n",
        "\n",
        "        # Save new weights and create a copy of X_modifiable for the inner loop\n",
        "        new_state_dict = model.state_dict()\n",
        "        X_inner = X_modifiable.clone().detach().requires_grad_(True)\n",
        "        optimizer_x = optim.SGD([X_inner], lr=xlr)\n",
        "\n",
        "        for _ in range(number_of_x_iterations):\n",
        "            # Load new weights\n",
        "            model.load_state_dict(new_state_dict)\n",
        "            print(\"before {}\".format(X_inner))\n",
        "            output_new = model(X_inner)\n",
        "            new_loss = -torch.mean(Y * torch.log(output_new) + (1 - Y) * torch.log(1 - output_new))\n",
        "\n",
        "            # Calculate combined loss\n",
        "            combined_loss = old_loss.detach() - new_loss\n",
        "            print(combined_loss)\n",
        "\n",
        "\n",
        "            # Check condition\n",
        "            if combined_loss <= 0:\n",
        "                print(\"acheive! at:{}\".format(current_iteration))\n",
        "                print(X_inner)\n",
        "                break\n",
        "            else:\n",
        "                optimizer_x.zero_grad()\n",
        "                combined_loss.backward()\n",
        "                optimizer_x.step()\n",
        "        #print(X_inner.data)\n",
        "        # Update X_modifiable with changes from the inner loop\n",
        "        # Calculate the difference between X_modifiable and X_inner\n",
        "        difference = (X_modifiable - X_inner).norm()\n",
        "\n",
        "        # Check if the difference is below a certain threshold\n",
        "        if difference < 1e-8:  # You can adjust this threshold as needed\n",
        "          print(\"X_modifiable did not change significantly, stopping early.\")\n",
        "          break\n",
        "        else:\n",
        "\n",
        "          # Update X_modifiable with changes from the inner loop\n",
        "          X_modifiable.data = X_inner.data\n",
        "\n",
        "\n",
        "    # After completing all iterations, load initial weights\n",
        "    model.load_state_dict(initial_state_dict)\n",
        "\n",
        "    # Forward pass with the final X_modifiable and initial weights\n",
        "    final_output = model(X_modifiable)\n",
        "    final_loss = -torch.mean(Y * torch.log(final_output) + (1 - Y) * torch.log(1 - final_output))\n",
        "\n",
        "    # Calculate gradients with respect to the initial weights\n",
        "    optimizer_weights.zero_grad()\n",
        "    final_loss.backward()\n",
        "\n",
        "    # Calculate and print the norm of the gradients for each parameter\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(f\"Gradient norm for {name}: {param.grad.norm().item()}\")\n",
        "\n",
        "    # Return the modified X\n",
        "    return X_modifiable\n",
        "\n",
        "# Initialize your model, X, Y, etc. outside the function\n",
        "# ... [Your initialization code here] ...\n",
        "\n",
        "# Call the function\n",
        "# ... [Call to train_model function here] ...\n",
        "#Example\n",
        "# Initialize your model, X, Y, etc. outside the function\n",
        "#custom_W_0 = [...]  # Replace [...] with your initial weights\n",
        "#custom_b = [...]   # Replace [...] with your initial bias\n",
        "#custom_V_0 = [...]  # Replace [...] with your next layer weights\n",
        "#custom_c = [...]   # Replace [...] with your next layer bias\n",
        "#model = SimpleNN(custom_W_0, custom_b, custom_V_0, custom_c)\n",
        "#X = torch.tensor([...], dtype=torch.float64, requires_grad=True)  # Replace [...] with your initial data\n",
        "#Y = torch.tensor([...], dtype=torch.float64)  # Replace [...] with target data\n",
        "\n",
        "# Call the function\n",
        "nn_model_trap = SimpleNN(W_0, b, V_0, c)\n",
        "trap_X = trap_model(nn_model_trap, X_raw, Y, max_iterations=1000, number_of_x_iterations=2, weightlr= 0.01, xlr= 0.05)\n",
        "plot_tensor(X_raw, label='X_raw', marker='o', color='blue')\n",
        "plot_tensor(trap_X, label='trap_X', marker='o', color='blue')"
      ],
      "metadata": {
        "id": "szMjSQUcynkl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a8ca69b-c547-4849-e5d7-7119ef69b9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4444, -2.4870],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0682,  2.1464],\n",
            "        [ 1.1326, -0.7886],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4446, -2.4873],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0686,  2.1468],\n",
            "        [ 1.1326, -0.7885],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4449, -2.4876],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0689,  2.1471],\n",
            "        [ 1.1326, -0.7885],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4452, -2.4879],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0693,  2.1475],\n",
            "        [ 1.1327, -0.7884],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4455, -2.4881],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0697,  2.1479],\n",
            "        [ 1.1327, -0.7883],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4458, -2.4884],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0700,  2.1483],\n",
            "        [ 1.1328, -0.7882],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4460, -2.4887],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0704,  2.1486],\n",
            "        [ 1.1328, -0.7882],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4463, -2.4890],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0707,  2.1490],\n",
            "        [ 1.1329, -0.7881],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4466, -2.4893],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0711,  2.1494],\n",
            "        [ 1.1329, -0.7880],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4469, -2.4896],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0714,  2.1498],\n",
            "        [ 1.1330, -0.7880],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4472, -2.4899],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0718,  2.1501],\n",
            "        [ 1.1330, -0.7879],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4474, -2.4902],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0721,  2.1505],\n",
            "        [ 1.1330, -0.7878],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4477, -2.4905],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0725,  2.1509],\n",
            "        [ 1.1331, -0.7877],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4480, -2.4908],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0729,  2.1513],\n",
            "        [ 1.1331, -0.7877],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4483, -2.4911],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0732,  2.1516],\n",
            "        [ 1.1332, -0.7876],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4486, -2.4914],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0736,  2.1520],\n",
            "        [ 1.1332, -0.7875],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4488, -2.4917],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0739,  2.1524],\n",
            "        [ 1.1333, -0.7875],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4491, -2.4919],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0743,  2.1528],\n",
            "        [ 1.1333, -0.7874],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4494, -2.4922],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0746,  2.1531],\n",
            "        [ 1.1334, -0.7873],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4497, -2.4925],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0750,  2.1535],\n",
            "        [ 1.1334, -0.7872],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4499, -2.4928],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0753,  2.1539],\n",
            "        [ 1.1334, -0.7872],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4502, -2.4931],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0757,  2.1542],\n",
            "        [ 1.1335, -0.7871],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4505, -2.4934],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0760,  2.1546],\n",
            "        [ 1.1335, -0.7870],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4508, -2.4937],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0764,  2.1550],\n",
            "        [ 1.1336, -0.7870],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4510, -2.4940],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0767,  2.1554],\n",
            "        [ 1.1336, -0.7869],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4513, -2.4943],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0771,  2.1557],\n",
            "        [ 1.1337, -0.7868],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4516, -2.4946],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0775,  2.1561],\n",
            "        [ 1.1337, -0.7867],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4519, -2.4948],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0778,  2.1565],\n",
            "        [ 1.1338, -0.7867],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4521, -2.4951],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0782,  2.1568],\n",
            "        [ 1.1338, -0.7866],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4524, -2.4954],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0785,  2.1572],\n",
            "        [ 1.1338, -0.7865],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4527, -2.4957],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0789,  2.1576],\n",
            "        [ 1.1339, -0.7865],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4530, -2.4960],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0792,  2.1579],\n",
            "        [ 1.1339, -0.7864],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4532, -2.4963],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0796,  2.1583],\n",
            "        [ 1.1340, -0.7863],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4535, -2.4966],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0799,  2.1587],\n",
            "        [ 1.1340, -0.7863],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4538, -2.4969],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0802,  2.1590],\n",
            "        [ 1.1341, -0.7862],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4541, -2.4972],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0806,  2.1594],\n",
            "        [ 1.1341, -0.7861],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4543, -2.4974],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0809,  2.1598],\n",
            "        [ 1.1342, -0.7860],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4546, -2.4977],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0813,  2.1601],\n",
            "        [ 1.1342, -0.7860],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4549, -2.4980],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0816,  2.1605],\n",
            "        [ 1.1342, -0.7859],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4552, -2.4983],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0820,  2.1609],\n",
            "        [ 1.1343, -0.7858],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4554, -2.4986],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0823,  2.1612],\n",
            "        [ 1.1343, -0.7858],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4557, -2.4989],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0827,  2.1616],\n",
            "        [ 1.1344, -0.7857],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4560, -2.4992],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0830,  2.1619],\n",
            "        [ 1.1344, -0.7856],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4563, -2.4994],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0834,  2.1623],\n",
            "        [ 1.1345, -0.7856],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4565, -2.4997],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0837,  2.1627],\n",
            "        [ 1.1345, -0.7855],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4568, -2.5000],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0841,  2.1630],\n",
            "        [ 1.1345, -0.7854],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4571, -2.5003],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0844,  2.1634],\n",
            "        [ 1.1346, -0.7853],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4573, -2.5006],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0848,  2.1638],\n",
            "        [ 1.1346, -0.7853],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4576, -2.5009],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0851,  2.1641],\n",
            "        [ 1.1347, -0.7852],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4579, -2.5012],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0854,  2.1645],\n",
            "        [ 1.1347, -0.7851],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4582, -2.5014],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0858,  2.1648],\n",
            "        [ 1.1348, -0.7851],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4584, -2.5017],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0861,  2.1652],\n",
            "        [ 1.1348, -0.7850],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4587, -2.5020],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0865,  2.1656],\n",
            "        [ 1.1348, -0.7849],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4590, -2.5023],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0868,  2.1659],\n",
            "        [ 1.1349, -0.7849],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4592, -2.5026],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0872,  2.1663],\n",
            "        [ 1.1349, -0.7848],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4595, -2.5029],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0875,  2.1666],\n",
            "        [ 1.1350, -0.7847],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4598, -2.5031],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0878,  2.1670],\n",
            "        [ 1.1350, -0.7847],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4600, -2.5034],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0882,  2.1674],\n",
            "        [ 1.1351, -0.7846],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4603, -2.5037],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0885,  2.1677],\n",
            "        [ 1.1351, -0.7845],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4606, -2.5040],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0889,  2.1681],\n",
            "        [ 1.1352, -0.7844],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4609, -2.5043],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0892,  2.1684],\n",
            "        [ 1.1352, -0.7844],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4611, -2.5046],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0896,  2.1688],\n",
            "        [ 1.1352, -0.7843],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4614, -2.5048],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0899,  2.1692],\n",
            "        [ 1.1353, -0.7842],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4617, -2.5051],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0902,  2.1695],\n",
            "        [ 1.1353, -0.7842],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4619, -2.5054],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0906,  2.1699],\n",
            "        [ 1.1354, -0.7841],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4622, -2.5057],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0909,  2.1702],\n",
            "        [ 1.1354, -0.7840],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4625, -2.5060],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0913,  2.1706],\n",
            "        [ 1.1355, -0.7840],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4627, -2.5063],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0916,  2.1709],\n",
            "        [ 1.1355, -0.7839],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4630, -2.5065],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0919,  2.1713],\n",
            "        [ 1.1355, -0.7838],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4633, -2.5068],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0923,  2.1716],\n",
            "        [ 1.1356, -0.7838],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4635, -2.5071],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0926,  2.1720],\n",
            "        [ 1.1356, -0.7837],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4638, -2.5074],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0929,  2.1724],\n",
            "        [ 1.1357, -0.7836],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4641, -2.5077],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0933,  2.1727],\n",
            "        [ 1.1357, -0.7836],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4643, -2.5079],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0936,  2.1731],\n",
            "        [ 1.1358, -0.7835],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4646, -2.5082],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0940,  2.1734],\n",
            "        [ 1.1358, -0.7834],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4649, -2.5085],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0943,  2.1738],\n",
            "        [ 1.1358, -0.7833],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4651, -2.5088],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0946,  2.1741],\n",
            "        [ 1.1359, -0.7833],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4654, -2.5091],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0950,  2.1745],\n",
            "        [ 1.1359, -0.7832],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4657, -2.5093],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0953,  2.1748],\n",
            "        [ 1.1360, -0.7831],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4659, -2.5096],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0956,  2.1752],\n",
            "        [ 1.1360, -0.7831],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4662, -2.5099],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0960,  2.1755],\n",
            "        [ 1.1361, -0.7830],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4665, -2.5102],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0963,  2.1759],\n",
            "        [ 1.1361, -0.7829],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4667, -2.5105],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0966,  2.1762],\n",
            "        [ 1.1361, -0.7829],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4670, -2.5107],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0970,  2.1766],\n",
            "        [ 1.1362, -0.7828],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4673, -2.5110],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0973,  2.1769],\n",
            "        [ 1.1362, -0.7827],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4675, -2.5113],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0976,  2.1773],\n",
            "        [ 1.1363, -0.7827],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4678, -2.5116],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0980,  2.1776],\n",
            "        [ 1.1363, -0.7826],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4681, -2.5118],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0983,  2.1780],\n",
            "        [ 1.1364, -0.7825],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4683, -2.5121],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0986,  2.1783],\n",
            "        [ 1.1364, -0.7825],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4686, -2.5124],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0990,  2.1787],\n",
            "        [ 1.1364, -0.7824],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4689, -2.5127],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0993,  2.1790],\n",
            "        [ 1.1365, -0.7823],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4691, -2.5130],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.0996,  2.1794],\n",
            "        [ 1.1365, -0.7823],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4694, -2.5132],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1000,  2.1797],\n",
            "        [ 1.1366, -0.7822],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4696, -2.5135],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1003,  2.1801],\n",
            "        [ 1.1366, -0.7821],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4699, -2.5138],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1006,  2.1804],\n",
            "        [ 1.1367, -0.7821],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4702, -2.5141],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1010,  2.1808],\n",
            "        [ 1.1367, -0.7820],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4704, -2.5143],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1013,  2.1811],\n",
            "        [ 1.1367, -0.7819],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4707, -2.5146],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1016,  2.1815],\n",
            "        [ 1.1368, -0.7818],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4710, -2.5149],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1020,  2.1818],\n",
            "        [ 1.1368, -0.7818],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4712, -2.5152],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1023,  2.1822],\n",
            "        [ 1.1369, -0.7817],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4715, -2.5154],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1026,  2.1825],\n",
            "        [ 1.1369, -0.7816],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4717, -2.5157],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1029,  2.1829],\n",
            "        [ 1.1370, -0.7816],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4720, -2.5160],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1033,  2.1832],\n",
            "        [ 1.1370, -0.7815],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4723, -2.5163],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1036,  2.1835],\n",
            "        [ 1.1370, -0.7814],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4725, -2.5165],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1039,  2.1839],\n",
            "        [ 1.1371, -0.7814],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4728, -2.5168],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1043,  2.1842],\n",
            "        [ 1.1371, -0.7813],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4731, -2.5171],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1046,  2.1846],\n",
            "        [ 1.1372, -0.7812],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4733, -2.5174],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1049,  2.1849],\n",
            "        [ 1.1372, -0.7812],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4736, -2.5176],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1052,  2.1853],\n",
            "        [ 1.1372, -0.7811],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4738, -2.5179],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1056,  2.1856],\n",
            "        [ 1.1373, -0.7810],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4741, -2.5182],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1059,  2.1860],\n",
            "        [ 1.1373, -0.7810],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4744, -2.5185],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-4.1062,  2.1863],\n",
            "        [ 1.1374, -0.7809],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4746, -2.5187],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1066,  2.1866],\n",
            "        [ 1.1374, -0.7808],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4749, -2.5190],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1069,  2.1870],\n",
            "        [ 1.1375, -0.7808],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4751, -2.5193],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1072,  2.1873],\n",
            "        [ 1.1375, -0.7807],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4754, -2.5195],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1075,  2.1877],\n",
            "        [ 1.1375, -0.7806],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4757, -2.5198],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1079,  2.1880],\n",
            "        [ 1.1376, -0.7806],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4759, -2.5201],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1082,  2.1884],\n",
            "        [ 1.1376, -0.7805],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4762, -2.5204],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1085,  2.1887],\n",
            "        [ 1.1377, -0.7804],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4764, -2.5206],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1088,  2.1890],\n",
            "        [ 1.1377, -0.7804],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4767, -2.5209],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1092,  2.1894],\n",
            "        [ 1.1378, -0.7803],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4770, -2.5212],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1095,  2.1897],\n",
            "        [ 1.1378, -0.7802],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4772, -2.5214],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1098,  2.1901],\n",
            "        [ 1.1378, -0.7802],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4775, -2.5217],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1101,  2.1904],\n",
            "        [ 1.1379, -0.7801],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4777, -2.5220],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1105,  2.1907],\n",
            "        [ 1.1379, -0.7800],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4780, -2.5223],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1108,  2.1911],\n",
            "        [ 1.1380, -0.7800],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4782, -2.5225],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1111,  2.1914],\n",
            "        [ 1.1380, -0.7799],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4785, -2.5228],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1114,  2.1917],\n",
            "        [ 1.1380, -0.7798],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4788, -2.5231],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1117,  2.1921],\n",
            "        [ 1.1381, -0.7798],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4790, -2.5233],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1121,  2.1924],\n",
            "        [ 1.1381, -0.7797],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4793, -2.5236],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1124,  2.1928],\n",
            "        [ 1.1382, -0.7796],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4795, -2.5239],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1127,  2.1931],\n",
            "        [ 1.1382, -0.7796],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4798, -2.5241],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1130,  2.1934],\n",
            "        [ 1.1383, -0.7795],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4800, -2.5244],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1133,  2.1938],\n",
            "        [ 1.1383, -0.7794],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4803, -2.5247],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1137,  2.1941],\n",
            "        [ 1.1383, -0.7794],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4806, -2.5250],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1140,  2.1944],\n",
            "        [ 1.1384, -0.7793],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4808, -2.5252],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1143,  2.1948],\n",
            "        [ 1.1384, -0.7792],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4811, -2.5255],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1146,  2.1951],\n",
            "        [ 1.1385, -0.7792],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4813, -2.5258],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1149,  2.1955],\n",
            "        [ 1.1385, -0.7791],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4816, -2.5260],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1153,  2.1958],\n",
            "        [ 1.1385, -0.7790],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4818, -2.5263],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1156,  2.1961],\n",
            "        [ 1.1386, -0.7790],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4821, -2.5266],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1159,  2.1965],\n",
            "        [ 1.1386, -0.7789],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4823, -2.5268],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1162,  2.1968],\n",
            "        [ 1.1387, -0.7788],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4826, -2.5271],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1165,  2.1971],\n",
            "        [ 1.1387, -0.7788],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4829, -2.5274],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1169,  2.1975],\n",
            "        [ 1.1387, -0.7787],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4831, -2.5276],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1172,  2.1978],\n",
            "        [ 1.1388, -0.7786],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4834, -2.5279],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1175,  2.1981],\n",
            "        [ 1.1388, -0.7786],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4836, -2.5282],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1178,  2.1985],\n",
            "        [ 1.1389, -0.7785],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4839, -2.5284],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1181,  2.1988],\n",
            "        [ 1.1389, -0.7784],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4841, -2.5287],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1184,  2.1991],\n",
            "        [ 1.1390, -0.7784],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4844, -2.5290],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1188,  2.1995],\n",
            "        [ 1.1390, -0.7783],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4846, -2.5292],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1191,  2.1998],\n",
            "        [ 1.1390, -0.7783],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4849, -2.5295],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1194,  2.2001],\n",
            "        [ 1.1391, -0.7782],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [ 0.4851, -2.5298],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1197,  2.2005],\n",
            "        [ 1.1391, -0.7781],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4854, -2.5300],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1200,  2.2008],\n",
            "        [ 1.1392, -0.7781],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4856, -2.5303],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1203,  2.2011],\n",
            "        [ 1.1392, -0.7780],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4859, -2.5306],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1207,  2.2014],\n",
            "        [ 1.1392, -0.7779],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4862, -2.5308],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1210,  2.2018],\n",
            "        [ 1.1393, -0.7779],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4864, -2.5311],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1213,  2.2021],\n",
            "        [ 1.1393, -0.7778],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4867, -2.5314],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1216,  2.2024],\n",
            "        [ 1.1394, -0.7777],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4869, -2.5316],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1219,  2.2028],\n",
            "        [ 1.1394, -0.7777],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4872, -2.5319],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1222,  2.2031],\n",
            "        [ 1.1394, -0.7776],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4874, -2.5322],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1225,  2.2034],\n",
            "        [ 1.1395, -0.7775],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4877, -2.5324],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1229,  2.2038],\n",
            "        [ 1.1395, -0.7775],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4879, -2.5327],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1232,  2.2041],\n",
            "        [ 1.1396, -0.7774],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4882, -2.5329],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1235,  2.2044],\n",
            "        [ 1.1396, -0.7773],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4884, -2.5332],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1238,  2.2047],\n",
            "        [ 1.1397, -0.7773],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4887, -2.5335],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1241,  2.2051],\n",
            "        [ 1.1397, -0.7772],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4889, -2.5337],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1244,  2.2054],\n",
            "        [ 1.1397, -0.7771],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4892, -2.5340],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1247,  2.2057],\n",
            "        [ 1.1398, -0.7771],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4894, -2.5343],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1250,  2.2061],\n",
            "        [ 1.1398, -0.7770],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4897, -2.5345],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1254,  2.2064],\n",
            "        [ 1.1399, -0.7769],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4899, -2.5348],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1257,  2.2067],\n",
            "        [ 1.1399, -0.7769],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4902, -2.5351],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1260,  2.2070],\n",
            "        [ 1.1399, -0.7768],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4904, -2.5353],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1263,  2.2074],\n",
            "        [ 1.1400, -0.7767],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4907, -2.5356],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1266,  2.2077],\n",
            "        [ 1.1400, -0.7767],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4909, -2.5358],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1269,  2.2080],\n",
            "        [ 1.1401, -0.7766],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4912, -2.5361],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1272,  2.2083],\n",
            "        [ 1.1401, -0.7766],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4914, -2.5364],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1275,  2.2087],\n",
            "        [ 1.1401, -0.7765],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4917, -2.5366],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1278,  2.2090],\n",
            "        [ 1.1402, -0.7764],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4919, -2.5369],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1281,  2.2093],\n",
            "        [ 1.1402, -0.7764],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4922, -2.5371],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1285,  2.2096],\n",
            "        [ 1.1403, -0.7763],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4924, -2.5374],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1288,  2.2100],\n",
            "        [ 1.1403, -0.7762],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4927, -2.5377],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1291,  2.2103],\n",
            "        [ 1.1403, -0.7762],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4929, -2.5379],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1294,  2.2106],\n",
            "        [ 1.1404, -0.7761],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4932, -2.5382],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1297,  2.2109],\n",
            "        [ 1.1404, -0.7760],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4934, -2.5384],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1300,  2.2113],\n",
            "        [ 1.1405, -0.7760],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4937, -2.5387],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1303,  2.2116],\n",
            "        [ 1.1405, -0.7759],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4939, -2.5390],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1306,  2.2119],\n",
            "        [ 1.1405, -0.7758],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4942, -2.5392],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1309,  2.2122],\n",
            "        [ 1.1406, -0.7758],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4944, -2.5395],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1312,  2.2125],\n",
            "        [ 1.1406, -0.7757],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4946, -2.5397],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1315,  2.2129],\n",
            "        [ 1.1407, -0.7757],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4949, -2.5400],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1318,  2.2132],\n",
            "        [ 1.1407, -0.7756],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4951, -2.5403],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1321,  2.2135],\n",
            "        [ 1.1407, -0.7755],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4954, -2.5405],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1325,  2.2138],\n",
            "        [ 1.1408, -0.7755],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4956, -2.5408],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1328,  2.2141],\n",
            "        [ 1.1408, -0.7754],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4959, -2.5410],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1331,  2.2145],\n",
            "        [ 1.1409, -0.7753],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4961, -2.5413],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1334,  2.2148],\n",
            "        [ 1.1409, -0.7753],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4964, -2.5416],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1337,  2.2151],\n",
            "        [ 1.1409, -0.7752],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4966, -2.5418],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1340,  2.2154],\n",
            "        [ 1.1410, -0.7751],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4969, -2.5421],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1343,  2.2157],\n",
            "        [ 1.1410, -0.7751],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4971, -2.5423],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1346,  2.2161],\n",
            "        [ 1.1411, -0.7750],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4974, -2.5426],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1349,  2.2164],\n",
            "        [ 1.1411, -0.7749],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4976, -2.5429],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1352,  2.2167],\n",
            "        [ 1.1411, -0.7749],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4979, -2.5431],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1355,  2.2170],\n",
            "        [ 1.1412, -0.7748],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4981, -2.5434],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1358,  2.2173],\n",
            "        [ 1.1412, -0.7748],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4983, -2.5436],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1361,  2.2177],\n",
            "        [ 1.1413, -0.7747],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4986, -2.5439],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1364,  2.2180],\n",
            "        [ 1.1413, -0.7746],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4988, -2.5441],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1367,  2.2183],\n",
            "        [ 1.1413, -0.7746],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4991, -2.5444],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1370,  2.2186],\n",
            "        [ 1.1414, -0.7745],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4993, -2.5447],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1373,  2.2189],\n",
            "        [ 1.1414, -0.7744],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4996, -2.5449],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1376,  2.2192],\n",
            "        [ 1.1415, -0.7744],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.4998, -2.5452],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1379,  2.2196],\n",
            "        [ 1.1415, -0.7743],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5001, -2.5454],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1382,  2.2199],\n",
            "        [ 1.1415, -0.7742],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5003, -2.5457],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1385,  2.2202],\n",
            "        [ 1.1416, -0.7742],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5005, -2.5459],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1388,  2.2205],\n",
            "        [ 1.1416, -0.7741],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5008, -2.5462],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1391,  2.2208],\n",
            "        [ 1.1417, -0.7741],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5010, -2.5464],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1394,  2.2211],\n",
            "        [ 1.1417, -0.7740],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5013, -2.5467],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1397,  2.2215],\n",
            "        [ 1.1417, -0.7739],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5015, -2.5470],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1400,  2.2218],\n",
            "        [ 1.1418, -0.7739],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5018, -2.5472],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1403,  2.2221],\n",
            "        [ 1.1418, -0.7738],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5020, -2.5475],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1406,  2.2224],\n",
            "        [ 1.1419, -0.7737],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5022, -2.5477],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1409,  2.2227],\n",
            "        [ 1.1419, -0.7737],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5025, -2.5480],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1412,  2.2230],\n",
            "        [ 1.1419, -0.7736],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5027, -2.5482],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1415,  2.2233],\n",
            "        [ 1.1420, -0.7736],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5030, -2.5485],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1418,  2.2237],\n",
            "        [ 1.1420, -0.7735],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5032, -2.5487],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1421,  2.2240],\n",
            "        [ 1.1421, -0.7734],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5035, -2.5490],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1424,  2.2243],\n",
            "        [ 1.1421, -0.7734],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5037, -2.5492],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1427,  2.2246],\n",
            "        [ 1.1421, -0.7733],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5039, -2.5495],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1430,  2.2249],\n",
            "        [ 1.1422, -0.7732],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5042, -2.5497],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1433,  2.2252],\n",
            "        [ 1.1422, -0.7732],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5044, -2.5500],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1436,  2.2255],\n",
            "        [ 1.1423, -0.7731],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5047, -2.5503],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1439,  2.2258],\n",
            "        [ 1.1423, -0.7730],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5049, -2.5505],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1442,  2.2262],\n",
            "        [ 1.1423, -0.7730],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5051, -2.5508],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1445,  2.2265],\n",
            "        [ 1.1424, -0.7729],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5054, -2.5510],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1448,  2.2268],\n",
            "        [ 1.1424, -0.7729],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5056, -2.5513],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1451,  2.2271],\n",
            "        [ 1.1425, -0.7728],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5059, -2.5515],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1454,  2.2274],\n",
            "        [ 1.1425, -0.7727],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5061, -2.5518],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1457,  2.2277],\n",
            "        [ 1.1425, -0.7727],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5063, -2.5520],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1460,  2.2280],\n",
            "        [ 1.1426, -0.7726],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5066, -2.5523],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1463,  2.2283],\n",
            "        [ 1.1426, -0.7725],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5068, -2.5525],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1466,  2.2286],\n",
            "        [ 1.1427, -0.7725],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5071, -2.5528],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1469,  2.2290],\n",
            "        [ 1.1427, -0.7724],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5073, -2.5530],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1472,  2.2293],\n",
            "        [ 1.1427, -0.7724],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5075, -2.5533],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1474,  2.2296],\n",
            "        [ 1.1428, -0.7723],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5078, -2.5535],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1477,  2.2299],\n",
            "        [ 1.1428, -0.7722],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5080, -2.5538],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1480,  2.2302],\n",
            "        [ 1.1428, -0.7722],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5083, -2.5540],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1483,  2.2305],\n",
            "        [ 1.1429, -0.7721],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5085, -2.5543],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1486,  2.2308],\n",
            "        [ 1.1429, -0.7720],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5087, -2.5545],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1489,  2.2311],\n",
            "        [ 1.1430, -0.7720],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5090, -2.5548],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1492,  2.2314],\n",
            "        [ 1.1430, -0.7719],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5092, -2.5550],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1495,  2.2317],\n",
            "        [ 1.1430, -0.7719],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5095, -2.5553],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1498,  2.2320],\n",
            "        [ 1.1431, -0.7718],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5097, -2.5555],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1501,  2.2323],\n",
            "        [ 1.1431, -0.7717],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5099, -2.5558],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1504,  2.2327],\n",
            "        [ 1.1432, -0.7717],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5102, -2.5560],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1507,  2.2330],\n",
            "        [ 1.1432, -0.7716],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5104, -2.5563],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1510,  2.2333],\n",
            "        [ 1.1432, -0.7715],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5106, -2.5565],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1513,  2.2336],\n",
            "        [ 1.1433, -0.7715],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5109, -2.5568],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1516,  2.2339],\n",
            "        [ 1.1433, -0.7714],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5111, -2.5570],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1518,  2.2342],\n",
            "        [ 1.1434, -0.7714],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5114, -2.5573],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1521,  2.2345],\n",
            "        [ 1.1434, -0.7713],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5116, -2.5575],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1524,  2.2348],\n",
            "        [ 1.1434, -0.7712],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5118, -2.5578],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1527,  2.2351],\n",
            "        [ 1.1435, -0.7712],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5121, -2.5580],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1530,  2.2354],\n",
            "        [ 1.1435, -0.7711],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5123, -2.5583],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1533,  2.2357],\n",
            "        [ 1.1436, -0.7711],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5125, -2.5585],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1536,  2.2360],\n",
            "        [ 1.1436, -0.7710],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5128, -2.5588],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1539,  2.2363],\n",
            "        [ 1.1436, -0.7709],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5130, -2.5590],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1542,  2.2366],\n",
            "        [ 1.1437, -0.7709],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5132, -2.5593],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1545,  2.2369],\n",
            "        [ 1.1437, -0.7708],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5135, -2.5595],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1547,  2.2372],\n",
            "        [ 1.1437, -0.7707],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5137, -2.5598],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1550,  2.2375],\n",
            "        [ 1.1438, -0.7707],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5140, -2.5600],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1553,  2.2378],\n",
            "        [ 1.1438, -0.7706],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5142, -2.5603],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1556,  2.2381],\n",
            "        [ 1.1439, -0.7706],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5144, -2.5605],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1559,  2.2384],\n",
            "        [ 1.1439, -0.7705],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5147, -2.5607],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1562,  2.2387],\n",
            "        [ 1.1439, -0.7704],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5149, -2.5610],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1565,  2.2390],\n",
            "        [ 1.1440, -0.7704],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5151, -2.5612],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1568,  2.2393],\n",
            "        [ 1.1440, -0.7703],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5154, -2.5615],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1570,  2.2396],\n",
            "        [ 1.1441, -0.7703],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5156, -2.5617],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1573,  2.2399],\n",
            "        [ 1.1441, -0.7702],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5158, -2.5620],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1576,  2.2402],\n",
            "        [ 1.1441, -0.7701],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5161, -2.5622],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1579,  2.2406],\n",
            "        [ 1.1442, -0.7701],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5163, -2.5625],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1582,  2.2409],\n",
            "        [ 1.1442, -0.7700],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5165, -2.5627],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1585,  2.2412],\n",
            "        [ 1.1442, -0.7699],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5168, -2.5630],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1588,  2.2415],\n",
            "        [ 1.1443, -0.7699],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5170, -2.5632],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1591,  2.2418],\n",
            "        [ 1.1443, -0.7698],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5172, -2.5635],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1593,  2.2421],\n",
            "        [ 1.1444, -0.7698],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5175, -2.5637],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1596,  2.2424],\n",
            "        [ 1.1444, -0.7697],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5177, -2.5639],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1599,  2.2427],\n",
            "        [ 1.1444, -0.7696],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5179, -2.5642],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1602,  2.2430],\n",
            "        [ 1.1445, -0.7696],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5182, -2.5644],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1605,  2.2433],\n",
            "        [ 1.1445, -0.7695],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5184, -2.5647],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1608,  2.2435],\n",
            "        [ 1.1446, -0.7695],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5186, -2.5649],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1611,  2.2438],\n",
            "        [ 1.1446, -0.7694],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5189, -2.5652],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1613,  2.2441],\n",
            "        [ 1.1446, -0.7693],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5191, -2.5654],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1616,  2.2444],\n",
            "        [ 1.1447, -0.7693],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5193, -2.5657],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1619,  2.2447],\n",
            "        [ 1.1447, -0.7692],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5196, -2.5659],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1622,  2.2450],\n",
            "        [ 1.1447, -0.7692],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5198, -2.5661],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1625,  2.2453],\n",
            "        [ 1.1448, -0.7691],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5200, -2.5664],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1628,  2.2456],\n",
            "        [ 1.1448, -0.7690],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5203, -2.5666],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1630,  2.2459],\n",
            "        [ 1.1449, -0.7690],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5205, -2.5669],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1633,  2.2462],\n",
            "        [ 1.1449, -0.7689],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5207, -2.5671],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1636,  2.2465],\n",
            "        [ 1.1449, -0.7688],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5210, -2.5674],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1639,  2.2468],\n",
            "        [ 1.1450, -0.7688],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5212, -2.5676],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1642,  2.2471],\n",
            "        [ 1.1450, -0.7687],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5214, -2.5678],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1645,  2.2474],\n",
            "        [ 1.1450, -0.7687],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5216, -2.5681],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1647,  2.2477],\n",
            "        [ 1.1451, -0.7686],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5219, -2.5683],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1650,  2.2480],\n",
            "        [ 1.1451, -0.7685],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5221, -2.5686],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1653,  2.2483],\n",
            "        [ 1.1452, -0.7685],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5223, -2.5688],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1656,  2.2486],\n",
            "        [ 1.1452, -0.7684],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5226, -2.5691],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1659,  2.2489],\n",
            "        [ 1.1452, -0.7684],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5228, -2.5693],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1661,  2.2492],\n",
            "        [ 1.1453, -0.7683],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5230, -2.5695],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1664,  2.2495],\n",
            "        [ 1.1453, -0.7682],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5233, -2.5698],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1667,  2.2498],\n",
            "        [ 1.1454, -0.7682],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5235, -2.5700],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1670,  2.2501],\n",
            "        [ 1.1454, -0.7681],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5237, -2.5703],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1673,  2.2504],\n",
            "        [ 1.1454, -0.7681],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5240, -2.5705],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1675,  2.2507],\n",
            "        [ 1.1455, -0.7680],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5242, -2.5707],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1678,  2.2510],\n",
            "        [ 1.1455, -0.7679],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5244, -2.5710],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1681,  2.2512],\n",
            "        [ 1.1455, -0.7679],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5246, -2.5712],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1684,  2.2515],\n",
            "        [ 1.1456, -0.7678],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5249, -2.5715],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1687,  2.2518],\n",
            "        [ 1.1456, -0.7678],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5251, -2.5717],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1689,  2.2521],\n",
            "        [ 1.1457, -0.7677],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5253, -2.5719],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1692,  2.2524],\n",
            "        [ 1.1457, -0.7676],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5256, -2.5722],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1695,  2.2527],\n",
            "        [ 1.1457, -0.7676],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5258, -2.5724],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1698,  2.2530],\n",
            "        [ 1.1458, -0.7675],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5260, -2.5727],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1701,  2.2533],\n",
            "        [ 1.1458, -0.7675],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5262, -2.5729],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1703,  2.2536],\n",
            "        [ 1.1458, -0.7674],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5265, -2.5731],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1706,  2.2539],\n",
            "        [ 1.1459, -0.7673],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5267, -2.5734],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1709,  2.2542],\n",
            "        [ 1.1459, -0.7673],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5269, -2.5736],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1712,  2.2545],\n",
            "        [ 1.1460, -0.7672],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5272, -2.5739],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1714,  2.2548],\n",
            "        [ 1.1460, -0.7672],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5274, -2.5741],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1717,  2.2550],\n",
            "        [ 1.1460, -0.7671],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5276, -2.5743],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1720,  2.2553],\n",
            "        [ 1.1461, -0.7670],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5278, -2.5746],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1723,  2.2556],\n",
            "        [ 1.1461, -0.7670],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5281, -2.5748],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1725,  2.2559],\n",
            "        [ 1.1461, -0.7669],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5283, -2.5751],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1728,  2.2562],\n",
            "        [ 1.1462, -0.7669],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5285, -2.5753],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1731,  2.2565],\n",
            "        [ 1.1462, -0.7668],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5287, -2.5755],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1734,  2.2568],\n",
            "        [ 1.1463, -0.7667],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5290, -2.5758],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1737,  2.2571],\n",
            "        [ 1.1463, -0.7667],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5292, -2.5760],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1739,  2.2574],\n",
            "        [ 1.1463, -0.7666],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5294, -2.5762],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1742,  2.2577],\n",
            "        [ 1.1464, -0.7666],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5297, -2.5765],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1745,  2.2579],\n",
            "        [ 1.1464, -0.7665],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5299, -2.5767],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1748,  2.2582],\n",
            "        [ 1.1464, -0.7664],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5301, -2.5770],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1750,  2.2585],\n",
            "        [ 1.1465, -0.7664],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5303, -2.5772],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1753,  2.2588],\n",
            "        [ 1.1465, -0.7663],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5306, -2.5774],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1756,  2.2591],\n",
            "        [ 1.1466, -0.7663],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5308, -2.5777],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1758,  2.2594],\n",
            "        [ 1.1466, -0.7662],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5310, -2.5779],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1761,  2.2597],\n",
            "        [ 1.1466, -0.7661],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5312, -2.5781],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1764,  2.2600],\n",
            "        [ 1.1467, -0.7661],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5315, -2.5784],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1767,  2.2602],\n",
            "        [ 1.1467, -0.7660],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5317, -2.5786],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1769,  2.2605],\n",
            "        [ 1.1467, -0.7660],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5319, -2.5789],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1772,  2.2608],\n",
            "        [ 1.1468, -0.7659],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5321, -2.5791],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1775,  2.2611],\n",
            "        [ 1.1468, -0.7659],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5324, -2.5793],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1778,  2.2614],\n",
            "        [ 1.1468, -0.7658],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5326, -2.5796],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1780,  2.2617],\n",
            "        [ 1.1469, -0.7657],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5328, -2.5798],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1783,  2.2620],\n",
            "        [ 1.1469, -0.7657],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5330, -2.5800],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1786,  2.2622],\n",
            "        [ 1.1470, -0.7656],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5333, -2.5803],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1789,  2.2625],\n",
            "        [ 1.1470, -0.7656],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5335, -2.5805],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1791,  2.2628],\n",
            "        [ 1.1470, -0.7655],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5337, -2.5807],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1794,  2.2631],\n",
            "        [ 1.1471, -0.7654],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5339, -2.5810],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1797,  2.2634],\n",
            "        [ 1.1471, -0.7654],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5342, -2.5812],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1799,  2.2637],\n",
            "        [ 1.1471, -0.7653],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5344, -2.5814],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1802,  2.2640],\n",
            "        [ 1.1472, -0.7653],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5346, -2.5817],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1805,  2.2642],\n",
            "        [ 1.1472, -0.7652],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5348, -2.5819],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1808,  2.2645],\n",
            "        [ 1.1473, -0.7651],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5351, -2.5822],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1810,  2.2648],\n",
            "        [ 1.1473, -0.7651],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5353, -2.5824],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1813,  2.2651],\n",
            "        [ 1.1473, -0.7650],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5355, -2.5826],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1816,  2.2654],\n",
            "        [ 1.1474, -0.7650],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5357, -2.5829],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1818,  2.2657],\n",
            "        [ 1.1474, -0.7649],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5359, -2.5831],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1821,  2.2659],\n",
            "        [ 1.1474, -0.7648],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5362, -2.5833],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1824,  2.2662],\n",
            "        [ 1.1475, -0.7648],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5364, -2.5836],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1826,  2.2665],\n",
            "        [ 1.1475, -0.7647],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5366, -2.5838],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1829,  2.2668],\n",
            "        [ 1.1476, -0.7647],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5368, -2.5840],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1832,  2.2671],\n",
            "        [ 1.1476, -0.7646],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5371, -2.5843],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1835,  2.2674],\n",
            "        [ 1.1476, -0.7646],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5373, -2.5845],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1837,  2.2676],\n",
            "        [ 1.1477, -0.7645],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5375, -2.5847],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1840,  2.2679],\n",
            "        [ 1.1477, -0.7644],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5377, -2.5850],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1843,  2.2682],\n",
            "        [ 1.1477, -0.7644],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5379, -2.5852],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1845,  2.2685],\n",
            "        [ 1.1478, -0.7643],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5382, -2.5854],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1848,  2.2688],\n",
            "        [ 1.1478, -0.7643],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5384, -2.5857],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1851,  2.2690],\n",
            "        [ 1.1478, -0.7642],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5386, -2.5859],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1853,  2.2693],\n",
            "        [ 1.1479, -0.7641],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5388, -2.5861],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1856,  2.2696],\n",
            "        [ 1.1479, -0.7641],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5390, -2.5863],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1859,  2.2699],\n",
            "        [ 1.1480, -0.7640],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5393, -2.5866],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1861,  2.2702],\n",
            "        [ 1.1480, -0.7640],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5395, -2.5868],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1864,  2.2705],\n",
            "        [ 1.1480, -0.7639],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5397, -2.5870],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1867,  2.2707],\n",
            "        [ 1.1481, -0.7639],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5399, -2.5873],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1869,  2.2710],\n",
            "        [ 1.1481, -0.7638],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5402, -2.5875],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1872,  2.2713],\n",
            "        [ 1.1481, -0.7637],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5404, -2.5877],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1875,  2.2716],\n",
            "        [ 1.1482, -0.7637],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5406, -2.5880],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1877,  2.2719],\n",
            "        [ 1.1482, -0.7636],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5408, -2.5882],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1880,  2.2721],\n",
            "        [ 1.1482, -0.7636],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5410, -2.5884],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1883,  2.2724],\n",
            "        [ 1.1483, -0.7635],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5413, -2.5887],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1885,  2.2727],\n",
            "        [ 1.1483, -0.7634],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5415, -2.5889],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1888,  2.2730],\n",
            "        [ 1.1484, -0.7634],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5417, -2.5891],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1891,  2.2732],\n",
            "        [ 1.1484, -0.7633],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5419, -2.5894],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1893,  2.2735],\n",
            "        [ 1.1484, -0.7633],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5421, -2.5896],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1896,  2.2738],\n",
            "        [ 1.1485, -0.7632],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5424, -2.5898],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1899,  2.2741],\n",
            "        [ 1.1485, -0.7632],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5426, -2.5900],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1901,  2.2744],\n",
            "        [ 1.1485, -0.7631],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5428, -2.5903],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1904,  2.2746],\n",
            "        [ 1.1486, -0.7630],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5430, -2.5905],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1906,  2.2749],\n",
            "        [ 1.1486, -0.7630],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5432, -2.5907],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1909,  2.2752],\n",
            "        [ 1.1486, -0.7629],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5434, -2.5910],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1912,  2.2755],\n",
            "        [ 1.1487, -0.7629],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5437, -2.5912],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1914,  2.2757],\n",
            "        [ 1.1487, -0.7628],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5439, -2.5914],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1917,  2.2760],\n",
            "        [ 1.1488, -0.7628],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5441, -2.5917],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1920,  2.2763],\n",
            "        [ 1.1488, -0.7627],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5443, -2.5919],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1922,  2.2766],\n",
            "        [ 1.1488, -0.7626],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5445, -2.5921],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1925,  2.2768],\n",
            "        [ 1.1489, -0.7626],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5448, -2.5923],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1928,  2.2771],\n",
            "        [ 1.1489, -0.7625],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5450, -2.5926],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1930,  2.2774],\n",
            "        [ 1.1489, -0.7625],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5452, -2.5928],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1933,  2.2777],\n",
            "        [ 1.1490, -0.7624],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5454, -2.5930],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1935,  2.2779],\n",
            "        [ 1.1490, -0.7624],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5456, -2.5933],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1938,  2.2782],\n",
            "        [ 1.1490, -0.7623],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5458, -2.5935],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1941,  2.2785],\n",
            "        [ 1.1491, -0.7622],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5461, -2.5937],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1943,  2.2788],\n",
            "        [ 1.1491, -0.7622],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5463, -2.5939],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1946,  2.2790],\n",
            "        [ 1.1492, -0.7621],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5465, -2.5942],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1948,  2.2793],\n",
            "        [ 1.1492, -0.7621],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5467, -2.5944],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1951,  2.2796],\n",
            "        [ 1.1492, -0.7620],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5469, -2.5946],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1954,  2.2799],\n",
            "        [ 1.1493, -0.7619],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5471, -2.5948],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1956,  2.2801],\n",
            "        [ 1.1493, -0.7619],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5474, -2.5951],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1959,  2.2804],\n",
            "        [ 1.1493, -0.7618],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5476, -2.5953],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1962,  2.2807],\n",
            "        [ 1.1494, -0.7618],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5478, -2.5955],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1964,  2.2810],\n",
            "        [ 1.1494, -0.7617],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5480, -2.5958],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1967,  2.2812],\n",
            "        [ 1.1494, -0.7617],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5482, -2.5960],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1969,  2.2815],\n",
            "        [ 1.1495, -0.7616],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5484, -2.5962],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1972,  2.2818],\n",
            "        [ 1.1495, -0.7615],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5487, -2.5964],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1975,  2.2821],\n",
            "        [ 1.1495, -0.7615],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5489, -2.5967],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1977,  2.2823],\n",
            "        [ 1.1496, -0.7614],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5491, -2.5969],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1980,  2.2826],\n",
            "        [ 1.1496, -0.7614],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5493, -2.5971],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1982,  2.2829],\n",
            "        [ 1.1497, -0.7613],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5495, -2.5973],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1985,  2.2831],\n",
            "        [ 1.1497, -0.7613],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5497, -2.5976],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1987,  2.2834],\n",
            "        [ 1.1497, -0.7612],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5500, -2.5978],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1990,  2.2837],\n",
            "        [ 1.1498, -0.7611],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5502, -2.5980],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1993,  2.2840],\n",
            "        [ 1.1498, -0.7611],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5504, -2.5982],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1995,  2.2842],\n",
            "        [ 1.1498, -0.7610],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5506, -2.5985],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.1998,  2.2845],\n",
            "        [ 1.1499, -0.7610],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5508, -2.5987],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2000,  2.2848],\n",
            "        [ 1.1499, -0.7609],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5510, -2.5989],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2003,  2.2850],\n",
            "        [ 1.1499, -0.7609],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5512, -2.5991],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2006,  2.2853],\n",
            "        [ 1.1500, -0.7608],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5515, -2.5994],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2008,  2.2856],\n",
            "        [ 1.1500, -0.7608],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5517, -2.5996],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2011,  2.2859],\n",
            "        [ 1.1500, -0.7607],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5519, -2.5998],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2013,  2.2861],\n",
            "        [ 1.1501, -0.7606],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5521, -2.6000],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2016,  2.2864],\n",
            "        [ 1.1501, -0.7606],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5523, -2.6003],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2018,  2.2867],\n",
            "        [ 1.1502, -0.7605],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5525, -2.6005],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2021,  2.2869],\n",
            "        [ 1.1502, -0.7605],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5527, -2.6007],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2024,  2.2872],\n",
            "        [ 1.1502, -0.7604],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5530, -2.6009],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2026,  2.2875],\n",
            "        [ 1.1503, -0.7604],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5532, -2.6012],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2029,  2.2877],\n",
            "        [ 1.1503, -0.7603],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5534, -2.6014],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2031,  2.2880],\n",
            "        [ 1.1503, -0.7602],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5536, -2.6016],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2034,  2.2883],\n",
            "        [ 1.1504, -0.7602],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5538, -2.6018],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2036,  2.2885],\n",
            "        [ 1.1504, -0.7601],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5540, -2.6021],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2039,  2.2888],\n",
            "        [ 1.1504, -0.7601],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5542, -2.6023],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2041,  2.2891],\n",
            "        [ 1.1505, -0.7600],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5544, -2.6025],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2044,  2.2893],\n",
            "        [ 1.1505, -0.7600],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "before tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5547, -2.6027],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2047,  2.2896],\n",
            "        [ 1.1505, -0.7599],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor(0.0010, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "Gradient norm for W_0: 0.019236825903563848\n",
            "Gradient norm for b: 0.0006579592084753721\n",
            "Gradient norm for V_0: 0.2777855909850183\n",
            "Gradient norm for c: 0.15941364973995825\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_raw)\n",
        "print(trap_X)\n",
        "hessian_matrix_trap, eigenvalues_trap = compute_hessian_and_eigenvalues(nn_model_trap, trap_X, Y)\n",
        "\n",
        "print(eigenvalues_trap)\n",
        "check_local_minimum(eigenvalues_trap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0CYBTk5J-Fu",
        "outputId": "2bc038e3-d1d9-4050-aca8-4c1e48cecade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0000],\n",
            "        [-1.0000, -1.0000],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5000, -4.0000],\n",
            "        [-1.5000, -0.5000],\n",
            "        [ 1.0000, -1.0000],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([[-4.0000, -4.0000],\n",
            "        [-4.0000, -1.0001],\n",
            "        [ 0.5549, -2.6030],\n",
            "        [-1.0000,  2.0000],\n",
            "        [ 2.0000,  2.0000],\n",
            "        [-3.0000, -4.0000],\n",
            "        [-1.5001, -4.0000],\n",
            "        [-4.2049,  2.2899],\n",
            "        [ 1.1506, -0.7598],\n",
            "        [ 1.5000,  2.5000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([ 2.2362e-01+0.j,  4.8039e-02+0.j, -4.8185e-02+0.j, -6.2189e-02+0.j,\n",
            "        -8.2112e-03+0.j, -1.0625e-02+0.j, -2.0603e-07+0.j,  9.2078e-05+0.j,\n",
            "        -1.8258e-04+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_data_with_gradients(X_raw, Y, W_0, b, V_0, c, max_iterations=20, learning_rate=0.1, MC_num_samples=100, surrounding_proportion=0.5, max_deviation_for_weight=0.05, threshold=0.001):\n",
        "    \"\"\"\n",
        "    Optimize data using gradient calculations and Monte Carlo method.\n",
        "\n",
        "    :param nn_model: Neural Network Model class.\n",
        "    :param X_raw: Input data.\n",
        "    :param Y: Target data.\n",
        "    :param W_0, b, V_0, c: Initial weights and biases for the neural network.\n",
        "    :param max_iterations: Maximum number of iterations.\n",
        "    :param learning_rate: Learning rate for optimization.\n",
        "    :param MC_num_samples: Number of samples for Monte Carlo method.\n",
        "    :param surrounding_proportion: Proportion of surrounding points' gradients.\n",
        "    :param max_deviation_for_weight: Maximum deviation for weight perturbation.\n",
        "    :param threshold: Threshold for the norm of the second-order gradient.\n",
        "    :return: Optimized X_raw tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_raw_tensor = X_raw.clone().detach().requires_grad_(True) if X_raw.requires_grad else torch.tensor(X_raw, dtype=torch.float64, requires_grad=True)\n",
        "    Y_tensor = Y.clone().detach().requires_grad_(True) if Y.requires_grad else torch.tensor(Y, dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "\n",
        "    # Initialize the neural network with provided weights\n",
        "    nn_model_sur = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "    # Store original weights\n",
        "    original_weights = {\n",
        "        'W_0': nn_model_sur.W_0.data.clone(),\n",
        "        'b': nn_model_sur.b.data.clone(),\n",
        "        'V_0': nn_model_sur.V_0.data.clone(),\n",
        "        'c': nn_model_sur.c.data.clone()\n",
        "    }\n",
        "    print(\"Original weight is {}\".format(original_weights))\n",
        "    print(\"Initial X_raw_pre {}\".format(X_raw_tensor))\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # [Insert the existing logic of your loop here, using nn_model_instance, X_raw_tensor, Y_tensor, and other parameters]\n",
        "        # Calculate the gradient at the central point\n",
        "        central_grad = calculate_second_order_grad(nn_model_sur, X_raw_tensor, Y_tensor)\n",
        "        # Check if grad_X is None before proceeding\n",
        "        central_grad_norm = torch.norm(central_grad)\n",
        "        central_grad = central_grad / central_grad_norm\n",
        "        #print(central_grad)\n",
        "        # Surrouning points' grads\n",
        "        surrounding_grads_pre = []\n",
        "        norms_pre = []\n",
        "\n",
        "\n",
        "        # Calculate the gradient at the surrounding points by MC\n",
        "        for _ in range(MC_num_samples):\n",
        "\n",
        "            nn_model_sample_pre = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "            #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "            # Perturb weights\n",
        "            #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "            perturb_weights_uniform_fixed_range(nn_model_sample_pre, scale = max_deviation_for_weight)\n",
        "            #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "            grad_pre = calculate_second_order_grad(nn_model_sample_pre, X_raw_tensor, Y_tensor)\n",
        "            grad_norm = torch.norm(grad_pre)\n",
        "            grad_pre = grad_pre / grad_norm\n",
        "            surrounding_grads_pre.append(grad_pre)\n",
        "            #negative_eigenvalues.append(torch.norm(grad_pre).item())\n",
        "\n",
        "        sum_surrounding_grads_pre = sum(surrounding_grads_pre)\n",
        "\n",
        "        # Average the large norm gradients\n",
        "        average_surrounding_grads_pre = sum_surrounding_grads_pre / len(surrounding_grads_pre)\n",
        "        # Calculate average pre_norm\n",
        "        #average_negative_eigenvalues = sum(negative_eigenvalues) / len(negative_eigenvalues)\n",
        "\n",
        "        # Calculate the median of negative eigenvalues\n",
        "        #median_negative_eigenvalue = np.median([eigenvalue for eigenvalue in negative_eigenvalues if eigenvalue < 0])\n",
        "\n",
        "\n",
        "        # Filter gradients corresponding to the smallest 50% of negative eigenvalues\n",
        "        #above_average_negative_eigenvalues = [grad for grad, eigenvalue in zip(surrounding_grads_pre, negative_eigenvalues) if eigenvalue < median_negative_eigenvalue]\n",
        "        #above_average_negative_eigenvalues = [grad / torch.norm(grad) for grad in above_average_negative_eigenvalues]\n",
        "\n",
        "        #print(above_average_grads)\n",
        "        #sum_above_average_negative_eigenvalues = sum(above_average_negative_eigenvalues)\n",
        "\n",
        "        # Average the large norm gradients\n",
        "        #if above_average_negative_eigenvalues:\n",
        "          #average_above_average_negative_eigenvalues = sum_above_average_negative_eigenvalues / len(above_average_negative_eigenvalues)\n",
        "          #print(average_above_average_grad)\n",
        "        #else:\n",
        "          # Handle the case where no gradient is above average\n",
        "          #average_above_average_negative_eigenvalues = torch.zeros_like(X_raw_tensor)\n",
        "\n",
        "\n",
        "        #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "        # Combine gradients\n",
        "        combined_grad = (1-surrounding_proportion) * central_grad + surrounding_proportion * average_surrounding_grads_pre\n",
        "        #combined_grad =  average_surrounding_grad\n",
        "        #print(combined_grad)\n",
        "        # Calculate the norm of the combined gradient\n",
        "        combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "        # Check for a non-zero norm to avoid division by zero\n",
        "        if combined_grad_norm > 0:\n",
        "        # Normalize the gradient\n",
        "          normalized_grad = combined_grad / combined_grad_norm\n",
        "\n",
        "        else:\n",
        "          print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "        # Check if the norm of the second-order gradient is below the threshold\n",
        "        if torch.norm(combined_grad) < threshold:\n",
        "            print(f\"Convergence reached at iteration {i}\")\n",
        "            break\n",
        "        # Update X_raw using the normalized gradient and learning rate\n",
        "        X_raw_tensor.data -= learning_rate * normalized_grad\n",
        "\n",
        "        # Zero out gradients for the next iteration\n",
        "        nn_model_sur.zero_grad()\n",
        "        X_raw_tensor.grad = None\n",
        "        # Update and checks as per your original code\n",
        "\n",
        "\n",
        "\n",
        "    # Print final modified data\n",
        "    #print(surrounding_grads)\n",
        "    #print(\"Final modified X_raw:\")\n",
        "    #if len(above_average_negative_eigenvalues) < 0.5*MC_pre_num_samples:\n",
        "      #print(\"need more MC_pre_num_samples\")\n",
        "    #else:\n",
        "    #print(\"Used surrounding points: {}\".format(len(above_average_negative_eigenvalues)))\n",
        "\n",
        "    print(\"Output X is: {}\".format(X_raw_tensor))\n",
        "    #print(negative_eigenvalues)\n",
        "\n",
        "\n",
        "\n",
        "    # Return the optimized X_raw tensor\n",
        "    return X_raw_tensor\n",
        "\n",
        "# Example usage\n",
        "optimized_X_2 = optimize_data_with_gradients(trap_X, Y, W_0, b, V_0, c, max_iterations=50, learning_rate=0.1, MC_num_samples=100, surrounding_proportion=0.3, max_deviation_for_weight=0.01, threshold=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ug5G6aLkors",
        "outputId": "b59b5df0-4436-4512-c39f-90e4814308fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-5816f8b1477c>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_tensor = Y.clone().detach().requires_grad_(True) if Y.requires_grad else torch.tensor(Y, dtype=torch.float64, requires_grad=True)\n",
            "<ipython-input-3-2e4822dd5dc3>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-3-2e4822dd5dc3>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-3-2e4822dd5dc3>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-3-2e4822dd5dc3>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight is {'W_0': tensor([[-0.9854, -7.0746],\n",
            "        [-0.7950, -5.9667]], dtype=torch.float64), 'b': tensor([[2.9537, 2.3977]], dtype=torch.float64), 'V_0': tensor([[-8.6610],\n",
            "        [-6.9776]], dtype=torch.float64), 'c': tensor([[6.6996]], dtype=torch.float64)}\n",
            "Initial X_raw_pre tensor([[ 0.1222,  0.1974],\n",
            "        [ 0.6699,  1.4908],\n",
            "        [ 0.1828,  0.1296],\n",
            "        [-3.1624, -3.5892],\n",
            "        [ 0.2212,  0.0867],\n",
            "        [ 0.2715, -0.6380],\n",
            "        [ 0.7825,  1.3818],\n",
            "        [ 1.3966,  3.5302],\n",
            "        [ 0.2329,  0.0736],\n",
            "        [ 2.6141, -0.3701]], dtype=torch.float64, requires_grad=True)\n",
            "Output X is: tensor([[-0.1250, -0.0626],\n",
            "        [ 0.2896,  1.2047],\n",
            "        [-0.0676, -0.1333],\n",
            "        [-3.1624, -3.5891],\n",
            "        [-0.0313, -0.1780],\n",
            "        [ 0.2775, -0.6330],\n",
            "        [ 0.3866,  1.0816],\n",
            "        [ 4.3140,  5.7800],\n",
            "        [-0.0203, -0.1916],\n",
            "        [ 1.9599, -0.9091]], dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model_X2 = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_surrounding, eigenvalues_surrounding = compute_hessian_and_eigenvalues(nn_model_X2, optimized_X_2, Y)\n",
        "\n",
        "print(eigenvalues_surrounding)\n",
        "check_local_minimum(eigenvalues_surrounding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4_An8vDPjyE",
        "outputId": "20e0a19c-c0ae-4875-9371-0d09ca831f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.3254+0.j,  0.2093+0.j, -0.1277+0.j, -0.0679+0.j,  0.0189+0.j,  0.0015+0.j,\n",
            "        -0.0028+0.j, -0.0007+0.j, -0.0010+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_X_2 = optimize_data_with_gradients(optimized_X_1, Y, W_0, b, V_0, c, max_iterations=20, learning_rate=0.05, MC_num_samples=100, surrounding_proportion=0.9, max_deviation_for_weight=0.02, threshold=0.001)"
      ],
      "metadata": {
        "id": "mi-VQ5D7R535",
        "outputId": "7105b639-e3a4-4236-cfde-d2a8e4470466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1e9be29dee3d>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_tensor = torch.tensor(X_raw, requires_grad=True)\n",
            "<ipython-input-19-1e9be29dee3d>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_tensor = torch.tensor(Y)\n",
            "<ipython-input-3-a815e9696fa5>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight is {'W_0': tensor([[ 3.9979,  5.2638],\n",
            "        [ 1.0698, -1.8901]], dtype=torch.float64), 'b': tensor([[1.8545, 0.8022]], dtype=torch.float64), 'V_0': tensor([[-10.1389],\n",
            "        [-10.4242]], dtype=torch.float64), 'c': tensor([[5.0116]], dtype=torch.float64)}\n",
            "Initial X_raw_pre tensor([[11.0898, -7.6508],\n",
            "        [ 1.6931, -6.0680],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5432, -6.2103],\n",
            "        [ 1.1082,  0.1752],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-1.3351,  2.2657],\n",
            "        [-3.4568,  2.3168]], dtype=torch.float64, requires_grad=True)\n",
            "Used surrounding points: 50\n",
            "Output X is: tensor([[11.0898, -7.6508],\n",
            "        [ 1.6931, -6.0680],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5432, -6.2103],\n",
            "        [ 1.1082,  0.1752],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-1.3568,  2.0730],\n",
            "        [-3.4338,  2.3244]], dtype=torch.float64, requires_grad=True)\n",
            "[tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0002, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_X_3 = optimize_data_with_gradients(optimized_X_2, Y, W_0, b, V_0, c, max_iterations=20, learning_rate=0.02, MC_num_samples=100, surrounding_proportion=0.9, max_deviation_for_weight=0.01, threshold=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7uQAynJwxIu",
        "outputId": "f67d55c1-bd3b-4311-b085-90588d7c7964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1e9be29dee3d>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_tensor = torch.tensor(X_raw, requires_grad=True)\n",
            "<ipython-input-19-1e9be29dee3d>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_tensor = torch.tensor(Y)\n",
            "<ipython-input-3-a815e9696fa5>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-3-a815e9696fa5>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight is {'W_0': tensor([[ 3.9979,  5.2638],\n",
            "        [ 1.0698, -1.8901]], dtype=torch.float64), 'b': tensor([[1.8545, 0.8022]], dtype=torch.float64), 'V_0': tensor([[-10.1389],\n",
            "        [-10.4242]], dtype=torch.float64), 'c': tensor([[5.0116]], dtype=torch.float64)}\n",
            "Initial X_raw_pre tensor([[11.0898, -7.6508],\n",
            "        [ 1.6931, -6.0680],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5432, -6.2103],\n",
            "        [ 1.1082,  0.1752],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-1.3568,  2.0730],\n",
            "        [-3.4338,  2.3244]], dtype=torch.float64, requires_grad=True)\n",
            "Used surrounding points: 50\n",
            "Output X is: tensor([[11.0898, -7.6508],\n",
            "        [ 1.6932, -6.0680],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5432, -6.2103],\n",
            "        [ 1.1082,  0.1752],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-1.2705,  1.9455],\n",
            "        [-3.4149,  2.3306]], dtype=torch.float64, requires_grad=True)\n",
            "[tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64), tensor(-0.0003, dtype=torch.float64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model_pre = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_pre, eigenvalues_pre = compute_hessian_and_eigenvalues(nn_model_pre, optimized_X_3, Y)\n",
        "\n",
        "print(eigenvalues_pre)\n",
        "check_local_minimum(eigenvalues_pre)"
      ],
      "metadata": {
        "id": "PRbJ8sIWR1ge",
        "outputId": "73e82f59-33e2-4458-ad9c-39c4afa3ba9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.9566e-01+0.j,  4.0277e-03+0.j, -2.7943e-04+0.j, -2.2500e-04+0.j,\n",
            "         3.3531e-05+0.j, -4.1906e-06+0.j,  1.2490e-06+0.j,  4.5318e-08+0.j,\n",
            "         2.1484e-10+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## use the eigenvalues of hessian to decide whether use the grad.\n",
        "\n",
        "def flatten_gradients(X_raw, Y, W_0, b, V_0, c, max_iterations=20, learning_rate=0.1, MC_num_samples=100, surrounding_proportion=0.5, max_deviation_for_weight=0.05, threshold=0.001):\n",
        "    \"\"\"\n",
        "    Optimize data using gradient calculations and Monte Carlo method.\n",
        "\n",
        "    :param nn_model: Neural Network Model class.\n",
        "    :param X_raw: Input data.\n",
        "    :param Y: Target data.\n",
        "    :param W_0, b, V_0, c: Initial weights and biases for the neural network.\n",
        "    :param max_iterations: Maximum number of iterations.\n",
        "    :param learning_rate: Learning rate for optimization.\n",
        "    :param MC_num_samples: Number of samples for Monte Carlo method.\n",
        "    :param surrounding_proportion: Proportion of surrounding points' gradients.\n",
        "    :param max_deviation_for_weight: Maximum deviation for weight perturbation.\n",
        "    :param threshold: Threshold for the norm of the second-order gradient.\n",
        "    :return: Optimized X_raw tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_raw_tensor = torch.tensor(X_raw, requires_grad=True)\n",
        "    Y_tensor = torch.tensor(Y)\n",
        "\n",
        "    # Initialize the neural network with provided weights\n",
        "    nn_model_instance = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "    # Store original weights\n",
        "    original_weights = {\n",
        "        'W_0': nn_model_instance.W_0.data.clone(),\n",
        "        'b': nn_model_instance.b.data.clone(),\n",
        "        'V_0': nn_model_instance.V_0.data.clone(),\n",
        "        'c': nn_model_instance.c.data.clone()\n",
        "    }\n",
        "    print(\"Original weight is {}\".format(original_weights))\n",
        "    print(\"Initial X_raw_pre {}\".format(X_raw_tensor))\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # [Insert the existing logic of your loop here, using nn_model_instance, X_raw_tensor, Y_tensor, and other parameters]\n",
        "        # Calculate the gradient at the central point\n",
        "        central_grad = calculate_second_order_grad(nn_model_instance, X_raw_tensor, Y_tensor)\n",
        "        central_grad_norm = torch.norm(central_grad)\n",
        "        central_grad = central_grad / central_grad_norm\n",
        "        #print(central_grad)\n",
        "        # Surrouning points' grads\n",
        "        surrounding_grads_pre = []\n",
        "        norms_pre = []\n",
        "        negative_eigenvalues = []\n",
        "\n",
        "\n",
        "        # Calculate the gradient at the surrounding points by MC\n",
        "        for _ in range(MC_num_samples):\n",
        "\n",
        "            nn_model_sample_pre = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "            #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "            # Perturb weights\n",
        "            #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "            perturb_weights_uniform_fixed_range(nn_model_sample_pre, scale = max_deviation_for_weight)\n",
        "            #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "            _, eigenvalues = compute_hessian_and_eigenvalues(nn_model_sample_pre, X_raw_tensor, Y_tensor)\n",
        "            # Filter out negative eigenvalues (considering the real part)\n",
        "            negative = [e.real for e in eigenvalues if e.real < 0]\n",
        "            if negative:\n",
        "              most_negative_eigenvalue = min(negative)\n",
        "            else:\n",
        "              # Return None if there are no negative eigenvalues\n",
        "              print(\"FOUND A LOCAM MINIMUM!\")\n",
        "              print(\"FOUND A LOCAM MINIMUM!at:{}\".format(X_raw_tensor))\n",
        "            negative_eigenvalues.append(most_negative_eigenvalue)\n",
        "            negative = []\n",
        "            grad_pre = calculate_second_order_grad(nn_model_sample_pre, X_raw_tensor, Y_tensor)\n",
        "            #grad_norm = torch.norm(grad)\n",
        "            #grad = grad / grad_norm\n",
        "            surrounding_grads_pre.append(grad_pre)\n",
        "            #negative_eigenvalues.append(torch.norm(grad_pre).item())\n",
        "\n",
        "\n",
        "        # Calculate average pre_norm\n",
        "        #average_negative_eigenvalues = sum(negative_eigenvalues) / len(negative_eigenvalues)\n",
        "\n",
        "        # Calculate the median of negative eigenvalues\n",
        "        median_negative_eigenvalue = np.median([eigenvalue for eigenvalue in negative_eigenvalues if eigenvalue < 0])\n",
        "\n",
        "\n",
        "        # Filter gradients corresponding to the smallest 50% of negative eigenvalues\n",
        "        above_average_negative_eigenvalues = [grad for grad, eigenvalue in zip(surrounding_grads_pre, negative_eigenvalues) if eigenvalue < median_negative_eigenvalue]\n",
        "        above_average_negative_eigenvalues = [grad / torch.norm(grad) for grad in above_average_negative_eigenvalues]\n",
        "\n",
        "        #print(above_average_grads)\n",
        "        sum_above_average_negative_eigenvalues = sum(above_average_negative_eigenvalues)\n",
        "\n",
        "        # Average the large norm gradients\n",
        "        if above_average_negative_eigenvalues:\n",
        "          average_above_average_negative_eigenvalues = sum_above_average_negative_eigenvalues / len(above_average_negative_eigenvalues)\n",
        "          #print(average_above_average_grad)\n",
        "        else:\n",
        "          # Handle the case where no gradient is above average\n",
        "          average_above_average_negative_eigenvalues = torch.zeros_like(X_raw_tensor)\n",
        "\n",
        "\n",
        "        #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "        # Combine gradients\n",
        "        combined_grad = (1-surrounding_proportion) * central_grad + surrounding_proportion * average_above_average_negative_eigenvalues\n",
        "        #combined_grad =  average_surrounding_grad\n",
        "        #print(combined_grad)\n",
        "        # Calculate the norm of the combined gradient\n",
        "        combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "        # Check for a non-zero norm to avoid division by zero\n",
        "        if combined_grad_norm > 0:\n",
        "        # Normalize the gradient\n",
        "          normalized_grad = combined_grad / combined_grad_norm\n",
        "\n",
        "        else:\n",
        "          print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "        # Check if the norm of the second-order gradient is below the threshold\n",
        "        if torch.norm(combined_grad) < threshold:\n",
        "            print(f\"Convergence reached at iteration {i}\")\n",
        "            break\n",
        "        # Update X_raw using the normalized gradient and learning rate\n",
        "        X_raw_tensor.data -= learning_rate * normalized_grad\n",
        "\n",
        "        # Zero out gradients for the next iteration\n",
        "        nn_model_instance.zero_grad()\n",
        "        X_raw_tensor.grad = None\n",
        "        # Update and checks as per your original code\n",
        "\n",
        "\n",
        "\n",
        "    # Print final modified data\n",
        "    #print(surrounding_grads)\n",
        "    #print(\"Final modified X_raw:\")\n",
        "    #if len(above_average_negative_eigenvalues) < 0.5*MC_pre_num_samples:\n",
        "      #print(\"need more MC_pre_num_samples\")\n",
        "    #else:\n",
        "    print(\"Used surrounding points: {}\".format(len(above_average_negative_eigenvalues)))\n",
        "\n",
        "    print(\"Output X is: {}\".format(X_raw_tensor))\n",
        "    print(negative_eigenvalues)\n",
        "\n",
        "\n",
        "\n",
        "    # Return the optimized X_raw tensor\n",
        "    return X_raw_tensor\n",
        "\n",
        "# Example usage\n",
        "optimized_X_1 = optimize_data_with_gradients(X_raw, Y, W_0, b, V_0, c, max_iterations=50, learning_rate=0.1, MC_num_samples=100, surrounding_proportion=0.9, max_deviation_for_weight=0.05, threshold=0.001)\n"
      ],
      "metadata": {
        "id": "hzb_VvUdxrG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_raw_torch = torch.tensor(optimized_X_3, requires_grad=True)\n",
        "Y_torch = torch.tensor(Y)\n",
        "\n",
        "\n",
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.001 # Adjust this threshold as needed\n",
        "max_iterations = 30 # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 200\n",
        "\n",
        "# Surrouning points' grads' propotion\n",
        "surrounding_propotion = 0.9\n",
        "\n",
        "# Weight perturbation\n",
        "max_deviation_for_weight = 0.01\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "#original_weights = W_0, b, V_0, c\n",
        "original_weights = {\n",
        "    'W_0': nn_model.W_0.data.clone(),\n",
        "    'b': nn_model.b.data.clone(),\n",
        "    'V_0': nn_model.V_0.data.clone(),\n",
        "    'c': nn_model.c.data.clone()\n",
        "}\n",
        "print(\"Original weight is {}\".format(original_weights))\n",
        "print(\"Initial X_raw {}\".format(X_raw_torch))\n",
        "#max_deviation_for_X = 0.02  # You can adjust this value as needed\n",
        "#perturb_data(X_raw_torch, max_deviation=max_deviation_for_X)\n",
        "#print(\"Perturbed X_raw {}\".format(X_raw_torch))\n",
        "\n",
        "for i in range(max_iterations):\n",
        "\n",
        "    # Calculate the gradient at the central point\n",
        "    central_grad = calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "    central_grad_norm = torch.norm(central_grad)\n",
        "    central_grad = central_grad / central_grad_norm\n",
        "    #print(central_grad)\n",
        "    # Surrouning points' grads\n",
        "    surrounding_grads = []\n",
        "    norms = []\n",
        "\n",
        "\n",
        "    # Calculate the gradient at the surrounding points by MC\n",
        "    for _ in range(MC_num_samples):\n",
        "\n",
        "      nn_model_sample = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "      #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Perturb weights\n",
        "      #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "      perturb_weights_uniform_fixed_range(nn_model_sample,scale = 0.05)\n",
        "      #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Calculate second-order gradient\n",
        "      grad = calculate_second_order_grad(nn_model_sample, X_raw_torch, Y_torch)\n",
        "      #grad_norm = torch.norm(grad)\n",
        "      #grad = grad / grad_norm\n",
        "      surrounding_grads.append(grad)\n",
        "      norms.append(torch.norm(grad).item())\n",
        "\n",
        "    # Calculate average norm\n",
        "    average_norm = sum(norms) / len(norms)\n",
        "\n",
        "    # Filter and sum gradients with norms above average\n",
        "    above_average_grads = [grad for grad, norm in zip(surrounding_grads, norms) if norm > 0.2 * average_norm]\n",
        "    above_average_grads = [grad / torch.norm(grad) for grad in above_average_grads]\n",
        "\n",
        "    #print(above_average_grads)\n",
        "    sum_above_average_grads = sum(above_average_grads)\n",
        "\n",
        "    # Average the large norm gradients\n",
        "    if above_average_grads:\n",
        "      average_above_average_grad = sum_above_average_grads / len(above_average_grads)\n",
        "      #print(average_above_average_grad)\n",
        "    else:\n",
        "    # Handle the case where no gradient is above average\n",
        "      average_above_average_grad = torch.zeros_like(X_raw_torch)\n",
        "\n",
        "\n",
        "    #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "    # Combine gradients\n",
        "    combined_grad = (1-surrounding_propotion) * central_grad + surrounding_propotion * average_above_average_grad\n",
        "    #combined_grad =  average_surrounding_grad\n",
        "    #print(combined_grad)\n",
        "    # Calculate the norm of the combined gradient\n",
        "    combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "    # Check for a non-zero norm to avoid division by zero\n",
        "    if combined_grad_norm > 0:\n",
        "    # Normalize the gradient\n",
        "      normalized_grad = combined_grad / combined_grad_norm\n",
        "\n",
        "    else:\n",
        "      print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "    # Check if the norm of the second-order gradient is below the threshold\n",
        "    if torch.norm(combined_grad) < threshold:\n",
        "        print(f\"Convergence reached at iteration {i}\")\n",
        "        break\n",
        "\n",
        "    # Update X_raw using gradient descent\n",
        "    X_raw_torch.data -= learning_rate * normalized_grad\n",
        "\n",
        "    # Zero out gradients for the next iteration\n",
        "    nn_model.zero_grad()\n",
        "    X_raw_torch.grad = None\n",
        "\n",
        "# Print final modified data\n",
        "#print(surrounding_grads)\n",
        "#print(\"Final modified X_raw:\")\n",
        "if len(above_average_grads) < 100:\n",
        "  print(\"need more MC_num_samples\")\n",
        "else:\n",
        "  print(\"Used surrounding points: {}\".format(len(above_average_grads)))\n",
        "\n",
        "print(X_raw_torch)\n",
        "print(negative_eigenvalues)"
      ],
      "metadata": {
        "id": "UUZ3a_stx46f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_raw_torch = torch.tensor(optimized_X_3, requires_grad=True)\n",
        "Y_torch = torch.tensor(Y)\n",
        "\n",
        "\n",
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.001 # Adjust this threshold as needed\n",
        "max_iterations = 30 # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 200\n",
        "\n",
        "# Surrouning points' grads' propotion\n",
        "surrounding_propotion = 0.9\n",
        "\n",
        "# Weight perturbation\n",
        "max_deviation_for_weight = 0.01\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "#original_weights = W_0, b, V_0, c\n",
        "original_weights = {\n",
        "    'W_0': nn_model.W_0.data.clone(),\n",
        "    'b': nn_model.b.data.clone(),\n",
        "    'V_0': nn_model.V_0.data.clone(),\n",
        "    'c': nn_model.c.data.clone()\n",
        "}\n",
        "print(\"Original weight is {}\".format(original_weights))\n",
        "print(\"Initial X_raw {}\".format(X_raw_torch))\n",
        "#max_deviation_for_X = 0.02  # You can adjust this value as needed\n",
        "#perturb_data(X_raw_torch, max_deviation=max_deviation_for_X)\n",
        "#print(\"Perturbed X_raw {}\".format(X_raw_torch))\n",
        "\n",
        "for i in range(max_iterations):\n",
        "\n",
        "    # Calculate the gradient at the central point\n",
        "    central_grad = calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "    central_grad_norm = torch.norm(central_grad)\n",
        "    central_grad = central_grad / central_grad_norm\n",
        "    #print(central_grad)\n",
        "    # Surrouning points' grads\n",
        "    surrounding_grads = []\n",
        "    norms = []\n",
        "\n",
        "\n",
        "    # Calculate the gradient at the surrounding points by MC\n",
        "    for _ in range(MC_num_samples):\n",
        "\n",
        "      nn_model_sample = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "      #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Perturb weights\n",
        "      #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "      perturb_weights_uniform_fixed_range(nn_model_sample,scale = 0.05)\n",
        "      #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Calculate second-order gradient\n",
        "      grad = calculate_second_order_grad(nn_model_sample, X_raw_torch, Y_torch)\n",
        "      #grad_norm = torch.norm(grad)\n",
        "      #grad = grad / grad_norm\n",
        "      surrounding_grads.append(grad)\n",
        "      norms.append(torch.norm(grad).item())\n",
        "\n",
        "    # Calculate average norm\n",
        "    average_norm = sum(norms) / len(norms)\n",
        "\n",
        "    # Filter and sum gradients with norms above average\n",
        "    above_average_grads = [grad for grad, norm in zip(surrounding_grads, norms) if norm > 0.2 * average_norm]\n",
        "    above_average_grads = [grad / torch.norm(grad) for grad in above_average_grads]\n",
        "\n",
        "    #print(above_average_grads)\n",
        "    sum_above_average_grads = sum(above_average_grads)\n",
        "\n",
        "    # Average the large norm gradients\n",
        "    if above_average_grads:\n",
        "      average_above_average_grad = sum_above_average_grads / len(above_average_grads)\n",
        "      #print(average_above_average_grad)\n",
        "    else:\n",
        "    # Handle the case where no gradient is above average\n",
        "      average_above_average_grad = torch.zeros_like(X_raw_torch)\n",
        "\n",
        "\n",
        "    #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "    # Combine gradients\n",
        "    combined_grad = (1-surrounding_propotion) * central_grad + surrounding_propotion * average_above_average_grad\n",
        "    #combined_grad =  average_surrounding_grad\n",
        "    #print(combined_grad)\n",
        "    # Calculate the norm of the combined gradient\n",
        "    combined_grad_norm = torch.norm(combined_grad)\n",
        "\n",
        "    # Check for a non-zero norm to avoid division by zero\n",
        "    if combined_grad_norm > 0:\n",
        "    # Normalize the gradient\n",
        "      normalized_grad = combined_grad / combined_grad_norm\n",
        "\n",
        "    else:\n",
        "      print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "    # Check if the norm of the second-order gradient is below the threshold\n",
        "    if torch.norm(combined_grad) < threshold:\n",
        "        print(f\"Convergence reached at iteration {i}\")\n",
        "        break\n",
        "\n",
        "    # Update X_raw using gradient descent\n",
        "    X_raw_torch.data -= learning_rate * normalized_grad\n",
        "\n",
        "    # Zero out gradients for the next iteration\n",
        "    nn_model.zero_grad()\n",
        "    X_raw_torch.grad = None\n",
        "\n",
        "# Print final modified data\n",
        "#print(surrounding_grads)\n",
        "#print(\"Final modified X_raw:\")\n",
        "if len(above_average_grads) < 100:\n",
        "  print(\"need more MC_num_samples\")\n",
        "else:\n",
        "  print(\"Used surrounding points: {}\".format(len(above_average_grads)))\n",
        "\n",
        "print(X_raw_torch)\n",
        "print(negative_eigenvalues)"
      ],
      "metadata": {
        "id": "fiqG3HcGV4u0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eead8b58-42e0-442a-f988-71507ff16df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-52c0fdd6e8f8>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_torch = torch.tensor(X_raw_pre, requires_grad=True)\n",
            "<ipython-input-49-52c0fdd6e8f8>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_torch = torch.tensor(Y_pre)\n",
            "<ipython-input-14-a815e9696fa5>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-14-a815e9696fa5>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-14-a815e9696fa5>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-14-a815e9696fa5>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weight is {'W_0': tensor([[ 3.9979,  5.2638],\n",
            "        [ 1.0698, -1.8901]], dtype=torch.float64), 'b': tensor([[1.8545, 0.8022]], dtype=torch.float64), 'V_0': tensor([[-10.1389],\n",
            "        [-10.4242]], dtype=torch.float64), 'c': tensor([[5.0116]], dtype=torch.float64)}\n",
            "Initial X_raw tensor([[11.0898, -7.6508],\n",
            "        [ 2.0012, -5.9457],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5426, -6.2105],\n",
            "        [ 1.1080,  0.1753],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-0.8698,  0.8182],\n",
            "        [-3.4649,  2.3132]], dtype=torch.float64, requires_grad=True)\n",
            "Used surrounding points: 200\n",
            "tensor([[11.0898, -7.6508],\n",
            "        [ 2.0012, -5.9457],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5426, -6.2105],\n",
            "        [ 1.1080,  0.1753],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-0.7339,  0.4347],\n",
            "        [-3.4439,  2.3197]], dtype=torch.float64, requires_grad=True)\n",
            "[tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0004, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64), tensor(-0.0005, dtype=torch.float64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model_final = SimpleNN(W_0, b, V_0, c)\n",
        "hessian_matrix_final, eigenvalues_final = compute_hessian_and_eigenvalues(nn_model, X_raw_torch, Y_torch)\n",
        "print(X_raw_torch)\n",
        "print(Y_torch)\n",
        "print(eigenvalues_final)\n",
        "check_local_minimum(eigenvalues_final)"
      ],
      "metadata": {
        "id": "nDgON5VoZ6_X",
        "outputId": "6be587a7-eb5d-4503-bf05-bb0fbe6f0322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[11.0898, -7.6508],\n",
            "        [ 2.0012, -5.9457],\n",
            "        [ 4.9975, -0.8779],\n",
            "        [ 6.1208, -5.1308],\n",
            "        [-5.5426, -6.2105],\n",
            "        [ 1.1080,  0.1753],\n",
            "        [-7.4783,  0.5124],\n",
            "        [-7.9947,  0.8285],\n",
            "        [-0.7339,  0.4347],\n",
            "        [-3.4439,  2.3197]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]], dtype=torch.float64)\n",
            "tensor([ 2.0579e-01+0.j,  7.9276e-03+0.j,  4.7714e-03+0.j, -1.4180e-03+0.j,\n",
            "        -5.0715e-04+0.j, -4.3948e-05+0.j, -4.1439e-06+0.j,  1.1427e-07+0.j,\n",
            "        -1.8893e-09+0.j], dtype=torch.complex128)\n",
            "This is not a local minimum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "def calculate_gradient_for_smallest_eigenvalue(model, X_raw_torch, Y_torch):\n",
        "    # Forward pass\n",
        "    output = model(X_raw_torch)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "\n",
        "    # First-order gradients (w.r.t weights)\n",
        "    first_order_grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "    # Flatten the first-order gradients\n",
        "    grads_flatten = torch.cat([g.contiguous().view(-1) for g in first_order_grads])\n",
        "\n",
        "    # Hessian computation\n",
        "    hessian = []\n",
        "    for grad in grads_flatten:\n",
        "        # Compute second-order gradients (w.r.t each element in the first-order gradients)\n",
        "        second_order_grads = torch.autograd.grad(grad, model.parameters(), retain_graph=True)\n",
        "\n",
        "        # Flatten and collect the second-order gradients\n",
        "        hessian_row = torch.cat([g.contiguous().view(-1) for g in second_order_grads])\n",
        "        hessian.append(hessian_row)\n",
        "\n",
        "    # Stack to form the Hessian matrix\n",
        "    hessian_matrix = torch.stack(hessian)\n",
        "\n",
        "    # Compute eigenvalues\n",
        "    eigenvalues, _ = torch.linalg.eig(hessian_matrix)\n",
        "    # Extract the real parts of eigenvalues\n",
        "    eigenvalues_real = eigenvalues.real\n",
        "\n",
        "    # Identify the smallest eigenvalue\n",
        "    smallest_eigenvalue = torch.min(eigenvalues_real)\n",
        "\n",
        "    # Check if the smallest eigenvalue requires gradients\n",
        "    if smallest_eigenvalue.requires_grad:\n",
        "      # Compute the gradient of the smallest eigenvalue (or its negative) with respect to X\n",
        "      if smallest_eigenvalue < 0:\n",
        "        grad_X = torch.autograd.grad(-smallest_eigenvalue, X_raw_torch, retain_graph=True)[0]\n",
        "      else:\n",
        "        print(\"FOUND A LOCAL MINIMUM!\")\n",
        "        print(f\"Local minimum at X: {X_raw_torch.detach().numpy()}\")\n",
        "        weights = {name: param.clone().detach().numpy() for name, param in model.named_parameters()}\n",
        "        print(f\"Local minimum at W: {weights}\")\n",
        "        grad_X = None  # or handle this case as you see fit\n",
        "    else:\n",
        "      print(\"Smallest eigenvalue does not require grad or is not part of the computation graph.\")\n",
        "      grad_X = None\n",
        "\n",
        "    return grad_X, eigenvalues, smallest_eigenvalue"
      ],
      "metadata": {
        "id": "E6mAQ3Hz-55q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.05 # Adjust this threshold as needed\n",
        "max_iterations = 10  # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.2\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 10\n",
        "\n",
        "# Surrouning points' grad\n",
        "surrounding_grads = []\n",
        "\n",
        "# parameters for the first layer\n",
        "W_0 = np.array([[1.05954587,-0.05625762],[-0.03749863,1.09518945]])\n",
        "b = np.array([[-0.050686,-0.06894291]])\n",
        "\n",
        "# parameters for the second layer\n",
        "\n",
        "V_0 = np.array([[3.76921058],[-3.72139955]])\n",
        "c = np.array([[-0.0148436]])\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "perturb_weights(nn_model, max_deviation=0.01)\n",
        "restore_weights(nn_model, original_weights)  # Assuming perturb_weights is defined as before\n",
        "print(perturb_weights)\n",
        "K=calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "print(K)\n",
        "print(\"W_0 (after perturbation):\", nn_model.W_0.data)\n",
        "print(\"b (after perturbation):\", nn_model.b.data)\n",
        "print(\"V_0 (after perturbation):\", nn_model.V_0.data)\n",
        "print(\"c (after perturbation):\", nn_model.c.data)"
      ],
      "metadata": {
        "id": "_qolZlVZ-x8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward pass\n",
        "output = nn_model(X_raw_torch)\n",
        "\n",
        "# Compute loss\n",
        "loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "print(loss)\n",
        "# Compute gradients of the loss w.r.t. weights\n",
        "loss.backward(create_graph=True)\n",
        "\n",
        "\n",
        "# Combine and compute the norm of all gradients\n",
        "all_grads = torch.cat([nn_model.W_0.grad.flatten(), nn_model.V_0.grad.flatten(), nn_model.b.grad.flatten(), nn_model.c.grad.flatten()])\n",
        "print(all_grads)\n",
        "grad_norm = torch.norm(all_grads)\n",
        "print(grad_norm)\n",
        "# Compute the derivative of the grad_norm with respect to X\n",
        "second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "print(torch.norm(second_order_grad))\n",
        "# If you want to perform gradient descent on X_raw\n",
        "learning_rate = 0.01\n",
        "#X_raw_torch.data -= learning_rate * second_order_grad"
      ],
      "metadata": {
        "id": "s_654-o11XCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_raw_pre_1 = torch.tensor(X_raw_pre, requires_grad=True)\n",
        "Y_pre_1 = torch.tensor(Y_pre_1)\n",
        "\n",
        "\n",
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.001 # Adjust this threshold as needed\n",
        "max_iterations_pre = 50 # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_pre_num_samples = 150\n",
        "\n",
        "# Surrouning points' grads' propotion\n",
        "surrounding_propotion_pre = 0.5\n",
        "\n",
        "# Weight perturbation\n",
        "max_deviation_for_weight_pre = 0.05\n",
        "\n",
        "nn_model_pre = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "#original_weights = W_0, b, V_0, c\n",
        "original_weights = {\n",
        "    'W_0': nn_model.W_0.data.clone(),\n",
        "    'b': nn_model.b.data.clone(),\n",
        "    'V_0': nn_model.V_0.data.clone(),\n",
        "    'c': nn_model.c.data.clone()\n",
        "}\n",
        "print(\"Original weight is {}\".format(original_weights))\n",
        "print(\"Initial X_raw_pre_1 {}\".format(X_raw_pre_1))\n",
        "#max_deviation_for_X = 0.02  # You can adjust this value as needed\n",
        "#perturb_data(X_raw_torch, max_deviation=max_deviation_for_X)\n",
        "#print(\"Perturbed X_raw {}\".format(X_raw_torch))\n",
        "\n",
        "for i in range(max_iterations_pre):\n",
        "\n",
        "    # Calculate the gradient at the central point\n",
        "    central_grad_pre = calculate_second_order_grad(nn_model_pre, X_raw_pre_1, Y_pre_1)\n",
        "    central_grad_pre_norm = torch.norm(central_grad_pre)\n",
        "    central_grad_pre = central_grad_pre / central_grad_pre_norm\n",
        "    #print(central_grad)\n",
        "    # Surrouning points' grads\n",
        "    surrounding_grads_pre = []\n",
        "    norms_pre = []\n",
        "    negative_eigenvalues = []\n",
        "\n",
        "\n",
        "    # Calculate the gradient at the surrounding points by MC\n",
        "    for _ in range(MC_pre_num_samples):\n",
        "\n",
        "      nn_model_sample_pre = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "      #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Perturb weights\n",
        "      #perturb_weights_uniform_fixed_range(nn_model_sample, max_deviation=max_deviation_for_weight)\n",
        "      perturb_weights_uniform_fixed_range(nn_model_sample_pre, scale = 0.1)\n",
        "      #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "      _, eigenvalues = compute_hessian_and_eigenvalues(nn_model_sample_pre, X_raw_pre_1, Y_pre_1)\n",
        "      most_negative_eigenvalue = select_most_negative_eigenvalue(eigenvalues)\n",
        "      negative_eigenvalues.append(most_negative_eigenvalue)\n",
        "      grad_pre = calculate_second_order_grad(nn_model_sample_pre, X_raw_pre_1, Y_pre_1)\n",
        "      #grad_norm = torch.norm(grad)\n",
        "      #grad = grad / grad_norm\n",
        "      surrounding_grads_pre.append(grad_pre)\n",
        "      #negative_eigenvalues.append(torch.norm(grad_pre).item())\n",
        "\n",
        "    # Calculate average pre_norm\n",
        "    average_negative_eigenvalues = sum(negative_eigenvalues) / len(negative_eigenvalues)\n",
        "\n",
        "    # Calculate the median of negative eigenvalues\n",
        "    median_negative_eigenvalue = np.median([eigenvalue for eigenvalue in negative_eigenvalues if eigenvalue < 0])\n",
        "\n",
        "    # Filter gradients corresponding to the smallest 50% of negative eigenvalues\n",
        "    above_average_negative_eigenvalues = [grad for grad, eigenvalue in zip(surrounding_grads_pre, negative_eigenvalues) if eigenvalue < median_negative_eigenvalue]\n",
        "    above_average_negative_eigenvalues = [grad / torch.norm(grad) for grad in above_average_negative_eigenvalues]\n",
        "\n",
        "    #print(above_average_grads)\n",
        "    sum_above_average_negative_eigenvalues = sum(above_average_negative_eigenvalues)\n",
        "\n",
        "    # Average the large norm gradients\n",
        "    if above_average_negative_eigenvalues:\n",
        "      average_above_average_negative_eigenvalues = sum_above_average_negative_eigenvalues / len(above_average_negative_eigenvalues)\n",
        "      #print(average_above_average_grad)\n",
        "    else:\n",
        "    # Handle the case where no gradient is above average\n",
        "      average_above_average_negative_eigenvalues = torch.zeros_like(X_raw_pre_1)\n",
        "\n",
        "\n",
        "    #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "\n",
        "    # Combine gradients\n",
        "    combined_grad_pre = (1-surrounding_propotion_pre) * central_grad_pre + surrounding_propotion_pre * average_above_average_negative_eigenvalues\n",
        "    #combined_grad =  average_surrounding_grad\n",
        "    #print(combined_grad)\n",
        "    # Calculate the norm of the combined gradient\n",
        "    combined_grad_pre_norm = torch.norm(combined_grad_pre)\n",
        "\n",
        "    # Check for a non-zero norm to avoid division by zero\n",
        "    if combined_grad_pre_norm > 0:\n",
        "    # Normalize the gradient\n",
        "      normalized_grad_pre = combined_grad_pre / combined_grad_pre_norm\n",
        "\n",
        "    else:\n",
        "      print(\"Gradient is zero; no update required.\")\n",
        "###############\n",
        "\n",
        "    # Check if the norm of the second-order gradient is below the threshold\n",
        "    if torch.norm(combined_grad_pre) < threshold:\n",
        "        print(f\"Convergence reached at iteration {i}\")\n",
        "        break\n",
        "    # Update X_raw using the normalized gradient and learning rate\n",
        "    X_raw_pre_1.data -= learning_rate * normalized_grad_pre\n",
        "\n",
        "    # Zero out gradients for the next iteration\n",
        "    nn_model.zero_grad()\n",
        "    X_raw_pre_1.grad = None\n",
        "\n",
        "# Print final modified data\n",
        "#print(surrounding_grads)\n",
        "#print(\"Final modified X_raw:\")\n",
        "if len(above_average_negative_eigenvalues) < 0.5*MC_pre_num_samples:\n",
        "  print(\"need more MC_pre_num_samples\")\n",
        "else:\n",
        "  print(\"Used surrounding points: {}\".format(len(above_average_negative_eigenvalues)))\n",
        "\n",
        "print(X_raw_pre_1)\n",
        "print(negative_eigenvalues)"
      ],
      "metadata": {
        "id": "9UEa48L2rbb3"
      }
    }
  ]
}