{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5pUz6rS77e0YryqMNougL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zw2788/LocalMinimaConstruction/blob/main/DwrtXGradientW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import Image\n",
        "from torch.autograd import grad"
      ],
      "metadata": {
        "id": "6IhL4Cbb1mfH"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, custom_W_0, custom_b, custom_V_0, custom_c):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        # Ensure that the custom weights are tensors\n",
        "        custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
        "        custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
        "        custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
        "        custom_c = torch.tensor(custom_c, dtype=torch.float64)\n",
        "\n",
        "        # Set the custom weights and biases\n",
        "        self.W_0 = nn.Parameter(custom_W_0)\n",
        "        self.b = nn.Parameter(custom_b)\n",
        "        self.V_0 = nn.Parameter(custom_V_0)\n",
        "        self.c = nn.Parameter(custom_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.W_0), self.b))\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.V_0), self.c))\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "#custom_W_0 = [[0.1, 0.2], [0.3, 0.4]]  # Replace with your own initial values\n",
        "#custom_b = [0.1, 0.2]  # Replace with your own initial values\n",
        "#custom_V_0 = [[0.1], [0.2]]  # Replace with your own initial values\n",
        "#custom_c = [0.1]  # Replace with your own initial values\n",
        "\n",
        "\n",
        "def calculate_second_order_grad(model, X_raw_torch, Y_torch):\n",
        "    # Forward pass\n",
        "    output = model(X_raw_torch)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "    # Compute gradients of the loss w.r.t. weights\n",
        "    loss.backward(create_graph=True)\n",
        "    # Combine and compute the norm of all gradients\n",
        "    all_grads = torch.cat([param.grad.flatten() for param in model.parameters()])\n",
        "    grad_norm = torch.norm(all_grads)\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "    return second_order_grad\n",
        "\n",
        "def perturb_weights(model, max_deviation=0.01):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            std_dev = param.abs().mean() * max_deviation\n",
        "            noise = torch.randn(param.size()) * std_dev\n",
        "            param[:] = param + noise\n",
        "\n",
        "def restore_weights(model, saved_state):\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            param[:] = saved_state[name]\n",
        "\n",
        "def perturb_data(X, max_deviation=0.01):\n",
        "    \"\"\"Perturb the data tensor X.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        std_dev = X.abs().mean() * max_deviation\n",
        "        noise = torch.randn(X.size()) * std_dev\n",
        "        X.add_(noise)"
      ],
      "metadata": {
        "id": "vBoW060Y1pZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/zw2788/LocalMinimaConstruction/main/Ex1.csv\")\n",
        "\n",
        "data.head()\n",
        "\n",
        "# data , drop NaN values\n",
        "X_raw,  Y, W_0, b, V_0, c = data[['x_2dvec']].dropna().values, data['y'].dropna().values, data[['W_0']].dropna().values, data[['b']].dropna().values, data[['V_0']].dropna().values, data[['c']].dropna().values\n",
        "\n",
        "#convert string to array\n",
        "\n",
        "X_raw = np.array([eval(s[0]) for s in X_raw])\n",
        "\n",
        "W_0 = np.array([eval(s[0]) for s in W_0])\n",
        "\n",
        "b = np.array([eval(s[0]) for s in b])\n",
        "\n",
        "V_0 = np.array([eval(s[0]) for s in V_0])\n",
        "\n",
        "c = np.array([eval(s[0]) for s in c])\n",
        "\n",
        "# Standardize the input\n",
        "# Leave blank to match the example in paper\n",
        "\n",
        "# formatting\n",
        "Y = Y.reshape((-1, 1))\n",
        "print(X_raw)\n",
        "print(Y)\n",
        "print(W_0)\n",
        "#print(X_raw.shape[0])\n",
        "X_raw = torch.tensor(X_raw, requires_grad=True)\n",
        "Y = torch.tensor(Y)\n",
        "print(W_0, b, V_0, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0h9evkU59mR",
        "outputId": "ed48dfe2-27c7-455a-9714-9a9394949745"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.8  0.4]\n",
            " [ 3.1  4.3]\n",
            " [ 0.1 -3.4]\n",
            " [-4.2 -3.3]\n",
            " [-0.5  0.2]\n",
            " [-2.7 -0.4]\n",
            " [-3.  -4.3]\n",
            " [-0.1  3.4]\n",
            " [ 4.2  3.2]\n",
            " [ 0.4 -0.1]]\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "[[ 1.05954587 -0.05625762]\n",
            " [-0.03749863  1.09518945]]\n",
            "[[ 1.05954587 -0.05625762]\n",
            " [-0.03749863  1.09518945]] [[-0.03749863  1.09518945]] [[ 3.76921058]\n",
            " [-3.72139955]] [[-0.0148436]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_raw_torch = torch.tensor(X_raw, requires_grad=True)\n",
        "Y_torch = torch.tensor(Y)\n",
        "\n",
        "\n",
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.05 # Adjust this threshold as needed\n",
        "max_iterations = 20  # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 200\n",
        "\n",
        "# Surrouning points' grads' propotion\n",
        "surrounding_propotion = 50\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "#original_weights = W_0, b, V_0, c\n",
        "original_weights = {\n",
        "    'W_0': nn_model.W_0.data.clone(),\n",
        "    'b': nn_model.b.data.clone(),\n",
        "    'V_0': nn_model.V_0.data.clone(),\n",
        "    'c': nn_model.c.data.clone()\n",
        "}\n",
        "print(original_weights)\n",
        "print(\"Initial X_raw {}\".format(X_raw_torch))\n",
        "max_deviation_for_X = 0.02  # You can adjust this value as needed\n",
        "perturb_data(X_raw_torch, max_deviation=max_deviation_for_X)\n",
        "print(\"Perturbed X_raw {}\".format(X_raw_torch))\n",
        "\n",
        "for i in range(max_iterations):\n",
        "\n",
        "    # Calculate the gradient at the central point\n",
        "    central_grad = calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "\n",
        "    # Surrouning points' grads\n",
        "    surrounding_grads = []\n",
        "\n",
        "\n",
        "    # Calculate the gradient at the surrounding points by MC\n",
        "    for _ in range(MC_num_samples):\n",
        "\n",
        "      nn_model_sample = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "      #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Perturb weights\n",
        "      perturb_weights(nn_model_sample, max_deviation=0.01)\n",
        "      #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Calculate second-order gradient\n",
        "      grad = calculate_second_order_grad(nn_model_sample, X_raw_torch, Y_torch)\n",
        "      surrounding_grads.append(grad)\n",
        "\n",
        "    #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "    # Average surrounding gradients\n",
        "    average_surrounding_grad = sum(surrounding_grads) / MC_num_samples\n",
        "\n",
        "    # Combine gradients\n",
        "    combined_grad = central_grad + surrounding_propotion * average_surrounding_grad\n",
        "\n",
        "###############\n",
        "    # Combine and compute the norm of all gradients\n",
        "    #all_grads = torch.cat([nn_model.W_0.grad.flatten(), nn_model.V_0.grad.flatten(), nn_model.b.grad.flatten(), nn_model.c.grad.flatten()])\n",
        "    #grad_norm = torch.norm(all_grads)\n",
        "\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    #second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "\n",
        "    # Check if the norm of the second-order gradient is below the threshold\n",
        "    if torch.norm(combined_grad) < threshold:\n",
        "        print(f\"Convergence reached at iteration {i}\")\n",
        "        break\n",
        "\n",
        "    # Update X_raw using gradient descent\n",
        "    X_raw_torch.data -= learning_rate * combined_grad\n",
        "\n",
        "    # Zero out gradients for the next iteration\n",
        "    nn_model.zero_grad()\n",
        "    X_raw_torch.grad = None\n",
        "\n",
        "# Print final modified data\n",
        "#print(surrounding_grads)\n",
        "#print(\"Final modified X_raw:\")\n",
        "print(X_raw_torch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiqG3HcGV4u0",
        "outputId": "6ae846dd-7297-4900-840b-cbe504703e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-9309a5caa323>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_torch = torch.tensor(X_raw, requires_grad=True)\n",
            "<ipython-input-68-9309a5caa323>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_torch = torch.tensor(Y)\n",
            "<ipython-input-44-7093221fd565>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-44-7093221fd565>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-44-7093221fd565>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-44-7093221fd565>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'W_0': tensor([[ 1.0595, -0.0563],\n",
            "        [-0.0375,  1.0952]], dtype=torch.float64), 'b': tensor([[-0.0375,  1.0952]], dtype=torch.float64), 'V_0': tensor([[ 3.7692],\n",
            "        [-3.7214]], dtype=torch.float64), 'c': tensor([[-0.0148]], dtype=torch.float64)}\n",
            "Initial X_raw tensor([[ 2.8000,  0.4000],\n",
            "        [ 3.1000,  4.3000],\n",
            "        [ 0.1000, -3.4000],\n",
            "        [-4.2000, -3.3000],\n",
            "        [-0.5000,  0.2000],\n",
            "        [-2.7000, -0.4000],\n",
            "        [-3.0000, -4.3000],\n",
            "        [-0.1000,  3.4000],\n",
            "        [ 4.2000,  3.2000],\n",
            "        [ 0.4000, -0.1000]], dtype=torch.float64, requires_grad=True)\n",
            "Perturbed X_raw tensor([[ 2.7485,  0.3702],\n",
            "        [ 3.1686,  4.3364],\n",
            "        [ 0.1450, -3.4219],\n",
            "        [-4.1875, -3.2948],\n",
            "        [-0.5174,  0.1783],\n",
            "        [-2.7019, -0.4439],\n",
            "        [-2.9534, -4.3165],\n",
            "        [-0.1050,  3.3718],\n",
            "        [ 4.1801,  3.1714],\n",
            "        [ 0.4512, -0.1165]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([[ 2.8651,  0.1410],\n",
            "        [ 3.2493,  4.3206],\n",
            "        [ 0.2827, -3.4825],\n",
            "        [-4.1409, -3.5946],\n",
            "        [-0.2839,  0.0943],\n",
            "        [-2.6689, -0.6051],\n",
            "        [-2.8956, -4.2612],\n",
            "        [ 0.0802,  3.3631],\n",
            "        [ 4.1874,  3.1738],\n",
            "        [ 0.7516, -0.5086]], dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-executing the code to define the function for computing the Hessian matrix and its eigenvalues\n",
        "\n",
        "def compute_hessian_and_eigenvalues(model, data, target):\n",
        "    \"\"\"\n",
        "    Compute the Hessian matrix and its eigenvalues for the weights of a neural network model.\n",
        "\n",
        "    :param model: The neural network model.\n",
        "    :param data: Input data (X).\n",
        "    :param target: Target data (Y).\n",
        "    :return: Hessian matrix and its eigenvalues.\n",
        "    \"\"\"\n",
        "    # Forward pass\n",
        "    output = model(data)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(target * torch.log(output) + (1 - target) * torch.log(1 - output))\n",
        "\n",
        "    # First-order gradients (w.r.t weights)\n",
        "    first_order_grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "    # Flatten the first-order gradients\n",
        "    grads_flatten = torch.cat([g.contiguous().view(-1) for g in first_order_grads])\n",
        "\n",
        "    # Hessian computation\n",
        "    hessian = []\n",
        "    for grad in grads_flatten:\n",
        "        # Compute second-order gradients (w.r.t each element in the first-order gradients)\n",
        "        second_order_grads = torch.autograd.grad(grad, model.parameters(), retain_graph=True)\n",
        "\n",
        "        # Flatten and collect the second-order gradients\n",
        "        hessian_row = torch.cat([g.contiguous().view(-1) for g in second_order_grads])\n",
        "        hessian.append(hessian_row)\n",
        "\n",
        "    # Stack to form the Hessian matrix\n",
        "    hessian_matrix = torch.stack(hessian)\n",
        "\n",
        "    # Compute eigenvalues\n",
        "    eigenvalues, _ = torch.linalg.eig(hessian_matrix)\n",
        "\n",
        "    return hessian_matrix, eigenvalues\n",
        "\n",
        "# Note: To use this function, you'll need to provide your neural network model, the input data (X), and the target data (Y).\n"
      ],
      "metadata": {
        "id": "DG4bccfJA4Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "compute_hessian_and_eigenvalues(nn_model, X_raw_torch, Y_torch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkVm-srfFTQZ",
        "outputId": "f2fdaa6d-8b81-4517-8386-f9d5d09e7b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1229, -0.0537,  0.1374, -0.0068,  0.0104, -0.0119,  0.0367,  0.0274,\n",
              "           0.0183],\n",
              "         [-0.0537,  0.2881, -0.0068,  0.1489, -0.0119, -0.0250, -0.0553, -0.0279,\n",
              "          -0.0097],\n",
              "         [ 0.1374, -0.0068,  0.4870, -0.0480, -0.0080,  0.0173,  0.0074,  0.0458,\n",
              "          -0.0111],\n",
              "         [-0.0068,  0.1489, -0.0480,  0.1303,  0.0173, -0.0501,  0.0052,  0.0031,\n",
              "           0.0472],\n",
              "         [ 0.0104, -0.0119, -0.0080,  0.0173,  0.0408, -0.0349,  0.0322,  0.0441,\n",
              "           0.0712],\n",
              "         [-0.0119, -0.0250,  0.0173, -0.0501, -0.0349,  0.0310, -0.0373, -0.0468,\n",
              "          -0.0681],\n",
              "         [ 0.0367, -0.0553,  0.0074,  0.0052,  0.0322, -0.0373,  0.0877,  0.0855,\n",
              "           0.1066],\n",
              "         [ 0.0274, -0.0279,  0.0458,  0.0031,  0.0441, -0.0468,  0.0855,  0.0956,\n",
              "           0.1139],\n",
              "         [ 0.0183, -0.0097, -0.0111,  0.0472,  0.0712, -0.0681,  0.1066,  0.1139,\n",
              "           0.1931]], dtype=torch.float64),\n",
              " tensor([ 0.5579+0.j,  0.4084+0.j,  0.3788+0.j,  0.0837+0.j, -0.0113+0.j, -0.0032+0.j,\n",
              "          0.0295+0.j,  0.0154+0.j,  0.0173+0.j], dtype=torch.complex128))"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.05 # Adjust this threshold as needed\n",
        "max_iterations = 10  # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.2\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 10\n",
        "\n",
        "# Surrouning points' grad\n",
        "surrounding_grads = []\n",
        "\n",
        "# parameters for the first layer\n",
        "W_0 = np.array([[1.05954587,-0.05625762],[-0.03749863,1.09518945]])\n",
        "b = np.array([[-0.050686,-0.06894291]])\n",
        "\n",
        "# parameters for the second layer\n",
        "\n",
        "V_0 = np.array([[3.76921058],[-3.72139955]])\n",
        "c = np.array([[-0.0148436]])\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "perturb_weights(nn_model, max_deviation=0.01)\n",
        "restore_weights(nn_model, original_weights)  # Assuming perturb_weights is defined as before\n",
        "print(perturb_weights)\n",
        "K=calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "print(K)\n",
        "print(\"W_0 (after perturbation):\", nn_model.W_0.data)\n",
        "print(\"b (after perturbation):\", nn_model.b.data)\n",
        "print(\"V_0 (after perturbation):\", nn_model.V_0.data)\n",
        "print(\"c (after perturbation):\", nn_model.c.data)"
      ],
      "metadata": {
        "id": "_qolZlVZ-x8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward pass\n",
        "output = nn_model(X_raw_torch)\n",
        "\n",
        "# Compute loss\n",
        "loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "print(loss)\n",
        "# Compute gradients of the loss w.r.t. weights\n",
        "loss.backward(create_graph=True)\n",
        "\n",
        "\n",
        "# Combine and compute the norm of all gradients\n",
        "all_grads = torch.cat([nn_model.W_0.grad.flatten(), nn_model.V_0.grad.flatten(), nn_model.b.grad.flatten(), nn_model.c.grad.flatten()])\n",
        "print(all_grads)\n",
        "grad_norm = torch.norm(all_grads)\n",
        "print(grad_norm)\n",
        "# Compute the derivative of the grad_norm with respect to X\n",
        "second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "print(torch.norm(second_order_grad))\n",
        "# If you want to perform gradient descent on X_raw\n",
        "learning_rate = 0.01\n",
        "#X_raw_torch.data -= learning_rate * second_order_grad"
      ],
      "metadata": {
        "id": "s_654-o11XCY"
      }
    }
  ]
}