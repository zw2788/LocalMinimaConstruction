{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXrr2LT+Jpoz+0R5647acq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zw2788/LocalMinimaConstruction/blob/main/DwrtXGradientW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import Image\n",
        "from torch.autograd import grad"
      ],
      "metadata": {
        "id": "6IhL4Cbb1mfH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, custom_W_0, custom_b, custom_V_0, custom_c):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        # Ensure that the custom weights are tensors\n",
        "        custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
        "        custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
        "        custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
        "        custom_c = torch.tensor(custom_c, dtype=torch.float64)\n",
        "\n",
        "        # Set the custom weights and biases\n",
        "        self.W_0 = nn.Parameter(custom_W_0)\n",
        "        self.b = nn.Parameter(custom_b)\n",
        "        self.V_0 = nn.Parameter(custom_V_0)\n",
        "        self.c = nn.Parameter(custom_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.W_0), self.b))\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.V_0), self.c))\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "#custom_W_0 = [[0.1, 0.2], [0.3, 0.4]]  # Replace with your own initial values\n",
        "#custom_b = [0.1, 0.2]  # Replace with your own initial values\n",
        "#custom_V_0 = [[0.1], [0.2]]  # Replace with your own initial values\n",
        "#custom_c = [0.1]  # Replace with your own initial values\n",
        "\n",
        "\n",
        "def calculate_second_order_grad(model, X_raw_torch, Y_torch):\n",
        "    # Forward pass\n",
        "    output = model(X_raw_torch)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "    # Compute gradients of the loss w.r.t. weights\n",
        "    loss.backward(create_graph=True)\n",
        "    # Combine and compute the norm of all gradients\n",
        "    all_grads = torch.cat([param.grad.flatten() for param in model.parameters()])\n",
        "    grad_norm = torch.norm(all_grads)\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "    return second_order_grad\n",
        "\n",
        "def perturb_weights(model, max_deviation=0.01):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            std_dev = param.abs().mean() * max_deviation\n",
        "            noise = torch.randn(param.size()) * std_dev\n",
        "            param[:] = param + noise\n",
        "\n",
        "def restore_weights(model, saved_state):\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            param[:] = saved_state[name]\n",
        "\n",
        "def perturb_data(X, max_deviation=0.01):\n",
        "    \"\"\"Perturb the data tensor X.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        std_dev = X.abs().mean() * max_deviation\n",
        "        noise = torch.randn(X.size()) * std_dev\n",
        "        X.add_(noise)"
      ],
      "metadata": {
        "id": "vBoW060Y1pZB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/zw2788/LocalMinimaConstruction/main/Ex1.csv\")\n",
        "\n",
        "data.head()\n",
        "features = [\n",
        "    \"x_2dvec\",\n",
        "\n",
        "]\n",
        "label = \"y\"\n",
        "\n",
        "# train test split\n",
        "X_raw,  Y = data[features].values, data[label].values\n",
        "\n",
        "#convert string to array\n",
        "X_raw = np.array([eval(s[0]) for s in X_raw])\n",
        "\n",
        "# Standardize the input\n",
        "# Leave blank to match the example in paper\n",
        "\n",
        "# formatting\n",
        "Y = Y.reshape((-1, 1))\n",
        "print(X_raw)\n",
        "print(Y)\n",
        "print(X_raw.shape[0])\n",
        "X_raw = torch.tensor(X_raw, requires_grad=True)\n",
        "Y = torch.tensor(Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0h9evkU59mR",
        "outputId": "abdb108e-fa39-4f2d-f306-554ff003539d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.8  0.4]\n",
            " [ 3.1  4.3]\n",
            " [ 0.1 -3.4]\n",
            " [-4.2 -3.3]\n",
            " [-0.5  0.2]\n",
            " [-2.7 -0.4]\n",
            " [-3.  -4.3]\n",
            " [-0.1  3.4]\n",
            " [ 4.2  3.2]\n",
            " [ 0.4 -0.1]]\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_raw_torch = torch.tensor(X_raw, requires_grad=True)\n",
        "Y_torch = torch.tensor(Y)\n",
        "\n",
        "\n",
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.05 # Adjust this threshold as needed\n",
        "max_iterations = 50  # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.02\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 100\n",
        "\n",
        "# Surrouning points' grads' propotion\n",
        "surrounding_propotion = 10\n",
        "\n",
        "# parameters for the first layer\n",
        "W_0 = np.array([[1.05954587,-0.05625762],[-0.03749863,1.09518945]])\n",
        "b = np.array([[-0.050686,-0.06894291]])\n",
        "\n",
        "# parameters for the second layer\n",
        "\n",
        "V_0 = np.array([[3.76921058],[-3.72139955]])\n",
        "c = np.array([[-0.0148436]])\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "#original_weights = W_0, b, V_0, c\n",
        "original_weights = {\n",
        "    'W_0': nn_model.W_0.data.clone(),\n",
        "    'b': nn_model.b.data.clone(),\n",
        "    'V_0': nn_model.V_0.data.clone(),\n",
        "    'c': nn_model.c.data.clone()\n",
        "}\n",
        "print(original_weights)\n",
        "print(\"Initial X_raw {}\".format(X_raw_torch))\n",
        "max_deviation_for_X = 0.02  # You can adjust this value as needed\n",
        "perturb_data(X_raw_torch, max_deviation=max_deviation_for_X)\n",
        "print(\"Perturbed X_raw {}\".format(X_raw_torch))\n",
        "\n",
        "for i in range(max_iterations):\n",
        "\n",
        "    # Calculate the gradient at the central point\n",
        "    central_grad = calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "\n",
        "    # Surrouning points' grads\n",
        "    surrounding_grads = []\n",
        "\n",
        "\n",
        "    # Calculate the gradient at the surrounding points by MC\n",
        "    for _ in range(MC_num_samples):\n",
        "\n",
        "      nn_model_sample = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "      #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Perturb weights\n",
        "      perturb_weights(nn_model_sample, max_deviation=0.01)\n",
        "      #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Calculate second-order gradient\n",
        "      grad = calculate_second_order_grad(nn_model_sample, X_raw_torch, Y_torch)\n",
        "      surrounding_grads.append(grad)\n",
        "\n",
        "    #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "    # Average surrounding gradients\n",
        "    average_surrounding_grad = sum(surrounding_grads) / MC_num_samples\n",
        "\n",
        "    # Combine gradients\n",
        "    combined_grad = central_grad + surrounding_propotion * average_surrounding_grad\n",
        "\n",
        "###############\n",
        "    # Combine and compute the norm of all gradients\n",
        "    #all_grads = torch.cat([nn_model.W_0.grad.flatten(), nn_model.V_0.grad.flatten(), nn_model.b.grad.flatten(), nn_model.c.grad.flatten()])\n",
        "    #grad_norm = torch.norm(all_grads)\n",
        "\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    #second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "\n",
        "    # Check if the norm of the second-order gradient is below the threshold\n",
        "    if torch.norm(combined_grad) < threshold:\n",
        "        print(f\"Convergence reached at iteration {i}\")\n",
        "        break\n",
        "\n",
        "    # Update X_raw using gradient descent\n",
        "    X_raw_torch.data -= learning_rate * combined_grad\n",
        "\n",
        "    # Zero out gradients for the next iteration\n",
        "    nn_model.zero_grad()\n",
        "    X_raw_torch.grad = None\n",
        "\n",
        "# Print final modified data\n",
        "#print(surrounding_grads)\n",
        "#print(\"Final modified X_raw:\")\n",
        "print(X_raw_torch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiqG3HcGV4u0",
        "outputId": "e4ad0d68-f972-4965-b293-0882500bad1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-56bcc3b34ebb>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_torch = torch.tensor(X_raw, requires_grad=True)\n",
            "<ipython-input-11-56bcc3b34ebb>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_torch = torch.tensor(Y)\n",
            "<ipython-input-3-7093221fd565>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-3-7093221fd565>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-3-7093221fd565>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-3-7093221fd565>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'W_0': tensor([[ 1.0595, -0.0563],\n",
            "        [-0.0375,  1.0952]], dtype=torch.float64), 'b': tensor([[-0.0507, -0.0689]], dtype=torch.float64), 'V_0': tensor([[ 3.7692],\n",
            "        [-3.7214]], dtype=torch.float64), 'c': tensor([[-0.0148]], dtype=torch.float64)}\n",
            "Initial X_raw tensor([[ 2.8000,  0.4000],\n",
            "        [ 3.1000,  4.3000],\n",
            "        [ 0.1000, -3.4000],\n",
            "        [-4.2000, -3.3000],\n",
            "        [-0.5000,  0.2000],\n",
            "        [-2.7000, -0.4000],\n",
            "        [-3.0000, -4.3000],\n",
            "        [-0.1000,  3.4000],\n",
            "        [ 4.2000,  3.2000],\n",
            "        [ 0.4000, -0.1000]], dtype=torch.float64, requires_grad=True)\n",
            "Perturbed X_raw tensor([[ 2.7835,  0.3905],\n",
            "        [ 3.1632,  4.3076],\n",
            "        [ 0.0907, -3.4155],\n",
            "        [-4.2168, -3.3395],\n",
            "        [-0.5400,  0.1713],\n",
            "        [-2.7579, -0.4912],\n",
            "        [-3.0073, -4.3137],\n",
            "        [-0.0742,  3.2803],\n",
            "        [ 4.1650,  3.1954],\n",
            "        [ 0.5012, -0.1633]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([[ 2.7529,  0.3936],\n",
            "        [ 3.1263,  4.3125],\n",
            "        [ 0.0873, -3.4125],\n",
            "        [-4.2352, -3.3132],\n",
            "        [-0.5234,  0.1909],\n",
            "        [-2.7423, -0.4585],\n",
            "        [-2.9909, -4.3177],\n",
            "        [-0.1123,  3.2835],\n",
            "        [ 4.1879,  3.1808],\n",
            "        [ 0.3903, -0.0961]], dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-executing the code to define the function for computing the Hessian matrix and its eigenvalues\n",
        "\n",
        "def compute_hessian_and_eigenvalues(model, data, target):\n",
        "    \"\"\"\n",
        "    Compute the Hessian matrix and its eigenvalues for the weights of a neural network model.\n",
        "\n",
        "    :param model: The neural network model.\n",
        "    :param data: Input data (X).\n",
        "    :param target: Target data (Y).\n",
        "    :return: Hessian matrix and its eigenvalues.\n",
        "    \"\"\"\n",
        "    # Forward pass\n",
        "    output = model(data)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(target * torch.log(output) + (1 - target) * torch.log(1 - output))\n",
        "\n",
        "    # First-order gradients (w.r.t weights)\n",
        "    first_order_grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "    # Flatten the first-order gradients\n",
        "    grads_flatten = torch.cat([g.contiguous().view(-1) for g in first_order_grads])\n",
        "\n",
        "    # Hessian computation\n",
        "    hessian = []\n",
        "    for grad in grads_flatten:\n",
        "        # Compute second-order gradients (w.r.t each element in the first-order gradients)\n",
        "        second_order_grads = torch.autograd.grad(grad, model.parameters(), retain_graph=True)\n",
        "\n",
        "        # Flatten and collect the second-order gradients\n",
        "        hessian_row = torch.cat([g.contiguous().view(-1) for g in second_order_grads])\n",
        "        hessian.append(hessian_row)\n",
        "\n",
        "    # Stack to form the Hessian matrix\n",
        "    hessian_matrix = torch.stack(hessian)\n",
        "\n",
        "    # Compute eigenvalues\n",
        "    eigenvalues, _ = torch.linalg.eig(hessian_matrix)\n",
        "\n",
        "    return hessian_matrix, eigenvalues\n",
        "\n",
        "# Note: To use this function, you'll need to provide your neural network model, the input data (X), and the target data (Y).\n"
      ],
      "metadata": {
        "id": "DG4bccfJA4Jk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "compute_hessian_and_eigenvalues(nn_model, X_raw, Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkVm-srfFTQZ",
        "outputId": "4de0899f-1aa0-4227-c841-ab763a43b6c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1.2736e-01, -5.5936e-02,  1.5215e-01, -1.0956e-02,  1.7885e-03,\n",
              "           1.0932e-03,  2.5085e-02,  1.5584e-02, -7.4265e-04],\n",
              "         [-5.5936e-02,  3.5809e-01, -1.0956e-02,  1.4712e-01,  1.0932e-03,\n",
              "           7.9460e-03, -5.3010e-02, -1.9899e-02, -4.2689e-04],\n",
              "         [ 1.5215e-01, -1.0956e-02,  4.9734e-01, -3.0057e-02, -2.8171e-03,\n",
              "          -1.9586e-03,  1.5771e-02,  5.2557e-02, -2.1099e-04],\n",
              "         [-1.0956e-02,  1.4712e-01, -3.0057e-02,  9.3355e-02, -1.9586e-03,\n",
              "           5.4486e-03, -1.9865e-02, -1.9757e-02, -4.3961e-03],\n",
              "         [ 1.7885e-03,  1.0932e-03, -2.8171e-03, -1.9586e-03,  4.8731e-02,\n",
              "          -4.4789e-02,  3.7027e-02,  3.7482e-02,  7.6185e-02],\n",
              "         [ 1.0932e-03,  7.9460e-03, -1.9586e-03,  5.4486e-03, -4.4789e-02,\n",
              "           5.9441e-02, -3.9161e-02, -3.9844e-02, -8.0345e-02],\n",
              "         [ 2.5085e-02, -5.3010e-02,  1.5771e-02, -1.9865e-02,  3.7027e-02,\n",
              "          -3.9161e-02,  7.6625e-02,  7.0310e-02,  9.6837e-02],\n",
              "         [ 1.5584e-02, -1.9899e-02,  5.2557e-02, -1.9757e-02,  3.7482e-02,\n",
              "          -3.9844e-02,  7.0310e-02,  7.5809e-02,  9.7399e-02],\n",
              "         [-7.4265e-04, -4.2689e-04, -2.1099e-04, -4.3961e-03,  7.6185e-02,\n",
              "          -8.0345e-02,  9.6837e-02,  9.7399e-02,  1.9752e-01]],\n",
              "        dtype=torch.float64),\n",
              " tensor([5.7883e-01+0.j, 4.3281e-01+0.j, 3.5826e-01+0.j, 7.8197e-02+0.j, 3.8277e-02+0.j,\n",
              "         1.9406e-04+0.j, 8.1434e-03+0.j, 1.8474e-02+0.j, 2.1101e-02+0.j],\n",
              "        dtype=torch.complex128))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.05 # Adjust this threshold as needed\n",
        "max_iterations = 10  # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.2\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 10\n",
        "\n",
        "# Surrouning points' grad\n",
        "surrounding_grads = []\n",
        "\n",
        "# parameters for the first layer\n",
        "W_0 = np.array([[1.05954587,-0.05625762],[-0.03749863,1.09518945]])\n",
        "b = np.array([[-0.050686,-0.06894291]])\n",
        "\n",
        "# parameters for the second layer\n",
        "\n",
        "V_0 = np.array([[3.76921058],[-3.72139955]])\n",
        "c = np.array([[-0.0148436]])\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "perturb_weights(nn_model, max_deviation=0.01)\n",
        "restore_weights(nn_model, original_weights)  # Assuming perturb_weights is defined as before\n",
        "print(perturb_weights)\n",
        "K=calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "print(K)\n",
        "print(\"W_0 (after perturbation):\", nn_model.W_0.data)\n",
        "print(\"b (after perturbation):\", nn_model.b.data)\n",
        "print(\"V_0 (after perturbation):\", nn_model.V_0.data)\n",
        "print(\"c (after perturbation):\", nn_model.c.data)"
      ],
      "metadata": {
        "id": "_qolZlVZ-x8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward pass\n",
        "output = nn_model(X_raw_torch)\n",
        "\n",
        "# Compute loss\n",
        "loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "print(loss)\n",
        "# Compute gradients of the loss w.r.t. weights\n",
        "loss.backward(create_graph=True)\n",
        "\n",
        "\n",
        "# Combine and compute the norm of all gradients\n",
        "all_grads = torch.cat([nn_model.W_0.grad.flatten(), nn_model.V_0.grad.flatten(), nn_model.b.grad.flatten(), nn_model.c.grad.flatten()])\n",
        "print(all_grads)\n",
        "grad_norm = torch.norm(all_grads)\n",
        "print(grad_norm)\n",
        "# Compute the derivative of the grad_norm with respect to X\n",
        "second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "print(torch.norm(second_order_grad))\n",
        "# If you want to perform gradient descent on X_raw\n",
        "learning_rate = 0.01\n",
        "#X_raw_torch.data -= learning_rate * second_order_grad"
      ],
      "metadata": {
        "id": "s_654-o11XCY"
      }
    }
  ]
}