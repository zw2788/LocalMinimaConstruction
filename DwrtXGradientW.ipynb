{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWX+Xy9QIbymE8z3kLWW8g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zw2788/LocalMinimaConstruction/blob/main/DwrtXGradientW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "6IhL4Cbb1mfH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, custom_W_0, custom_b, custom_V_0, custom_c):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        # Ensure that the custom weights are tensors\n",
        "        custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
        "        custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
        "        custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
        "        custom_c = torch.tensor(custom_c, dtype=torch.float64)\n",
        "\n",
        "        # Set the custom weights and biases\n",
        "        self.W_0 = nn.Parameter(custom_W_0)\n",
        "        self.b = nn.Parameter(custom_b)\n",
        "        self.V_0 = nn.Parameter(custom_V_0)\n",
        "        self.c = nn.Parameter(custom_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.W_0), self.b))\n",
        "        x = F.sigmoid(torch.add(torch.matmul(x, self.V_0), self.c))\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "#custom_W_0 = [[0.1, 0.2], [0.3, 0.4]]  # Replace with your own initial values\n",
        "#custom_b = [0.1, 0.2]  # Replace with your own initial values\n",
        "#custom_V_0 = [[0.1], [0.2]]  # Replace with your own initial values\n",
        "#custom_c = [0.1]  # Replace with your own initial values\n",
        "\n",
        "\n",
        "def calculate_second_order_grad(model, X_raw_torch, Y_torch):\n",
        "    # Forward pass\n",
        "    output = model(X_raw_torch)\n",
        "    # Compute loss\n",
        "    loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "    # Compute gradients of the loss w.r.t. weights\n",
        "    loss.backward(create_graph=True)\n",
        "    # Combine and compute the norm of all gradients\n",
        "    all_grads = torch.cat([param.grad.flatten() for param in model.parameters()])\n",
        "    grad_norm = torch.norm(all_grads)\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "    return second_order_grad\n",
        "\n",
        "def perturb_weights(model, max_deviation=0.01):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            std_dev = param.abs().mean() * max_deviation\n",
        "            noise = torch.randn(param.size()) * std_dev\n",
        "            param[:] = param + noise\n",
        "\n",
        "def restore_weights(model, saved_state):\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            param[:] = saved_state[name]\n",
        "\n",
        "def perturb_data(X, max_deviation=0.01):\n",
        "    \"\"\"Perturb the data tensor X.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        std_dev = X.abs().mean() * max_deviation\n",
        "        noise = torch.randn(X.size()) * std_dev\n",
        "        X.add_(noise)"
      ],
      "metadata": {
        "id": "vBoW060Y1pZB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/zw2788/LocalMinimaConstruction/main/Ex1.csv\")\n",
        "\n",
        "data.head()\n",
        "features = [\n",
        "    \"x_2dvec\",\n",
        "\n",
        "]\n",
        "label = \"y\"\n",
        "\n",
        "# train test split\n",
        "X_raw,  Y = data[features].values, data[label].values\n",
        "\n",
        "#convert string to array\n",
        "X_raw = np.array([eval(s[0]) for s in X_raw])\n",
        "\n",
        "# Standardize the input\n",
        "# Leave blank to match the example in paper\n",
        "\n",
        "# formatting\n",
        "Y = Y.reshape((-1, 1))\n",
        "print(X_raw)\n",
        "print(Y)\n",
        "print(X_raw.shape[0])\n",
        "X_raw = torch.tensor(X_raw, requires_grad=True)\n",
        "Y = torch.tensor(Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0h9evkU59mR",
        "outputId": "678f18f6-410f-4b78-dfe5-3d46df0b9a64"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.8  0.4]\n",
            " [ 3.1  4.3]\n",
            " [ 0.1 -3.4]\n",
            " [-4.2 -3.3]\n",
            " [-0.5  0.2]\n",
            " [-2.7 -0.4]\n",
            " [-3.  -4.3]\n",
            " [-0.1  3.4]\n",
            " [ 4.2  3.2]\n",
            " [ 0.4 -0.1]]\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "10\n",
            "tensor([[ 2.8000,  0.4000],\n",
            "        [ 3.1000,  4.3000],\n",
            "        [ 0.1000, -3.4000],\n",
            "        [-4.2000, -3.3000],\n",
            "        [-0.5000,  0.2000],\n",
            "        [-2.7000, -0.4000],\n",
            "        [-3.0000, -4.3000],\n",
            "        [-0.1000,  3.4000],\n",
            "        [ 4.2000,  3.2000],\n",
            "        [ 0.4000, -0.1000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([[ 2.8000,  0.4000],\n",
            "        [ 3.1000,  4.3000],\n",
            "        [ 0.1000, -3.4000],\n",
            "        [-4.2000, -3.3000],\n",
            "        [-0.5000,  0.2000],\n",
            "        [-2.7000, -0.4000],\n",
            "        [-3.0000, -4.3000],\n",
            "        [-0.1000,  3.4000],\n",
            "        [ 4.2000,  3.2000],\n",
            "        [ 0.4000, -0.1000]], dtype=torch.float64, requires_grad=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-2977234af17c>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_torch = torch.tensor(X_raw, requires_grad=True)\n",
            "<ipython-input-10-2977234af17c>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_torch = torch.tensor(Y)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_raw_torch = torch.tensor(X_raw, requires_grad=True)\n",
        "print(X_raw_torch)\n",
        "print(X_raw)\n",
        "Y_torch = torch.tensor(Y)\n",
        "\n",
        "\n",
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.05 # Adjust this threshold as needed\n",
        "max_iterations = 20  # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.02\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 500\n",
        "\n",
        "# Surrouning points' grads' propotion\n",
        "surrounding_propotion = 100\n",
        "\n",
        "# parameters for the first layer\n",
        "W_0 = np.array([[1.05954587,-0.05625762],[-0.03749863,1.09518945]])\n",
        "b = np.array([[-0.050686,-0.06894291]])\n",
        "\n",
        "# parameters for the second layer\n",
        "\n",
        "V_0 = np.array([[3.76921058],[-3.72139955]])\n",
        "c = np.array([[-0.0148436]])\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "#original_weights = W_0, b, V_0, c\n",
        "original_weights = {\n",
        "    'W_0': nn_model.W_0.data.clone(),\n",
        "    'b': nn_model.b.data.clone(),\n",
        "    'V_0': nn_model.V_0.data.clone(),\n",
        "    'c': nn_model.c.data.clone()\n",
        "}\n",
        "print(original_weights)\n",
        "print(\"Initial X_raw {}\".format(X_raw_torch))\n",
        "max_deviation_for_X = 0.05  # You can adjust this value as needed\n",
        "perturb_data(X_raw_torch, max_deviation=max_deviation_for_X)\n",
        "print(\"Perturbed X_raw {}\".format(X_raw_torch))\n",
        "\n",
        "for i in range(max_iterations):\n",
        "\n",
        "    # Calculate the gradient at the central point\n",
        "    central_grad = calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "\n",
        "    # Surrouning points' grads\n",
        "    surrounding_grads = []\n",
        "\n",
        "\n",
        "    # Calculate the gradient at the surrounding points by MC\n",
        "    for _ in range(MC_num_samples):\n",
        "\n",
        "      nn_model_sample = SimpleNN(custom_W_0=original_weights['W_0'],custom_b=original_weights['b'],custom_V_0=original_weights['V_0'],custom_c=original_weights['c'])\n",
        "      #print(\"W_0 (before perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Perturb weights\n",
        "      perturb_weights(nn_model_sample, max_deviation=0.01)\n",
        "      #print(\"W_0 (after perturbation):\", nn_model_sample.W_0.data)\n",
        "      # Calculate second-order gradient\n",
        "      grad = calculate_second_order_grad(nn_model_sample, X_raw_torch, Y_torch)\n",
        "      surrounding_grads.append(grad)\n",
        "\n",
        "    #print(\"Surrounding grad {}\".format(surrounding_grads))\n",
        "    # Average surrounding gradients\n",
        "    average_surrounding_grad = sum(surrounding_grads) / MC_num_samples\n",
        "\n",
        "    # Combine gradients\n",
        "    combined_grad = central_grad + surrounding_propotion * average_surrounding_grad\n",
        "\n",
        "###############\n",
        "    # Combine and compute the norm of all gradients\n",
        "    #all_grads = torch.cat([nn_model.W_0.grad.flatten(), nn_model.V_0.grad.flatten(), nn_model.b.grad.flatten(), nn_model.c.grad.flatten()])\n",
        "    #grad_norm = torch.norm(all_grads)\n",
        "\n",
        "    # Compute the derivative of the grad_norm with respect to X\n",
        "    #second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "\n",
        "    # Check if the norm of the second-order gradient is below the threshold\n",
        "    if torch.norm(combined_grad) < threshold:\n",
        "        print(f\"Convergence reached at iteration {i}\")\n",
        "        break\n",
        "\n",
        "    # Update X_raw using gradient descent\n",
        "    X_raw_torch.data -= learning_rate * combined_grad\n",
        "\n",
        "    # Zero out gradients for the next iteration\n",
        "    nn_model.zero_grad()\n",
        "    X_raw_torch.grad = None\n",
        "\n",
        "# Print final modified data\n",
        "#print(surrounding_grads)\n",
        "#print(\"Final modified X_raw:\")\n",
        "print(X_raw_torch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiqG3HcGV4u0",
        "outputId": "2b597c0c-47fb-4a62-da62-a49a8fa45d8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-01845999b45e>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_raw_torch = torch.tensor(X_raw, requires_grad=True)\n",
            "<ipython-input-30-01845999b45e>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_torch = torch.tensor(Y)\n",
            "<ipython-input-2-7093221fd565>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_W_0 = torch.tensor(custom_W_0, dtype=torch.float64)\n",
            "<ipython-input-2-7093221fd565>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_b = torch.tensor(custom_b, dtype=torch.float64)\n",
            "<ipython-input-2-7093221fd565>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_V_0 = torch.tensor(custom_V_0, dtype=torch.float64)\n",
            "<ipython-input-2-7093221fd565>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  custom_c = torch.tensor(custom_c, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.8000,  0.4000],\n",
            "        [ 3.1000,  4.3000],\n",
            "        [ 0.1000, -3.4000],\n",
            "        [-4.2000, -3.3000],\n",
            "        [-0.5000,  0.2000],\n",
            "        [-2.7000, -0.4000],\n",
            "        [-3.0000, -4.3000],\n",
            "        [-0.1000,  3.4000],\n",
            "        [ 4.2000,  3.2000],\n",
            "        [ 0.4000, -0.1000]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([[ 2.8000,  0.4000],\n",
            "        [ 3.1000,  4.3000],\n",
            "        [ 0.1000, -3.4000],\n",
            "        [-4.2000, -3.3000],\n",
            "        [-0.5000,  0.2000],\n",
            "        [-2.7000, -0.4000],\n",
            "        [-3.0000, -4.3000],\n",
            "        [-0.1000,  3.4000],\n",
            "        [ 4.2000,  3.2000],\n",
            "        [ 0.4000, -0.1000]], dtype=torch.float64, requires_grad=True)\n",
            "{'W_0': tensor([[ 1.0595, -0.0563],\n",
            "        [-0.0375,  1.0952]], dtype=torch.float64), 'b': tensor([[-0.0507, -0.0689]], dtype=torch.float64), 'V_0': tensor([[ 3.7692],\n",
            "        [-3.7214]], dtype=torch.float64), 'c': tensor([[-0.0148]], dtype=torch.float64)}\n",
            "Initial X_raw tensor([[ 2.8000,  0.4000],\n",
            "        [ 3.1000,  4.3000],\n",
            "        [ 0.1000, -3.4000],\n",
            "        [-4.2000, -3.3000],\n",
            "        [-0.5000,  0.2000],\n",
            "        [-2.7000, -0.4000],\n",
            "        [-3.0000, -4.3000],\n",
            "        [-0.1000,  3.4000],\n",
            "        [ 4.2000,  3.2000],\n",
            "        [ 0.4000, -0.1000]], dtype=torch.float64, requires_grad=True)\n",
            "Perturbed X_raw tensor([[ 2.8190,  0.4468],\n",
            "        [ 2.9588,  4.2332],\n",
            "        [ 0.1733, -3.5271],\n",
            "        [-4.1072, -3.3237],\n",
            "        [-0.4985,  0.3767],\n",
            "        [-2.6656, -0.2504],\n",
            "        [-2.8674, -4.3865],\n",
            "        [-0.2262,  3.3744],\n",
            "        [ 4.0489,  3.1090],\n",
            "        [ 0.5424, -0.2005]], dtype=torch.float64, requires_grad=True)\n",
            "tensor([[ 2.8083,  0.4233],\n",
            "        [ 3.0653,  4.1901],\n",
            "        [ 0.1168, -3.5451],\n",
            "        [-4.0959, -3.3338],\n",
            "        [-0.4943,  0.2493],\n",
            "        [-2.6471, -0.2550],\n",
            "        [-2.9539, -4.3533],\n",
            "        [-0.1544,  3.3885],\n",
            "        [ 4.0357,  3.1357],\n",
            "        [ 0.5095, -0.1240]], dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set a threshold for the norm of the second-order gradient\n",
        "threshold = 0.05 # Adjust this threshold as needed\n",
        "max_iterations = 10  # Maximum number of iterations to prevent infinite loops\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.2\n",
        "\n",
        "# Monte Carlo method sampling points\n",
        "MC_num_samples = 10\n",
        "\n",
        "# Surrouning points' grad\n",
        "surrounding_grads = []\n",
        "\n",
        "# parameters for the first layer\n",
        "W_0 = np.array([[1.05954587,-0.05625762],[-0.03749863,1.09518945]])\n",
        "b = np.array([[-0.050686,-0.06894291]])\n",
        "\n",
        "# parameters for the second layer\n",
        "\n",
        "V_0 = np.array([[3.76921058],[-3.72139955]])\n",
        "c = np.array([[-0.0148436]])\n",
        "\n",
        "nn_model = SimpleNN(W_0, b, V_0, c)\n",
        "\n",
        "perturb_weights(nn_model, max_deviation=0.01)\n",
        "restore_weights(nn_model, original_weights)  # Assuming perturb_weights is defined as before\n",
        "print(perturb_weights)\n",
        "K=calculate_second_order_grad(nn_model, X_raw_torch, Y_torch)\n",
        "print(K)\n",
        "print(\"W_0 (after perturbation):\", nn_model.W_0.data)\n",
        "print(\"b (after perturbation):\", nn_model.b.data)\n",
        "print(\"V_0 (after perturbation):\", nn_model.V_0.data)\n",
        "print(\"c (after perturbation):\", nn_model.c.data)"
      ],
      "metadata": {
        "id": "_qolZlVZ-x8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward pass\n",
        "output = nn_model(X_raw_torch)\n",
        "\n",
        "# Compute loss\n",
        "loss = -torch.mean(Y_torch * torch.log(output) + (1 - Y_torch) * torch.log(1 - output))\n",
        "print(loss)\n",
        "# Compute gradients of the loss w.r.t. weights\n",
        "loss.backward(create_graph=True)\n",
        "\n",
        "\n",
        "# Combine and compute the norm of all gradients\n",
        "all_grads = torch.cat([nn_model.W_0.grad.flatten(), nn_model.V_0.grad.flatten(), nn_model.b.grad.flatten(), nn_model.c.grad.flatten()])\n",
        "print(all_grads)\n",
        "grad_norm = torch.norm(all_grads)\n",
        "print(grad_norm)\n",
        "# Compute the derivative of the grad_norm with respect to X\n",
        "second_order_grad = torch.autograd.grad(grad_norm, X_raw_torch, retain_graph=True)[0]\n",
        "print(torch.norm(second_order_grad))\n",
        "# If you want to perform gradient descent on X_raw\n",
        "learning_rate = 0.01\n",
        "#X_raw_torch.data -= learning_rate * second_order_grad"
      ],
      "metadata": {
        "id": "s_654-o11XCY"
      }
    }
  ]
}